{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:22.368936Z",
     "start_time": "2025-07-03T05:13:19.261778Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from visualization import Visu\n",
    "from subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "\n",
    "workspace = \"NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283ca354729c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:24.765747Z",
     "start_time": "2025-07-03T05:13:24.749913Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": \"env_data.pkl\",\n",
    "        \"cov_module\": \"Matern\",\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\"\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.03,\n",
    "        \"epochs\": 500,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 300\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53557d7595a25f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:26.817204Z",
     "start_time": "2025-07-03T05:13:26.811533Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "547fbebdc9609041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:47.054759Z",
     "start_time": "2025-07-03T05:13:46.953006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 300 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_.pkl\",method='top_n', n=300)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "31d60be0ee55504a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:52.748583Z",
     "start_time": "2025-07-03T05:13:52.369645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 範例 1: 採樣 Top 20 ---\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 64\n",
      "\n",
      "--- 範例 2: 採樣 Top 5% ---\n",
      "方法: Top-P。從 2312 條軌跡中篩選出最好的前 5.0% (115 條)。\n",
      "抽樣檢查最好的一條軌跡：獎勵=68, 長度=40\n",
      "\n",
      "--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\n",
      "方法: Threshold。從 2312 條軌跡中篩選出 22 條獎勵不低於 68 的軌跡。\n",
      "所有高分軌跡的平均獎勵為: 68.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "# 範例2：獲取獎勵排名前 5% 的軌跡\n",
    "print(\"--- 範例 2: 採樣 Top 5% ---\")\n",
    "top_5_percent_trajectories = sample_excellent_trajectories(method='top_p', p=0.05)\n",
    "if top_5_percent_trajectories:\n",
    "    # 打印其中一條軌跡的詳細信息以供檢查\n",
    "    sample_traj_data = top_5_percent_trajectories[0]\n",
    "    print(f\"抽樣檢查最好的一條軌跡：獎勵={sample_traj_data['reward']}, 長度={len(sample_traj_data['states'])}\\n\")\n",
    "\n",
    "# 範例3：獲取所有獎勵值大於等於 45 的軌跡\n",
    "print(\"--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\")\n",
    "high_reward_trajectories = sample_excellent_trajectories(method='threshold', threshold=68)\n",
    "if high_reward_trajectories:\n",
    "    print(f\"所有高分軌跡的平均獎勵為: {np.mean([d['reward'] for d in high_reward_trajectories]):.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c30ac3a9af75ead",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:53.934793Z",
     "start_time": "2025-07-03T05:13:53.927952Z"
    }
   },
   "outputs": [],
   "source": [
    "#基于embedding的模仿学习\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F  # 这里导入 F\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalStateEncoder(nn.Module):\n",
    "    def __init__(self, num_states=98, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_states, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim)\n",
    "\n",
    "    def forward(self, state_seq):\n",
    "        indices = [i for i in state_seq if i >= 0]\n",
    "        if not indices:\n",
    "            return torch.zeros(self.lstm.hidden_size)\n",
    "\n",
    "        input_emb = self.embedding(torch.tensor(indices).long()).unsqueeze(1)  # [T, 1, D]\n",
    "        _, (h_n, _) = self.lstm(input_emb)\n",
    "        return h_n.squeeze(0).squeeze(0)  # [hidden_dim]\n",
    "\n",
    "def encode_temporal_state(state_seq, embed_table):\n",
    "    \"\"\"\n",
    "    输入: state_seq: Tensor[H]，如 [34, 33, -1, ..., -1]\n",
    "    输出: Tensor[embed_dim]，嵌入向量\n",
    "    \"\"\"\n",
    "    indices = [i for i in state_seq if i >= 0]  # 去除 -1\n",
    "    if not indices:\n",
    "        return torch.zeros(embed_table.embedding_dim)\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    embeds = embed_table(indices_tensor)\n",
    "    return embeds.mean(dim=0)\n",
    "def encode_temporal_state2(state_seq, encoder):\n",
    "    return encoder(state_seq)\n",
    "\n",
    "def submodular_selector_temporal(trajectories, embed_table,temporal_encoder, budget=50, lambda_div=0.5, per_traj_limit=True):\n",
    "    \"\"\"\n",
    "    改进版子模选择器：\n",
    "    - 使用欧几里得距离作为 diversity 惩罚项\n",
    "    - 可选启用：每条轨迹最多选一个状态（保证轨迹多样性）\n",
    "    \"\"\"\n",
    "    state_vectors = []\n",
    "    action_labels = []\n",
    "    traj_ids = []\n",
    "\n",
    "    for traj_id, traj in enumerate(trajectories):\n",
    "        states = [int(s.item()) for s in traj['states']]\n",
    "        actions = traj['actions']\n",
    "        for t, action in enumerate(actions):\n",
    "            temporal_state = [-1]*40\n",
    "            for h in range(t+1):\n",
    "                temporal_state[h] = states[h]\n",
    "            temporal_tensor = torch.tensor(temporal_state, dtype=torch.long)\n",
    "            # encoded = encode_temporal_state(temporal_tensor, embed_table) # 使用嵌入表编码\n",
    "            # temporal_encoder = TemporalStateEncoder() #使用lstm+embedding编码\n",
    "            vec = encode_temporal_state2(temporal_tensor, temporal_encoder)\n",
    "            state_vectors.append(vec.detach())\n",
    "            action_labels.append(action)\n",
    "            traj_ids.append(traj_id)\n",
    "\n",
    "    print(f\"Total states collected: {len(state_vectors)}\")\n",
    "    all_states = torch.stack(state_vectors)\n",
    "    all_actions = torch.tensor(action_labels, dtype=torch.long)\n",
    "    traj_ids = torch.tensor(traj_ids)\n",
    "\n",
    "    selected_indices = []\n",
    "    selected_vectors = []\n",
    "    selected_trajs = set()\n",
    "\n",
    "    for _ in range(min(budget, len(all_states))):\n",
    "        best_score, best_idx = -float(\"inf\"), -1\n",
    "\n",
    "        for i in range(len(all_states)):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            if per_traj_limit and traj_ids[i].item() in selected_trajs:\n",
    "                continue\n",
    "\n",
    "            candidate = all_states[i].unsqueeze(0)\n",
    "            reward = torch.abs(candidate).mean().item()\n",
    "\n",
    "            if selected_vectors:\n",
    "                selected_tensor = torch.stack(selected_vectors)\n",
    "                sims = ((candidate - selected_tensor)**2).sum(dim=1)  # Euclidean squared\n",
    "                diversity_penalty = -sims.mean().item()  # maximize distance\n",
    "            else:\n",
    "                diversity_penalty = 0\n",
    "\n",
    "            score = reward + lambda_div * diversity_penalty\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx == -1:\n",
    "            break\n",
    "\n",
    "        selected_indices.append(best_idx)\n",
    "        selected_vectors.append(all_states[best_idx])\n",
    "        selected_trajs.add(traj_ids[best_idx].item())\n",
    "\n",
    "    return all_states[selected_indices], all_actions[selected_indices],all_states,all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70b3e9b47236fd91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:09.743414Z",
     "start_time": "2025-07-03T05:13:55.320175Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 150 條。\n",
      "Embedding(98, 16)\n",
      "Total states collected: 5709\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 16\n",
    "num_states = 98\n",
    "elite_trajectories_data = sample_excellent_trajectories(\n",
    "        filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "        method='top_n', \n",
    "        n=150)\n",
    "embed_table = torch.nn.Embedding(num_states, 16)\n",
    "temporal_encoder = TemporalStateEncoder()\n",
    "print(embed_table)\n",
    "selected_states, selected_actions, all_states,all_actions = submodular_selector_temporal(\n",
    "    trajectories=elite_trajectories_data[:],\n",
    "    embed_table=embed_table,\n",
    "    temporal_encoder=temporal_encoder,\n",
    "    budget=500,\n",
    "    lambda_div=2.0,\n",
    "    per_traj_limit=True\n",
    ")\n",
    "# print(len(selected_states))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5136b575295662c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:14.099463Z",
     "start_time": "2025-07-03T05:14:14.095999Z"
    }
   },
   "outputs": [],
   "source": [
    "#策略网络训练模块\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_policy_network(states, actions, num_actions, epochs=50, batch_size=64, lr=1e-3):\n",
    "    dataset = TensorDataset(states, actions)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = PolicyNetwork(states.size(1), hidden_dim=64, output_dim=num_actions)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_states, batch_actions in loader:\n",
    "            logits = model(batch_states)\n",
    "            loss = criterion(logits, batch_actions)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7df98baeb9c5426",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:31.036085Z",
     "start_time": "2025-07-03T05:14:16.096505Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 1.4721\n",
      "Epoch 101/5000, Loss: 0.2183\n",
      "Epoch 201/5000, Loss: 0.1921\n",
      "Epoch 301/5000, Loss: 0.1762\n",
      "Epoch 401/5000, Loss: 0.1741\n",
      "Epoch 501/5000, Loss: 0.1682\n",
      "Epoch 601/5000, Loss: 0.1649\n",
      "Epoch 701/5000, Loss: 0.1609\n",
      "Epoch 801/5000, Loss: 0.1619\n",
      "Epoch 901/5000, Loss: 0.1588\n",
      "Epoch 1001/5000, Loss: 0.1606\n",
      "Epoch 1101/5000, Loss: 0.1578\n",
      "Epoch 1201/5000, Loss: 0.1575\n",
      "Epoch 1301/5000, Loss: 0.1564\n",
      "Epoch 1401/5000, Loss: 0.1544\n",
      "Epoch 1501/5000, Loss: 0.1562\n",
      "Epoch 1601/5000, Loss: 0.1585\n",
      "Epoch 1701/5000, Loss: 0.1547\n",
      "Epoch 1801/5000, Loss: 0.1535\n",
      "Epoch 1901/5000, Loss: 0.1525\n",
      "Epoch 2001/5000, Loss: 0.1531\n",
      "Epoch 2101/5000, Loss: 0.1517\n",
      "Epoch 2201/5000, Loss: 0.1526\n",
      "Epoch 2301/5000, Loss: 0.1535\n",
      "Epoch 2401/5000, Loss: 0.1524\n",
      "Epoch 2501/5000, Loss: 0.1511\n",
      "Epoch 2601/5000, Loss: 0.1502\n",
      "Epoch 2701/5000, Loss: 0.1495\n",
      "Epoch 2801/5000, Loss: 0.1517\n",
      "Epoch 2901/5000, Loss: 0.1501\n",
      "Epoch 3001/5000, Loss: 0.1498\n",
      "Epoch 3101/5000, Loss: 0.1513\n",
      "Epoch 3201/5000, Loss: 0.1512\n",
      "Epoch 3301/5000, Loss: 0.1516\n",
      "Epoch 3401/5000, Loss: 0.1496\n",
      "Epoch 3501/5000, Loss: 0.1535\n",
      "Epoch 3601/5000, Loss: 0.1492\n",
      "Epoch 3701/5000, Loss: 0.1495\n",
      "Epoch 3801/5000, Loss: 0.1507\n",
      "Epoch 3901/5000, Loss: 0.1498\n",
      "Epoch 4001/5000, Loss: 0.1490\n",
      "Epoch 4101/5000, Loss: 0.1479\n",
      "Epoch 4201/5000, Loss: 0.1487\n",
      "Epoch 4301/5000, Loss: 0.1481\n",
      "Epoch 4401/5000, Loss: 0.1502\n",
      "Epoch 4501/5000, Loss: 0.1489\n",
      "Epoch 4601/5000, Loss: 0.1489\n",
      "Epoch 4701/5000, Loss: 0.1471\n",
      "Epoch 4801/5000, Loss: 0.1490\n",
      "Epoch 4901/5000, Loss: 0.1488\n"
     ]
    }
   ],
   "source": [
    "model = train_policy_network(all_states, all_actions,epochs=5000, num_actions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f62ba1c7de731df3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T04:34:08.245761Z",
     "start_time": "2025-07-03T04:34:08.228990Z"
    }
   },
   "outputs": [],
   "source": [
    "#评估函数\n",
    "def evaluate_policy(model, states, actions):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(states)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == actions).float().mean().item()\n",
    "    print(f\"Evaluation Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "#保存模型\n",
    "def save_policy_model(model, filepath=\"policy_model.pt\"):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "#加载模型\n",
    "def load_policy_model(filepath, input_dim, hidden_dim, output_dim):\n",
    "    model = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "    return model\n",
    "#状态预测函数\n",
    "def predict_action(model, states_tensor):\n",
    "    \"\"\"\n",
    "    state_tensor: shape [1, D] 或 [D]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    actions=[]\n",
    "    probss=[]\n",
    "    for i in range(len(states_tensor)):\n",
    "        # if states_tensor[i] < 0:\n",
    "        #     states_tensor[i] = 0\n",
    "        state_tensor = states_tensor[i]\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            action = torch.argmax(probs, dim=1).item()\n",
    "            actions.append(action)\n",
    "            probss.append(probs.squeeze().tolist())\n",
    "    return actions, probss\n",
    "\n",
    "def predict_action_batch(model, state_batch):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        state_batch: Tensor[N, D]，多个状态向量组成的 batch\n",
    "    输出:\n",
    "        actions: List[int]，每个状态对应的预测动作\n",
    "        probs: List[List[float]]，每个状态的动作概率分布\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if state_batch.dim() == 1:\n",
    "        state_batch = state_batch.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(state_batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        actions = torch.argmax(probs, dim=1).tolist()\n",
    "    return actions, probs.tolist()\n",
    "\n",
    "def predict_action_batch_sample(model, state_batch):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        state_batch: Tensor[N, D]，多个状态向量组成的 batch\n",
    "    输出:\n",
    "        actions: List[int]，每个状态对应的采样动作\n",
    "        probs: List[List[float]]，每个状态的动作概率分布\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if state_batch.dim() == 1:\n",
    "        state_batch = state_batch.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(state_batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        actions = torch.multinomial(probs, num_samples=1).squeeze(1).tolist()\n",
    "    return actions, probs.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd687faf30fb1e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:16:55.466285Z",
     "start_time": "2025-07-03T03:16:55.459780Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 96.00%\n",
      "Model saved to policy_model.pt\n",
      "Predicted Action: [3, 3, 3, 3, 2, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3], Probabilities: [[0.002398516284301877, 1.8067243257036125e-09, 1.737188722472638e-05, 0.9975841045379639, 4.1697767461101876e-10], [0.002398516284301877, 1.8067243257036125e-09, 1.737188722472638e-05, 0.9975841045379639, 4.1697767461101876e-10], [0.002398516284301877, 1.8067243257036125e-09, 1.737188722472638e-05, 0.9975841045379639, 4.1697767461101876e-10], [0.002398516284301877, 1.8067243257036125e-09, 1.737188722472638e-05, 0.9975841045379639, 4.1697767461101876e-10], [6.673606556262257e-09, 0.00024310723529197276, 0.9997256398200989, 3.123933493043296e-05, 4.2011041862096907e-14], [0.0006132003618404269, 0.0011266376823186874, 0.35403868556022644, 3.3643245966885615e-09, 0.6442214846611023], [0.0006132003618404269, 0.0011266376823186874, 0.35403868556022644, 3.3643245966885615e-09, 0.6442214846611023], [0.0006132003618404269, 0.0011266376823186874, 0.35403868556022644, 3.3643245966885615e-09, 0.6442214846611023], [0.0006122399354353547, 0.001129346783272922, 0.35246527194976807, 3.3618967609783112e-09, 0.645793080329895], [0.0006122399354353547, 0.001129346783272922, 0.35246527194976807, 3.3618967609783112e-09, 0.645793080329895], [0.00019461980264168233, 0.0010644944850355387, 0.00011701764742610976, 2.2598501345783006e-06, 0.9986215829849243], [0.000256202882155776, 0.0007194510544650257, 4.818971501663327e-05, 5.805382170365192e-05, 0.9989180564880371], [0.000256202882155776, 0.0007194510544650257, 4.818971501663327e-05, 5.805382170365192e-05, 0.9989180564880371], [0.000256202882155776, 0.0007194510544650257, 4.818971501663327e-05, 5.805382170365192e-05, 0.9989180564880371], [0.000256202882155776, 0.0007194510544650257, 4.818971501663327e-05, 5.805382170365192e-05, 0.9989180564880371], [0.000256202882155776, 0.0007194510544650257, 4.818971501663327e-05, 5.805382170365192e-05, 0.9989180564880371], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [0.0002561143191996962, 0.0007154446793720126, 4.803314004675485e-05, 5.8110683312406763e-05, 0.9989223480224609], [3.467385613475926e-05, 0.23766514658927917, 0.747600257396698, 0.01016880664974451, 0.00453107850626111], [3.467385613475926e-05, 0.23766514658927917, 0.747600257396698, 0.01016880664974451, 0.00453107850626111], [3.467385613475926e-05, 0.23766514658927917, 0.747600257396698, 0.01016880664974451, 0.00453107850626111], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.689102079373896e-10, 1.7464994016336277e-05, 0.0018016318790614605, 0.9981808662414551, 4.563451283523534e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [3.060487696604497e-10, 1.3983401004225016e-05, 0.0011646292405202985, 0.9988214373588562, 3.64883151471318e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.1797806882293287e-10, 1.895889545266982e-05, 0.0018024543533101678, 0.9981786012649536, 5.11161057925591e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [4.663880948108101e-10, 1.3716840840061195e-05, 0.002393444301560521, 0.9975928664207458, 5.551546999882362e-10], [1.0586900245357356e-08, 0.023562133312225342, 0.4029560387134552, 0.5734816789627075, 7.514080380133237e-08], [1.0586900245357356e-08, 0.023562133312225342, 0.4029560387134552, 0.5734816789627075, 7.514080380133237e-08], [0.00023281486937776208, 0.843930184841156, 0.0007149941520765424, 8.176788833225146e-05, 0.1550401896238327], [0.00023281486937776208, 0.843930184841156, 0.0007149941520765424, 8.176788833225146e-05, 0.1550401896238327], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07], [7.963095782770324e-08, 0.021373070776462555, 0.04789811000227928, 0.9307278394699097, 8.432032814198465e-07]]\n"
     ]
    }
   ],
   "source": [
    "# 假设你已经训练好模型\n",
    "acc = evaluate_policy(model, selected_states, selected_actions)\n",
    "save_policy_model(model)\n",
    "\n",
    "# 使用模型预测一个新状态\n",
    "states = selected_states\n",
    "actions, prob = predict_action_batch(model, states)\n",
    "print(f\"Predicted Action: {actions}, Probabilities: {prob}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9a7204788c4a918",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:16:55.663590Z",
     "start_time": "2025-07-03T03:16:55.660697Z"
    }
   },
   "outputs": [],
   "source": [
    "def trajectory_to_temporal_states(batch_trajectory, temporal_encoder):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        batch_trajectory: Tensor[B, 40]\n",
    "        temporal_encoder: TemporalStateEncoder 实例\n",
    "    输出:\n",
    "        Tensor[B, D]  # 每个轨迹编码后的向量\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "    for traj in batch_trajectory:\n",
    "        traj = traj.tolist()\n",
    "        vec = encode_temporal_state2(traj, temporal_encoder).detach()\n",
    "        batch_embeddings.append(vec)\n",
    "    return torch.stack(batch_embeddings)  # [B, D]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53ef1efbe586c39a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T04:40:14.462644Z",
     "start_time": "2025-07-03T04:36:26.318004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean  tensor(57.8774)  max  tensor(68.)  median  tensor(65.)  min  tensor(21.)\n",
      "Max index  tensor(4)\n",
      "State  0   tensor(34)  Action  3  Return  tensor(6)\n",
      "State  1   tensor(33)  Action  3  Return  tensor(8)\n",
      "State  2   tensor(32)  Action  3  Return  tensor(10)\n",
      "State  3   tensor(31)  Action  3  Return  tensor(12)\n",
      "State  4   tensor(30)  Action  2  Return  tensor(14)\n",
      "State  5   tensor(44)  Action  2  Return  tensor(16)\n",
      "State  6   tensor(58)  Action  2  Return  tensor(18)\n",
      "State  7   tensor(72)  Action  3  Return  tensor(20)\n",
      "State  8   tensor(71)  Action  3  Return  tensor(22)\n",
      "State  9   tensor(70)  Action  4  Return  tensor(24)\n",
      "State  10   tensor(56)  Action  4  Return  tensor(26)\n",
      "State  11   tensor(42)  Action  4  Return  tensor(28)\n",
      "State  12   tensor(28)  Action  4  Return  tensor(30)\n",
      "State  13   tensor(14)  Action  4  Return  tensor(32)\n",
      "State  14   tensor(0)  Action  1  Return  tensor(34)\n",
      "State  15   tensor(1)  Action  1  Return  tensor(36)\n",
      "State  16   tensor(2)  Action  1  Return  tensor(36)\n",
      "State  17   tensor(3)  Action  2  Return  tensor(36)\n",
      "State  18   tensor(17)  Action  2  Return  tensor(36)\n",
      "State  19   tensor(31)  Action  1  Return  tensor(36)\n",
      "State  20   tensor(32)  Action  1  Return  tensor(36)\n",
      "State  21   tensor(33)  Action  1  Return  tensor(36)\n",
      "State  22   tensor(34)  Action  1  Return  tensor(38)\n",
      "State  23   tensor(35)  Action  1  Return  tensor(40)\n",
      "State  24   tensor(36)  Action  1  Return  tensor(42)\n",
      "State  25   tensor(37)  Action  1  Return  tensor(44)\n",
      "State  26   tensor(38)  Action  4  Return  tensor(46)\n",
      "State  27   tensor(24)  Action  4  Return  tensor(48)\n",
      "State  28   tensor(10)  Action  1  Return  tensor(50)\n",
      "State  29   tensor(11)  Action  1  Return  tensor(52)\n",
      "State  30   tensor(12)  Action  2  Return  tensor(54)\n",
      "State  31   tensor(26)  Action  2  Return  tensor(56)\n",
      "State  32   tensor(40)  Action  2  Return  tensor(58)\n",
      "State  33   tensor(54)  Action  2  Return  tensor(60)\n",
      "State  34   tensor(68)  Action  2  Return  tensor(62)\n",
      "State  35   tensor(82)  Action  3  Return  tensor(64)\n",
      "State  36   tensor(81)  Action  3  Return  tensor(66)\n",
      "State  37   tensor(80)  Action  4  Return  tensor(68)\n",
      "State  38   tensor(66)  Action  4  Return  tensor(68)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "params[\"common\"][\"batch_size\"]=10000\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "env.get_horizon_transition_matrix()\n",
    "env.initialize()\n",
    "init_state = env.state\n",
    "mat_action = []\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "marginal_return = []\n",
    "mat_state.append(env.state)\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = env.state.reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    # print(batch_state)\n",
    "    temporal_states = trajectory_to_temporal_states(batch_state, temporal_encoder)\n",
    "    # print(\" temporal_states\",temporal_states)\n",
    "    actions, prob = predict_action_batch_sample(model, temporal_states)\n",
    "    random_action = random.randint(0, env.action_dim - 1)\n",
    "    # print(\"random_action\", random_action,\"action\", actions)\n",
    "    env.step(h_iter, torch.tensor(actions))\n",
    "    # env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_action.append(actions)\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    # print(\"Action \", actions, \" state \", env.state,\" mat return \", mat_return[-1])\n",
    "obj = env.weighted_traj_return(mat_state).float()\n",
    "print( \" mean \", obj.mean(), \" max \",\n",
    "          obj.max(), \" median \", obj.median(), \" min \", obj.min())\n",
    "max_index = torch.argmax(obj)\n",
    "print(\"Max index \", max_index)\n",
    "for i in range(len(mat_state)-1):\n",
    "    print(\"State \", i, \" \", mat_state[i][max_index], \" Action \", mat_action[i][max_index], \" Return \", mat_return[i][max_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c430c2405f5441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:21:00.844415Z",
     "start_time": "2025-07-03T03:21:00.842327Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a83b915e88a6e46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
