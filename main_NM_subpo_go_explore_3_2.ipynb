{
 "cells": [
  {
   "cell_type": "code",
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:22.368936Z",
     "start_time": "2025-07-03T05:13:19.261778Z"
    }
   },
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.environment import GridWorld\n",
    "from utils.network import append_state\n",
    "from utils.network import policy as agent_net\n",
    "from utils.visualization import Visu\n",
    "from utils.subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "\n",
    "workspace = \"NM\""
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:24.765747Z",
     "start_time": "2025-07-03T05:13:24.749913Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": \"env_data.pkl\",\n",
    "        \"cov_module\": \"Matern\",\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\"\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.03,\n",
    "        \"epochs\": 500,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 300\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ],
   "id": "283ca354729c8110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:26.817204Z",
     "start_time": "2025-07-03T05:13:26.811533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ],
   "id": "f53557d7595a25f0",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:47.054759Z",
     "start_time": "2025-07-03T05:13:46.953006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_.pkl\",method='top_n', n=300)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "    "
   ],
   "id": "547fbebdc9609041",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 300 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 59\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:52.748583Z",
     "start_time": "2025-07-03T05:13:52.369645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "# 範例2：獲取獎勵排名前 5% 的軌跡\n",
    "print(\"--- 範例 2: 採樣 Top 5% ---\")\n",
    "top_5_percent_trajectories = sample_excellent_trajectories(method='top_p', p=0.05)\n",
    "if top_5_percent_trajectories:\n",
    "    # 打印其中一條軌跡的詳細信息以供檢查\n",
    "    sample_traj_data = top_5_percent_trajectories[0]\n",
    "    print(f\"抽樣檢查最好的一條軌跡：獎勵={sample_traj_data['reward']}, 長度={len(sample_traj_data['states'])}\\n\")\n",
    "\n",
    "# 範例3：獲取所有獎勵值大於等於 45 的軌跡\n",
    "print(\"--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\")\n",
    "high_reward_trajectories = sample_excellent_trajectories(method='threshold', threshold=68)\n",
    "if high_reward_trajectories:\n",
    "    print(f\"所有高分軌跡的平均獎勵為: {np.mean([d['reward'] for d in high_reward_trajectories]):.2f}\\n\")\n"
   ],
   "id": "31d60be0ee55504a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 範例 1: 採樣 Top 20 ---\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 64\n",
      "\n",
      "--- 範例 2: 採樣 Top 5% ---\n",
      "方法: Top-P。從 2312 條軌跡中篩選出最好的前 5.0% (115 條)。\n",
      "抽樣檢查最好的一條軌跡：獎勵=68, 長度=40\n",
      "\n",
      "--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\n",
      "方法: Threshold。從 2312 條軌跡中篩選出 11 條獎勵不低於 68 的軌跡。\n",
      "所有高分軌跡的平均獎勵為: 68.00\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:13:53.934793Z",
     "start_time": "2025-07-03T05:13:53.927952Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#基于embedding的模仿学习\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F  # 这里导入 F\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalStateEncoder(nn.Module):\n",
    "    def __init__(self, num_states=98, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_states, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim)\n",
    "\n",
    "    def forward(self, state_seq):\n",
    "        indices = [i for i in state_seq if i >= 0]\n",
    "        if not indices:\n",
    "            return torch.zeros(self.lstm.hidden_size)\n",
    "\n",
    "        input_emb = self.embedding(torch.tensor(indices).long()).unsqueeze(1)  # [T, 1, D]\n",
    "        _, (h_n, _) = self.lstm(input_emb)\n",
    "        return h_n.squeeze(0).squeeze(0)  # [hidden_dim]\n",
    "\n",
    "def encode_temporal_state(state_seq, embed_table):\n",
    "    \"\"\"\n",
    "    输入: state_seq: Tensor[H]，如 [34, 33, -1, ..., -1]\n",
    "    输出: Tensor[embed_dim]，嵌入向量\n",
    "    \"\"\"\n",
    "    indices = [i for i in state_seq if i >= 0]  # 去除 -1\n",
    "    if not indices:\n",
    "        return torch.zeros(embed_table.embedding_dim)\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    embeds = embed_table(indices_tensor)\n",
    "    return embeds.mean(dim=0)\n",
    "def encode_temporal_state2(state_seq, encoder):\n",
    "    return encoder(state_seq)\n",
    "\n",
    "def submodular_selector_temporal(trajectories, embed_table,temporal_encoder, budget=50, lambda_div=0.5, per_traj_limit=True):\n",
    "    \"\"\"\n",
    "    改进版子模选择器：\n",
    "    - 使用欧几里得距离作为 diversity 惩罚项\n",
    "    - 可选启用：每条轨迹最多选一个状态（保证轨迹多样性）\n",
    "    \"\"\"\n",
    "    state_vectors = []\n",
    "    action_labels = []\n",
    "    traj_ids = []\n",
    "\n",
    "    for traj_id, traj in enumerate(trajectories):\n",
    "        states = [int(s.item()) for s in traj['states']]\n",
    "        actions = traj['actions']\n",
    "        for t, action in enumerate(actions):\n",
    "            temporal_state = [-1]*40\n",
    "            for h in range(t+1):\n",
    "                temporal_state[h] = states[h]\n",
    "            temporal_tensor = torch.tensor(temporal_state, dtype=torch.long)\n",
    "            # encoded = encode_temporal_state(temporal_tensor, embed_table) # 使用嵌入表编码\n",
    "            # temporal_encoder = TemporalStateEncoder() #使用lstm+embedding编码\n",
    "            vec = encode_temporal_state2(temporal_tensor, temporal_encoder)\n",
    "            state_vectors.append(vec.detach())\n",
    "            action_labels.append(action)\n",
    "            traj_ids.append(traj_id)\n",
    "\n",
    "    print(f\"Total states collected: {len(state_vectors)}\")\n",
    "    all_states = torch.stack(state_vectors)\n",
    "    all_actions = torch.tensor(action_labels, dtype=torch.long)\n",
    "    traj_ids = torch.tensor(traj_ids)\n",
    "\n",
    "    selected_indices = []\n",
    "    selected_vectors = []\n",
    "    selected_trajs = set()\n",
    "\n",
    "    for _ in range(min(budget, len(all_states))):\n",
    "        best_score, best_idx = -float(\"inf\"), -1\n",
    "\n",
    "        for i in range(len(all_states)):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            if per_traj_limit and traj_ids[i].item() in selected_trajs:\n",
    "                continue\n",
    "\n",
    "            candidate = all_states[i].unsqueeze(0)\n",
    "            reward = torch.abs(candidate).mean().item()\n",
    "\n",
    "            if selected_vectors:\n",
    "                selected_tensor = torch.stack(selected_vectors)\n",
    "                sims = ((candidate - selected_tensor)**2).sum(dim=1)  # Euclidean squared\n",
    "                diversity_penalty = -sims.mean().item()  # maximize distance\n",
    "            else:\n",
    "                diversity_penalty = 0\n",
    "\n",
    "            score = reward + lambda_div * diversity_penalty\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx == -1:\n",
    "            break\n",
    "\n",
    "        selected_indices.append(best_idx)\n",
    "        selected_vectors.append(all_states[best_idx])\n",
    "        selected_trajs.add(traj_ids[best_idx].item())\n",
    "\n",
    "    return all_states[selected_indices], all_actions[selected_indices],all_states,all_actions"
   ],
   "id": "9c30ac3a9af75ead",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:09.743414Z",
     "start_time": "2025-07-03T05:13:55.320175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "embed_dim = 16\n",
    "num_states = 98\n",
    "elite_trajectories_data = sample_excellent_trajectories(\n",
    "        filepath=\"go_explore_archive_spacetime.pkl\", \n",
    "        method='top_n', \n",
    "        n=150)\n",
    "embed_table = torch.nn.Embedding(num_states, 16)\n",
    "temporal_encoder = TemporalStateEncoder()\n",
    "print(embed_table)\n",
    "selected_states, selected_actions, all_states,all_actions = submodular_selector_temporal(\n",
    "    trajectories=elite_trajectories_data[:],\n",
    "    embed_table=embed_table,\n",
    "    temporal_encoder=temporal_encoder,\n",
    "    budget=500,\n",
    "    lambda_div=2.0,\n",
    "    per_traj_limit=True\n",
    ")\n",
    "# print(len(selected_states))\n"
   ],
   "id": "70b3e9b47236fd91",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 150 條。\n",
      "Embedding(98, 16)\n",
      "Total states collected: 5715\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:14.099463Z",
     "start_time": "2025-07-03T05:14:14.095999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#策略网络训练模块\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "def train_policy_network(states, actions, num_actions, epochs=50, batch_size=64, lr=1e-3):\n",
    "    dataset = TensorDataset(states, actions)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = PolicyNetwork(states.size(1), hidden_dim=64, output_dim=num_actions)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for batch_states, batch_actions in loader:\n",
    "            logits = model(batch_states)\n",
    "            loss = criterion(logits, batch_actions)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
    "    return model"
   ],
   "id": "5136b575295662c3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T05:14:31.036085Z",
     "start_time": "2025-07-03T05:14:16.096505Z"
    }
   },
   "cell_type": "code",
   "source": "model = train_policy_network(all_states, all_actions,epochs=5000, num_actions=5)",
   "id": "f7df98baeb9c5426",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000, Loss: 1.5180\n",
      "Epoch 101/5000, Loss: 0.2428\n",
      "Epoch 201/5000, Loss: 0.2084\n",
      "Epoch 301/5000, Loss: 0.1950\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_policy_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mall_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mall_actions\u001B[49m\u001B[43m,\u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_actions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 35\u001B[0m, in \u001B[0;36mtrain_policy_network\u001B[0;34m(states, actions, num_actions, epochs, batch_size, lr)\u001B[0m\n\u001B[1;32m     33\u001B[0m     optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     34\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[0;32m---> 35\u001B[0m     \u001B[43moptimizer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstep\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     37\u001B[0m     total_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/optim/optimizer.py:469\u001B[0m, in \u001B[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    467\u001B[0m \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m cast(Optimizer, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m    468\u001B[0m profile_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOptimizer.step#\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.step\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 469\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrecord_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprofile_name\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    470\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# call optimizer step pre hooks\u001B[39;49;00m\n\u001B[1;32m    471\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mpre_hook\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mchain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    472\u001B[0m \u001B[43m        \u001B[49m\u001B[43m_global_optimizer_pre_hooks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    473\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_optimizer_step_pre_hooks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    474\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    475\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresult\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpre_hook\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/autograd/profiler.py:705\u001B[0m, in \u001B[0;36mrecord_function.__exit__\u001B[0;34m(self, exc_type, exc_value, traceback)\u001B[0m\n\u001B[1;32m    703\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39mis_scripting():\n\u001B[1;32m    704\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39mDisableTorchFunctionSubclass():\n\u001B[0;32m--> 705\u001B[0m         \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprofiler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_record_function_exit\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_RecordFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mrecord\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    706\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    707\u001B[0m     torch\u001B[38;5;241m.\u001B[39mops\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39m_record_function_exit(record)\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/_ops.py:888\u001B[0m, in \u001B[0;36mTorchBindOpOverload.__call__\u001B[0;34m(self_, *args, **kwargs)\u001B[0m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(self_, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):  \u001B[38;5;66;03m# noqa: B902\u001B[39;00m\n\u001B[0;32m--> 888\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m_must_dispatch_in_python\u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    889\u001B[0m         \u001B[38;5;66;03m# When any inputs are FakeScriptObject, we need to\u001B[39;00m\n\u001B[1;32m    890\u001B[0m         \u001B[38;5;66;03m# skip c++ dispatcher and dispatch in python through _get_dispatch of python_dispatcher\u001B[39;00m\n\u001B[1;32m    891\u001B[0m         \u001B[38;5;66;03m# because C++ dispatcher will check the schema and cannot recognize FakeScriptObject.\u001B[39;00m\n\u001B[1;32m    892\u001B[0m         \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    893\u001B[0m         \u001B[38;5;66;03m# Note:\u001B[39;00m\n\u001B[1;32m    894\u001B[0m         \u001B[38;5;66;03m# 1. We only register the torchbind op temporarily as effectful op because we only want\u001B[39;00m\n\u001B[1;32m    895\u001B[0m         \u001B[38;5;66;03m#    the effect token functionalization logic to be applied during tracing. Otherwise, the behavior\u001B[39;00m\n\u001B[1;32m    896\u001B[0m         \u001B[38;5;66;03m#    of the eagerly executing the op might change after tracing.\u001B[39;00m\n\u001B[1;32m    897\u001B[0m         \u001B[38;5;66;03m# 2. We don't want to register the op as effectful for all torchbind ops in ctor because this might\u001B[39;00m\n\u001B[1;32m    898\u001B[0m         \u001B[38;5;66;03m#    cause unexpected behavior for some autograd.profiler ops e.g. profiler._record_function_exit._RecordFunction.\u001B[39;00m\n\u001B[1;32m    899\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m self_\u001B[38;5;241m.\u001B[39m_register_as_effectful_op_temporarily():\n\u001B[1;32m    900\u001B[0m             \u001B[38;5;28;01mreturn\u001B[39;00m self_\u001B[38;5;241m.\u001B[39m_dispatch_in_python(\n\u001B[1;32m    901\u001B[0m                 args, kwargs, self_\u001B[38;5;241m.\u001B[39m_fallthrough_keys()\n\u001B[1;32m    902\u001B[0m             )\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/_ops.py:944\u001B[0m, in \u001B[0;36m_must_dispatch_in_python\u001B[0;34m(args, kwargs)\u001B[0m\n\u001B[1;32m    943\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_must_dispatch_in_python\u001B[39m(args, kwargs):\n\u001B[0;32m--> 944\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpytree\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtree_any\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    945\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    946\u001B[0m \u001B[43m            \u001B[49m\u001B[43mobj\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_library\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfake_class_registry\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mFakeScriptObject\u001B[49m\n\u001B[1;32m    947\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    948\u001B[0m \u001B[43m        \u001B[49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    949\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/utils/_pytree.py:1187\u001B[0m, in \u001B[0;36mtree_any\u001B[0;34m(pred, tree, is_leaf)\u001B[0m\n\u001B[1;32m   1181\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtree_any\u001B[39m(\n\u001B[1;32m   1182\u001B[0m     pred: Callable[[Any], \u001B[38;5;28mbool\u001B[39m],\n\u001B[1;32m   1183\u001B[0m     tree: PyTree,\n\u001B[1;32m   1184\u001B[0m     is_leaf: Optional[Callable[[PyTree], \u001B[38;5;28mbool\u001B[39m]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1185\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[1;32m   1186\u001B[0m     flat_args \u001B[38;5;241m=\u001B[39m tree_iter(tree, is_leaf\u001B[38;5;241m=\u001B[39mis_leaf)\n\u001B[0;32m-> 1187\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43many\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmap\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mpred\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflat_args\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/utils/_pytree.py:888\u001B[0m, in \u001B[0;36mtree_iter\u001B[0;34m(tree, is_leaf)\u001B[0m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;66;03m# Recursively flatten the children\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child \u001B[38;5;129;01min\u001B[39;00m child_pytrees:\n\u001B[0;32m--> 888\u001B[0m     \u001B[38;5;28;01myield from\u001B[39;00m tree_iter(child, is_leaf\u001B[38;5;241m=\u001B[39mis_leaf)\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/utils/_pytree.py:884\u001B[0m, in \u001B[0;36mtree_iter\u001B[0;34m(tree, is_leaf)\u001B[0m\n\u001B[1;32m    882\u001B[0m node_type \u001B[38;5;241m=\u001B[39m _get_node_type(tree)\n\u001B[1;32m    883\u001B[0m flatten_fn \u001B[38;5;241m=\u001B[39m SUPPORTED_NODES[node_type]\u001B[38;5;241m.\u001B[39mflatten_fn\n\u001B[0;32m--> 884\u001B[0m child_pytrees, _ \u001B[38;5;241m=\u001B[39m \u001B[43mflatten_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtree\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    886\u001B[0m \u001B[38;5;66;03m# Recursively flatten the children\u001B[39;00m\n\u001B[1;32m    887\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m child \u001B[38;5;129;01min\u001B[39;00m child_pytrees:\n",
      "File \u001B[0;32m~/PycharmProjects/rayTrain3.12.5/lib/python3.12/site-packages/torch/utils/_pytree.py:424\u001B[0m, in \u001B[0;36m_dict_flatten\u001B[0;34m(d)\u001B[0m\n\u001B[1;32m    423\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_dict_flatten\u001B[39m(d: Dict[Any, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[List[Any], Context]:\n\u001B[0;32m--> 424\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\u001B[43md\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalues\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m), \u001B[38;5;28mlist\u001B[39m(d\u001B[38;5;241m.\u001B[39mkeys())\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T04:34:08.245761Z",
     "start_time": "2025-07-03T04:34:08.228990Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#评估函数\n",
    "def evaluate_policy(model, states, actions):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(states)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        accuracy = (preds == actions).float().mean().item()\n",
    "    print(f\"Evaluation Accuracy: {accuracy*100:.2f}%\")\n",
    "    return accuracy\n",
    "#保存模型\n",
    "def save_policy_model(model, filepath=\"policy_model.pt\"):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "#加载模型\n",
    "def load_policy_model(filepath, input_dim, hidden_dim, output_dim):\n",
    "    model = PolicyNetwork(input_dim, hidden_dim, output_dim)\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "    return model\n",
    "#状态预测函数\n",
    "def predict_action(model, states_tensor):\n",
    "    \"\"\"\n",
    "    state_tensor: shape [1, D] 或 [D]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    actions=[]\n",
    "    probss=[]\n",
    "    for i in range(len(states_tensor)):\n",
    "        # if states_tensor[i] < 0:\n",
    "        #     states_tensor[i] = 0\n",
    "        state_tensor = states_tensor[i]\n",
    "        if state_tensor.dim() == 1:\n",
    "            state_tensor = state_tensor.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            logits = model(state_tensor)\n",
    "            probs = torch.softmax(logits, dim=1)\n",
    "            action = torch.argmax(probs, dim=1).item()\n",
    "            actions.append(action)\n",
    "            probss.append(probs.squeeze().tolist())\n",
    "    return actions, probss\n",
    "\n",
    "def predict_action_batch(model, state_batch):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        state_batch: Tensor[N, D]，多个状态向量组成的 batch\n",
    "    输出:\n",
    "        actions: List[int]，每个状态对应的预测动作\n",
    "        probs: List[List[float]]，每个状态的动作概率分布\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if state_batch.dim() == 1:\n",
    "        state_batch = state_batch.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(state_batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        actions = torch.argmax(probs, dim=1).tolist()\n",
    "    return actions, probs.tolist()\n",
    "\n",
    "def predict_action_batch_sample(model, state_batch):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        state_batch: Tensor[N, D]，多个状态向量组成的 batch\n",
    "    输出:\n",
    "        actions: List[int]，每个状态对应的采样动作\n",
    "        probs: List[List[float]]，每个状态的动作概率分布\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if state_batch.dim() == 1:\n",
    "        state_batch = state_batch.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(state_batch)\n",
    "        probs = torch.softmax(logits, dim=1)\n",
    "        actions = torch.multinomial(probs, num_samples=1).squeeze(1).tolist()\n",
    "    return actions, probs.tolist()\n"
   ],
   "id": "f62ba1c7de731df3",
   "outputs": [],
   "execution_count": 201
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:16:55.466285Z",
     "start_time": "2025-07-03T03:16:55.459780Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设你已经训练好模型\n",
    "acc = evaluate_policy(model, selected_states, selected_actions)\n",
    "save_policy_model(model)\n",
    "\n",
    "# 使用模型预测一个新状态\n",
    "states = selected_states\n",
    "actions, prob = predict_action_batch(model, states)\n",
    "print(f\"Predicted Action: {actions}, Probabilities: {prob}\")"
   ],
   "id": "9bd687faf30fb1e4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Accuracy: 90.67%\n",
      "Model saved to policy_model.pt\n",
      "Predicted Action: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 4, 4, 4, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], Probabilities: [[0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.108256926992908e-05, 0.4728817939758301, 0.5270264148712158, 7.042189622552542e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.113693522522226e-05, 0.47351235151290894, 0.526395857334137, 7.046822361189697e-07], [0.0, 9.262152889277786e-05, 0.48326054215431213, 0.5166460871696472, 7.33222293547442e-07], [0.0, 0.03545814007520676, 0.9450743198394775, 0.01229328103363514, 0.007174304220825434], [0.0, 7.881437340984121e-05, 0.01820526272058487, 0.3703141212463379, 0.6114017963409424], [0.0, 7.879247277742252e-05, 0.018207209184765816, 0.37019988894462585, 0.6115140914916992], [0.0, 7.879247277742252e-05, 0.018207209184765816, 0.37019988894462585, 0.6115140914916992], [0.0, 0.8414512872695923, 0.1570490151643753, 0.0010147738503292203, 0.0004849840479437262], [0.0, 0.01525342557579279, 0.9799709320068359, 0.0004487508558668196, 0.004326913505792618], [0.0, 0.01525342557579279, 0.9799709320068359, 0.0004487508558668196, 0.004326913505792618], [0.0, 0.01525342557579279, 0.9799709320068359, 0.0004487508558668196, 0.004326913505792618], [0.0, 0.01525342557579279, 0.9799709320068359, 0.0004487508558668196, 0.004326913505792618], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 0.3567010164260864, 0.6401153802871704, 0.0031836098060011864, 3.3310935390259977e-13], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.4747210975183407e-07, 0.00018152959819417447, 0.9987133741378784, 0.0011047371663153172], [0.0, 3.480130317257135e-07, 0.00018179866310674697, 0.9987106323242188, 0.001107185729779303], [0.0, 3.480130317257135e-07, 0.00018179866310674697, 0.9987106323242188, 0.001107185729779303], [0.0, 3.480130317257135e-07, 0.00018179866310674697, 0.9987106323242188, 0.001107185729779303], [0.0, 3.480130317257135e-07, 0.00018179866310674697, 0.9987106323242188, 0.001107185729779303], [0.0, 3.480130317257135e-07, 0.00018179866310674697, 0.9987106323242188, 0.001107185729779303], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 0.9975966811180115, 0.0007897326140664518, 0.0016104162205010653, 3.0947467166697606e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.4109246876614634e-05, 0.0004472103319130838, 0.9995347261428833, 3.874574304063572e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 1.680634159129113e-05, 0.0006272260216064751, 0.9993517994880676, 4.144200374867069e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 2.819990913849324e-05, 0.0004185916332062334, 0.9995487332344055, 4.353591521066846e-06], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05], [0.0, 0.0006639990606345236, 0.8273972272872925, 0.17185078561306, 8.796616748441011e-05]]\n"
     ]
    }
   ],
   "execution_count": 194
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:16:55.663590Z",
     "start_time": "2025-07-03T03:16:55.660697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def trajectory_to_temporal_states(batch_trajectory, temporal_encoder):\n",
    "    \"\"\"\n",
    "    输入:\n",
    "        batch_trajectory: Tensor[B, 40]\n",
    "        temporal_encoder: TemporalStateEncoder 实例\n",
    "    输出:\n",
    "        Tensor[B, D]  # 每个轨迹编码后的向量\n",
    "    \"\"\"\n",
    "    batch_embeddings = []\n",
    "    for traj in batch_trajectory:\n",
    "        traj = traj.tolist()\n",
    "        vec = encode_temporal_state2(traj, temporal_encoder).detach()\n",
    "        batch_embeddings.append(vec)\n",
    "    return torch.stack(batch_embeddings)  # [B, D]"
   ],
   "id": "f9a7204788c4a918",
   "outputs": [],
   "execution_count": 195
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T04:40:14.462644Z",
     "start_time": "2025-07-03T04:36:26.318004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "params[\"common\"][\"batch_size\"]=10000\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "env.get_horizon_transition_matrix()\n",
    "env.initialize()\n",
    "init_state = env.state\n",
    "mat_action = []\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "marginal_return = []\n",
    "mat_state.append(env.state)\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = env.state.reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    # print(batch_state)\n",
    "    temporal_states = trajectory_to_temporal_states(batch_state, temporal_encoder)\n",
    "    # print(\" temporal_states\",temporal_states)\n",
    "    actions, prob = predict_action_batch_sample(model, temporal_states)\n",
    "    random_action = random.randint(0, env.action_dim - 1)\n",
    "    # print(\"random_action\", random_action,\"action\", actions)\n",
    "    env.step(h_iter, torch.tensor(actions))\n",
    "    # env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_action.append(actions)\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    # print(\"Action \", actions, \" state \", env.state,\" mat return \", mat_return[-1])\n",
    "obj = env.weighted_traj_return(mat_state).float()\n",
    "print( \" mean \", obj.mean(), \" max \",\n",
    "          obj.max(), \" median \", obj.median(), \" min \", obj.min())\n",
    "max_index = torch.argmax(obj)\n",
    "print(\"Max index \", max_index)\n",
    "for i in range(len(mat_state)-1):\n",
    "    print(\"State \", i, \" \", mat_state[i][max_index], \" Action \", mat_action[i][max_index], \" Return \", mat_return[i][max_index])"
   ],
   "id": "53ef1efbe586c39a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean  tensor(59.9615)  max  tensor(68.)  median  tensor(64.)  min  tensor(11.)\n",
      "Max index  tensor(0)\n",
      "State  0   tensor(34)  Action  3  Return  tensor(6)\n",
      "State  1   tensor(33)  Action  3  Return  tensor(8)\n",
      "State  2   tensor(32)  Action  3  Return  tensor(10)\n",
      "State  3   tensor(31)  Action  2  Return  tensor(11)\n",
      "State  4   tensor(45)  Action  3  Return  tensor(13)\n",
      "State  5   tensor(44)  Action  2  Return  tensor(15)\n",
      "State  6   tensor(58)  Action  2  Return  tensor(17)\n",
      "State  7   tensor(72)  Action  3  Return  tensor(19)\n",
      "State  8   tensor(71)  Action  3  Return  tensor(21)\n",
      "State  9   tensor(70)  Action  4  Return  tensor(23)\n",
      "State  10   tensor(56)  Action  4  Return  tensor(25)\n",
      "State  11   tensor(42)  Action  4  Return  tensor(27)\n",
      "State  12   tensor(28)  Action  4  Return  tensor(29)\n",
      "State  13   tensor(14)  Action  4  Return  tensor(31)\n",
      "State  14   tensor(0)  Action  1  Return  tensor(33)\n",
      "State  15   tensor(1)  Action  1  Return  tensor(35)\n",
      "State  16   tensor(2)  Action  2  Return  tensor(36)\n",
      "State  17   tensor(16)  Action  1  Return  tensor(36)\n",
      "State  18   tensor(17)  Action  2  Return  tensor(36)\n",
      "State  19   tensor(31)  Action  1  Return  tensor(36)\n",
      "State  20   tensor(32)  Action  1  Return  tensor(36)\n",
      "State  21   tensor(33)  Action  1  Return  tensor(36)\n",
      "State  22   tensor(34)  Action  1  Return  tensor(38)\n",
      "State  23   tensor(35)  Action  1  Return  tensor(40)\n",
      "State  24   tensor(36)  Action  1  Return  tensor(42)\n",
      "State  25   tensor(37)  Action  1  Return  tensor(44)\n",
      "State  26   tensor(38)  Action  2  Return  tensor(46)\n",
      "State  27   tensor(52)  Action  2  Return  tensor(48)\n",
      "State  28   tensor(66)  Action  2  Return  tensor(50)\n",
      "State  29   tensor(80)  Action  1  Return  tensor(52)\n",
      "State  30   tensor(81)  Action  1  Return  tensor(54)\n",
      "State  31   tensor(82)  Action  4  Return  tensor(56)\n",
      "State  32   tensor(68)  Action  4  Return  tensor(58)\n",
      "State  33   tensor(54)  Action  4  Return  tensor(60)\n",
      "State  34   tensor(40)  Action  4  Return  tensor(62)\n",
      "State  35   tensor(26)  Action  4  Return  tensor(64)\n",
      "State  36   tensor(12)  Action  3  Return  tensor(66)\n",
      "State  37   tensor(11)  Action  3  Return  tensor(68)\n",
      "State  38   tensor(10)  Action  2  Return  tensor(68)\n"
     ]
    }
   ],
   "execution_count": 204
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-03T03:21:00.844415Z",
     "start_time": "2025-07-03T03:21:00.842327Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "a2c430c2405f5441",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5a83b915e88a6e46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
