{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T10:07:42.518868Z",
     "start_time": "2025-07-02T10:07:42.516319Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu\n",
    "from subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "# from sub_go_explore import run_go_explore_for_dataset_generation\n",
    "\n",
    "workspace = \"NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "283ca354729c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T10:07:43.103071Z",
     "start_time": "2025-07-02T10:07:43.086431Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": \"env_data.pkl\",\n",
    "        \"cov_module\": \"Matern\",\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\"\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.03,\n",
    "        \"epochs\": 500,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 300\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f53557d7595a25f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T10:07:44.867450Z",
     "start_time": "2025-07-02T10:07:44.861819Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_file_two_Room_98.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6beefe3c0acb7741",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-01T06:31:11.445753Z",
     "start_time": "2025-07-01T06:22:12.478198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\n",
      "初始時空細胞加入存檔庫: Cell (0, 34), Reward: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a263fade3644b1cbf8b342a003a31b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Go-Exploring (Spacetime):   0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 0.00%\n",
      "當前存檔庫大小: 15\n",
      "找到的最佳獎勵值: 8\n",
      "對應的軌跡長度: 10\n",
      "探索步骤: 2.00%\n",
      "當前存檔庫大小: 2083\n",
      "找到的最佳獎勵值: 47\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 4.00%\n",
      "當前存檔庫大小: 2196\n",
      "找到的最佳獎勵值: 56\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 6.00%\n",
      "當前存檔庫大小: 2246\n",
      "找到的最佳獎勵值: 62\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 8.00%\n",
      "當前存檔庫大小: 2267\n",
      "找到的最佳獎勵值: 63\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 10.00%\n",
      "當前存檔庫大小: 2298\n",
      "找到的最佳獎勵值: 63\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 12.00%\n",
      "當前存檔庫大小: 2298\n",
      "找到的最佳獎勵值: 66\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 14.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 16.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 18.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 20.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 22.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 24.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 26.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 28.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 30.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 32.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 34.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 36.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 38.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 40.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 42.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 44.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 46.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 48.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 50.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 52.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 54.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 56.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 58.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 60.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 62.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 64.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 66.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 68.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 70.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 72.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 74.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 76.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 78.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 80.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 82.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 84.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 86.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 88.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 90.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 92.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 94.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 96.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "探索步骤: 98.00%\n",
      "當前存檔庫大小: 2312\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "\n",
      "--- 探索完成 ---\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "完整的時空細胞存檔庫已保存至: go_explore_archive_spacetime_.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Core parameter setting ---\n",
    "EXPLORATION_STEPS = 500000  # The total number of exploration steps can be adjusted as needed, and the larger the value, the more thorough the exploration\n",
    "K_STEPS = 15             # Random exploration steps starting from selected cells \n",
    "\n",
    "# --- Go-Explore main  ---\n",
    "def run_go_explore_phase1_spacetime(env=GridWorld, params=None):\n",
    "    \"\"\"\n",
    "    執行 Go-Explore 的第一階段探索（使用時空細胞）\n",
    "    \"\"\"\n",
    "    print(\"--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\")\n",
    "    \n",
    "    # 1. 初始化存檔庫 (Archive)\n",
    "    # ---【核心修改】：將細胞 Key 定義為 (時間, 狀態ID) ---\n",
    "    # 結構: { (time, state_id): {'reward': float, 'states': list, 'actions': list, 'times_selected': int} }\n",
    "    archive = {}\n",
    "    \n",
    "    env.common_params[\"batch_size\"]=1\n",
    "    env.initialize()\n",
    "    initial_state_tensor = env.state.clone()\n",
    "    initial_state_id = initial_state_tensor.item()\n",
    "    \n",
    "    # ---【核心修改】：初始細胞現在是 (時間=0, 狀態) ---\n",
    "    initial_cell_key = (0, initial_state_id)\n",
    "    \n",
    "    initial_reward = calculate_submodular_reward([initial_state_tensor], env)\n",
    "    \n",
    "    archive[initial_cell_key] = {\n",
    "        'reward': initial_reward,\n",
    "        'states': [initial_state_tensor],\n",
    "        'actions': [],\n",
    "        'times_selected': 0\n",
    "    }\n",
    "    print(f\"初始時空細胞加入存檔庫: Cell {initial_cell_key}, Reward: {initial_reward}\")\n",
    "    \n",
    "    # 2. 執行 N 次探索循環\n",
    "    pbar = tqdm(total=EXPLORATION_STEPS, desc=\"Go-Exploring (Spacetime)\")\n",
    "    for step in range(EXPLORATION_STEPS):\n",
    "        # 2.1 選擇細胞 (選擇函數無需修改，它通用於任何 key)\n",
    "        cell_key_to_explore_from, selected_cell_data = select_cell_from_archive(archive)\n",
    "        \n",
    "        if selected_cell_data is None:\n",
    "            print(\"錯誤：存檔庫為空，無法繼續探索。\")\n",
    "            break\n",
    "            \n",
    "        archive[cell_key_to_explore_from]['times_selected'] += 1\n",
    "\n",
    "        # 2.2 前往 (Go To) 該細胞狀態\n",
    "        env.initialize()\n",
    "        for action in selected_cell_data['actions']:\n",
    "            env.step(0, torch.tensor([action]))\n",
    "\n",
    "        # 2.3 從該狀態開始，隨機探索 (Explore) k 步\n",
    "        current_states = selected_cell_data['states'][:]\n",
    "        current_actions = selected_cell_data['actions'][:]\n",
    "        \n",
    "        k_STEPS=random.randint(5,K_STEPS)\n",
    "        \n",
    "        for _ in range(k_STEPS):\n",
    "            if len(current_actions) >= params[\"env\"][\"horizon\"] - 1:\n",
    "                break\n",
    "                \n",
    "            random_action = random.randint(0, env.action_dim - 1)\n",
    "            env.step(0, torch.tensor([random_action]))\n",
    "            \n",
    "            new_state_tensor = env.state.clone()\n",
    "            \n",
    "            current_states.append(new_state_tensor)\n",
    "            current_actions.append(random_action)\n",
    "            \n",
    "            # ---【核心修改】：更新存檔庫時使用 (時間, 狀態) 作為 Key ---\n",
    "            new_state_id = new_state_tensor.item()\n",
    "            time_step = len(current_actions) # 當前時間步 = 已執行動作的數量\n",
    "            new_cell_key = (time_step, new_state_id)\n",
    "            \n",
    "            new_reward = calculate_submodular_reward(current_states, env)\n",
    "            \n",
    "            if new_cell_key not in archive or new_reward > archive[new_cell_key]['reward']:\n",
    "                archive[new_cell_key] = {\n",
    "                    'reward': new_reward,\n",
    "                    'states': current_states[:],\n",
    "                    'actions': current_actions[:],\n",
    "                    'times_selected': 0\n",
    "                }\n",
    "        if step% 10000 == 0:\n",
    "            print(f\"探索步骤: {step / EXPLORATION_STEPS * 100:.2f}%\")\n",
    "            print(f\"當前存檔庫大小: {len(archive)}\")\n",
    "            _best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "            _max_reward = _best_trajectory_data['reward']\n",
    "            print(f\"找到的最佳獎勵值: {_max_reward}\")\n",
    "            print(f\"對應的軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "                \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. 探索結束\n",
    "    print(\"\\n--- 探索完成 ---\")\n",
    "    if not archive:\n",
    "        print(\"錯誤：存檔庫為空！\")\n",
    "        return None\n",
    "\n",
    "    best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "    max_reward = best_trajectory_data['reward']\n",
    "    print(f\"找到的最佳獎勵值: {max_reward}\")\n",
    "    print(f\"對應的軌跡長度: {len(best_trajectory_data['states'])}\")\n",
    "\n",
    "    # 4. 保存存檔庫\n",
    "    archive_filename = \"go_explore_archive_spacetime_.pkl\"\n",
    "    with open(archive_filename, \"wb\") as f:\n",
    "        pickle.dump(archive, f)\n",
    "    print(f\"完整的時空細胞存檔庫已保存至: {archive_filename}\")\n",
    "    \n",
    "    return best_trajectory_data\n",
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba5d13d5df5ec9c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-30T12:58:08.178446Z",
     "start_time": "2025-06-30T12:58:08.161619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reward': 68, 'states': [tensor([34]), tensor([33]), tensor([32]), tensor([32]), tensor([31]), tensor([30]), tensor([44]), tensor([58]), tensor([72]), tensor([71]), tensor([70]), tensor([56]), tensor([42]), tensor([28]), tensor([14]), tensor([0]), tensor([1]), tensor([2]), tensor([3]), tensor([17]), tensor([31]), tensor([32]), tensor([33]), tensor([34]), tensor([35]), tensor([36]), tensor([37]), tensor([38]), tensor([24]), tensor([10]), tensor([11]), tensor([12]), tensor([26]), tensor([40]), tensor([54]), tensor([68]), tensor([82]), tensor([81]), tensor([80]), tensor([66])], 'actions': [3, 3, 4, 3, 3, 2, 2, 2, 3, 3, 4, 4, 4, 4, 4, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 4, 4, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4], 'times_selected': 4106}\n"
     ]
    }
   ],
   "source": [
    "print(best_found_trajectory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "547fbebdc9609041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T10:08:24.364032Z",
     "start_time": "2025-07-02T10:08:24.239719Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 300 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 59\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\"go_explore_archive_file_two_Room_98.pkl\",method='top_n', n=300)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31d60be0ee55504a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-02T10:09:55.591014Z",
     "start_time": "2025-07-02T10:09:55.194075Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 範例 1: 採樣 Top 20 ---\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這300條中）獎勵為: 64\n",
      "\n",
      "--- 範例 2: 採樣 Top 5% ---\n",
      "方法: Top-P。從 2312 條軌跡中篩選出最好的前 5.0% (115 條)。\n",
      "抽樣檢查最好的一條軌跡：獎勵=68, 長度=40\n",
      "\n",
      "--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\n",
      "方法: Threshold。從 2312 條軌跡中篩選出 11 條獎勵不低於 68 的軌跡。\n",
      "所有高分軌跡的平均獎勵為: 68.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "# 範例2：獲取獎勵排名前 5% 的軌跡\n",
    "print(\"--- 範例 2: 採樣 Top 5% ---\")\n",
    "top_5_percent_trajectories = sample_excellent_trajectories(method='top_p', p=0.05)\n",
    "if top_5_percent_trajectories:\n",
    "    # 打印其中一條軌跡的詳細信息以供檢查\n",
    "    sample_traj_data = top_5_percent_trajectories[0]\n",
    "    print(f\"抽樣檢查最好的一條軌跡：獎勵={sample_traj_data['reward']}, 長度={len(sample_traj_data['states'])}\\n\")\n",
    "\n",
    "# 範例3：獲取所有獎勵值大於等於 45 的軌跡\n",
    "print(\"--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\")\n",
    "high_reward_trajectories = sample_excellent_trajectories(method='threshold', threshold=68)\n",
    "if high_reward_trajectories:\n",
    "    print(f\"所有高分軌跡的平均獎勵為: {np.mean([d['reward'] for d in high_reward_trajectories]):.2f}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
