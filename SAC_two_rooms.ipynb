{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf30926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3628d00b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda-python-3.11\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cbbd709",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755e8fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class SAC:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        critic_1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        critic_2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce7c735d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb72bb47",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = H-1\n",
    "action_dim = 5\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "target_entropy = -1\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b87aa8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    replay_buffer = ReplayBuffer(buffer_size)\n",
    "    agent = SAC(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, alpha_lr,\n",
    "                target_entropy, tau, gamma, device)\n",
    "\n",
    "    params[\"common\"][\"batch_size\"]=1      #采样的batch大小\n",
    "    return_list = []\n",
    "    for i in range(10):\n",
    "        with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "            for i_episode in range(int(num_episodes / 10)):\n",
    "                mat_state = []\n",
    "                mat_return = []\n",
    "                env.initialize()\n",
    "                mat_state.append(env.state)\n",
    "                init_state = env.state\n",
    "                for h_iter in range(H-1):\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "\n",
    "                    probs = agent.actor(batch_state.to(device))\n",
    "                    actions_dist = torch.distributions.Categorical(probs)\n",
    "                    actions = actions_dist.sample()\n",
    "\n",
    "                    env.step(h_iter, actions.cpu())\n",
    "\n",
    "                    mat_state.append(env.state)  # s+1\n",
    "                    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "                    if h_iter == 0:\n",
    "                        reward = mat_return[-1]\n",
    "                    else:\n",
    "                        reward = mat_return[-1]-mat_return[-2]\n",
    "\n",
    "                    if h_iter == H-2:\n",
    "                        next_state = batch_state\n",
    "                        done = 1\n",
    "                    else:\n",
    "                        next_state = append_state(mat_state, H-1)\n",
    "                        done = 0\n",
    "\n",
    "                    for j in range(params[\"common\"][\"batch_size\"]):\n",
    "                        replay_buffer.add(batch_state[j],actions[j],reward[j],next_state[j],done)\n",
    "\n",
    "                    # 当buffer数据的数量超过一定值后,才进行Q网络训练\n",
    "                    if replay_buffer.size() > minimal_size:\n",
    "                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                        transition_dict = {\n",
    "                            'states': b_s,\n",
    "                            'actions': b_a,\n",
    "                            'next_states': b_ns,\n",
    "                            'rewards': b_r,\n",
    "                            'dones': b_d\n",
    "                        }\n",
    "                        agent.update(transition_dict)\n",
    "\n",
    "                return_list.append(mat_return[-1].float().mean())\n",
    "\n",
    "                if (i_episode + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'episode':\n",
    "                        '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                pbar.update(1)\n",
    "    return agent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f554213d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:01<00:00,  9.06it/s, episode=10, return=18.500]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 11.24it/s, episode=20, return=18.600]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:11<00:00,  1.18s/it, episode=30, return=20.100]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it, episode=40, return=27.800]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:26<00:00,  2.62s/it, episode=50, return=27.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:26<00:00,  2.63s/it, episode=60, return=28.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:25<00:00,  2.58s/it, episode=70, return=28.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:25<00:00,  2.53s/it, episode=80, return=28.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:27<00:00,  2.79s/it, episode=90, return=28.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:28<00:00,  2.83s/it, episode=100, return=28.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 11.19it/s, episode=10, return=17.600]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 11.23it/s, episode=20, return=17.000]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:13<00:00,  1.35s/it, episode=30, return=17.300]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:28<00:00,  2.88s/it, episode=40, return=19.200]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:28<00:00,  2.81s/it, episode=50, return=20.200]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:27<00:00,  2.75s/it, episode=60, return=21.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:28<00:00,  2.85s/it, episode=70, return=20.600]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:27<00:00,  2.80s/it, episode=80, return=19.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:29<00:00,  2.98s/it, episode=90, return=19.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:27<00:00,  2.79s/it, episode=100, return=19.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 11.38it/s, episode=10, return=20.500]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 11.73it/s, episode=20, return=21.000]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:12<00:00,  1.25s/it, episode=30, return=18.200]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:27<00:00,  2.80s/it, episode=40, return=16.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:27<00:00,  2.77s/it, episode=50, return=16.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:28<00:00,  2.87s/it, episode=60, return=16.700]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:30<00:00,  3.03s/it, episode=70, return=19.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:29<00:00,  2.93s/it, episode=80, return=19.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:29<00:00,  2.96s/it, episode=90, return=19.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:28<00:00,  2.88s/it, episode=100, return=19.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 11.50it/s, episode=10, return=12.300]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:01<00:00,  9.62it/s, episode=20, return=11.300]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:12<00:00,  1.24s/it, episode=30, return=15.800]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:28<00:00,  2.82s/it, episode=40, return=28.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:28<00:00,  2.86s/it, episode=50, return=24.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:27<00:00,  2.79s/it, episode=60, return=20.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:26<00:00,  2.62s/it, episode=70, return=20.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:25<00:00,  2.60s/it, episode=80, return=20.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:26<00:00,  2.64s/it, episode=90, return=20.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:26<00:00,  2.63s/it, episode=100, return=20.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 12.55it/s, episode=10, return=21.100]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 11.65it/s, episode=20, return=21.300]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:11<00:00,  1.19s/it, episode=30, return=20.700]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:25<00:00,  2.60s/it, episode=40, return=19.200]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:25<00:00,  2.60s/it, episode=50, return=21.200]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:26<00:00,  2.63s/it, episode=60, return=20.700]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:26<00:00,  2.70s/it, episode=70, return=21.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:25<00:00,  2.56s/it, episode=80, return=21.600]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:24<00:00,  2.41s/it, episode=90, return=22.200]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:24<00:00,  2.40s/it, episode=100, return=21.600]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 14.06it/s, episode=10, return=4.800]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 13.84it/s, episode=20, return=5.400]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:11<00:00,  1.10s/it, episode=30, return=10.600]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it, episode=40, return=16.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:23<00:00,  2.37s/it, episode=50, return=16.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:24<00:00,  2.44s/it, episode=60, return=16.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:24<00:00,  2.48s/it, episode=70, return=16.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:24<00:00,  2.41s/it, episode=80, return=16.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:24<00:00,  2.45s/it, episode=90, return=16.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:24<00:00,  2.40s/it, episode=100, return=16.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 13.48it/s, episode=10, return=16.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 12.34it/s, episode=20, return=14.600]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:11<00:00,  1.11s/it, episode=30, return=16.900]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:28<00:00,  2.84s/it, episode=40, return=26.600]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:28<00:00,  2.89s/it, episode=50, return=28.400]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:26<00:00,  2.66s/it, episode=60, return=29.600]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:24<00:00,  2.45s/it, episode=70, return=26.200]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:24<00:00,  2.46s/it, episode=80, return=26.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:25<00:00,  2.55s/it, episode=90, return=27.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:24<00:00,  2.45s/it, episode=100, return=28.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 12.14it/s, episode=10, return=17.100]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 12.46it/s, episode=20, return=16.900]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:11<00:00,  1.16s/it, episode=30, return=17.700]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:27<00:00,  2.73s/it, episode=40, return=20.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:25<00:00,  2.53s/it, episode=50, return=20.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:24<00:00,  2.43s/it, episode=60, return=20.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:30<00:00,  3.01s/it, episode=70, return=20.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:28<00:00,  2.90s/it, episode=80, return=20.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:24<00:00,  2.41s/it, episode=90, return=20.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:24<00:00,  2.48s/it, episode=100, return=20.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 12.76it/s, episode=10, return=20.300]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 12.37it/s, episode=20, return=20.100]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:10<00:00,  1.09s/it, episode=30, return=24.300]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:24<00:00,  2.41s/it, episode=40, return=26.700]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.28s/it, episode=50, return=20.600]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:21<00:00,  2.18s/it, episode=60, return=22.600]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:21<00:00,  2.20s/it, episode=70, return=23.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it, episode=80, return=24.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=90, return=23.400]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:22<00:00,  2.26s/it, episode=100, return=24.000]\n",
      "Iteration 0: 100%|██████████| 10/10 [00:00<00:00, 12.52it/s, episode=10, return=20.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:00<00:00, 13.97it/s, episode=20, return=19.600]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:09<00:00,  1.01it/s, episode=30, return=17.900]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:22<00:00,  2.22s/it, episode=40, return=16.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it, episode=50, return=16.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:22<00:00,  2.27s/it, episode=60, return=16.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:22<00:00,  2.22s/it, episode=70, return=16.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:22<00:00,  2.24s/it, episode=80, return=16.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it, episode=90, return=16.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:22<00:00,  2.21s/it, episode=100, return=16.000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 20.00±3.35, max: 21.60±4.10, mean: 21.01±4.08, median: 21.00±4.10\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    agent = train()\n",
    "    params[\"common\"][\"batch_size\"]=3000\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample().cpu()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfe030ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([24])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19247be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 29), (6, 28), (7, 42), (8, 56), (9, 70), (10, 84), (11, 84), (12, 84), (13, 84), (14, 84), (15, 85), (16, 86), (17, 87), (18, 87), (19, 87), (20, 87), (21, 87), (22, 87), (23, 87), (24, 87), (25, 87), (26, 87), (27, 87), (28, 87), (29, 87), (30, 87), (31, 87), (32, 87), (33, 87), (34, 87), (35, 87), (36, 87), (37, 87), (38, 87), (39, 87)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e19404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "y [2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGMAAAJdCAYAAACWDbrjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAAPuFJREFUeJzt3Qu41VWdN/Df4XBRDFC0EgFFU7yilYqvZKloMT5qFzPKMRNjlEzJchrTbCCnC1qm1oR4GS5PM/pSU0OaM8LYRbqYHbQs7OItLEBM7c2DSnE57PdZfzoECOj/D6y9z+bzeZ7tvpx9zl7ne7aHs797rfVvqdVqtQAAAAAgi255HgYAAACARBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAFR29913R0tLS3EOAMDLo4wBgAYxY8aMoti477771t72P//zP/HJT34y6u36668vxtdofv3rX8ff/d3fxSte8Yro379/nHXWWfH0009neexf/epXxc/m8ccfj3qbMmVKvOtd74o999yzeA6NGTNmo/dbsmRJXHrppXH88cdHnz59Nluk/e///m+MHTs2DjnkkGhtbY0hQ4Zs4+8CALYfyhgAaGCpjLniiisatox505veFH/+85+L89wWLVpUPO6jjz4an/3sZ+OjH/1o/Pd//3e8+c1vjhUrVmQpY9LPphHKmKuuuiq++93vxsEHHxzdu3ff5P0eeuih4r6LFy+OYcOGbfZr3nrrrcWpX79+sccee2yDUQPA9mvT/1oDAE2pVqvFX/7yl9hxxx23+Gt169Ytdthhh6iHVMC88MILcf/99xczQpLhw4cXZUwqjs4777xSX2/VqlWxevXq6NmzZ9RT+p522mmnUp8zd+7ctbNi0iyhTTn88MPjj3/8YzGL6Otf/3oxm2Zz+d58883Ro0ePOOWUU+LBBx8sNSYAYNPMjAGABpWWmkyePLm4nF5kd546peLguuuuK2ZDpELk1a9+dYwbNy7+9Kc/rfd10vKS9GJ6zpw5ccQRRxQlzI033lh8bPr06TFy5Mh41ateFb169YqDDjqoWPKy4ef/8pe/LF7wd47huOOO2+yeMf/5n/9ZvPBPj7XbbrvFe9/73mI2xobfXyoO0u1vf/vbi8uvfOUrixkuHR0dL5nPN77xjeL76ixikhNPPDGGDh0aX/va1zb7uWk2Sxr31VdfXWT4mte8pvj+02yX5De/+U2cfvrpRWmRsk253X777Ws/P5U9nUVGWvLTmUtnDunyxpaXpSzXXULUuTQtZfvBD36w+DkMGjSo+FjKOC0RSmNKj9G7d+8YOHBgfO5zn3vR191rr73We25sSlqalL6nlyPNhklFDACw9ZkZAwANKhUrTzzxRNx1113x7//+7xv9eHoxf84558SHPvShWLBgQXz5y1+On/3sZ/GjH/1ovRfSaXnKGWecUXzOueeeG/vvv39xeypeUpnz1re+tVje8q1vfasoBVLRc8EFFxT3SWXF+PHji7Lk8ssvL25Lxc+mdI7pyCOPjEmTJsUf/vCH+OIXv1iMKY1t5513XnvfVLqMGjUqjjrqqKIY+fa3vx1f+MIXinLk/PPP3+RjpALnqaeeKkqSDaXZMWl518uRyqg0SyjNokllTCoqUvH0hje8oSg+0v4qaZZKKndSYZQKoHe84x3F8qiU+Ze+9KX4+Mc/HgceeGDx9TrPy0qZpyJqwoQJxcyYTqlYS3vinHbaaTF69OhiNsvHPvaxYonRSSedVOmxAID6U8YAQIM6+uiji1keqYxJM0vW9cMf/jD+7d/+LW655Zb4+7//+7W3pxkU6cV7mpmy7u1pX5XZs2cXxce60oyMdZcrXXjhhcXnX3PNNWvLmFRCfOITn1g7w2VzVq5cWZQFaUbH97///bVLmI455phiFsu111673h44qQh597vfHf/8z/9cXP/ABz4Qr3/962Pq1KmbLWPSRrTJgAEDXvSxdNv/+3//L5YvX14ULC+170zKJhUh686uSbNt5s2bt/bzU1mSvof0vaUyZp999ok3vvGNRRmTlkV1zhSqKpVA3/nOd4qNcteVyrivfOUrxcbESdpQN82CSfkoYwCg67JMCQC6oFS2pI1VUxHwzDPPrD2lpUFpBsv3vve99e6/9957v6iISdYtYtrb24uvceyxx8Zvf/vb4npZ6UhQacZKKi/W3Uvm5JNPjgMOOKDYYHdDqYBZVyo50uNvTto0ONlY2dL5uJ332Zx3vvOd6xUxqcRJG+GmWSjPPffc2lzTPispv0ceeeRFy622hjRbacMiJkk/y3ULsLSfTZr581L5AACNzcwYAOiCUimQypK0x8jGpEJkwzJmY9LSoYkTJ8aPf/zjWLZs2XofS18/FT5l/O53vyvOO5dBrSuVMWlGz4bFybplSLLLLru8aN+bTZVIafbLhtJsm3Xvszkb5pJmyaQNjtNMnc7ZOhvLNi1h2po29fNJ+8dsuBdMyucXv/jFVn18ACAvZQwAdEFpT5dUxKRlShuzYcGxsWLiscceixNOOKEoSdKypMGDBxczL9J+K2k5UXqMbW1js0Fejs7lSZ3LldaVbkvLfl5qidLGcun8ntMmwhubSZTsu+++UdWmNibeVHG0qXxSYQQAdF3KGABoYJs6Qk7a4DZtdps2mq16iOq0WW+aWZKOErTuEYk2XOK0uXFsKO1n0rlhcDpK07rSbZ0f31JpZkoqnNKyqA21tbXFa1/72kpfN+0Fk6TNj9PeMZuzuUzS7JVnn312vdtWrFix0fIIANj+2DMGABpYOpJPsuEL+7SnSZpl8alPfepFn7Nq1aoX3X9zsy7WnWWRlialIwxtbBwv52umoxulGTs33HDDekuI7rzzzvj1r39d7B2ztaT9Xu64445YuHDh2tvSJrgPP/zw2sNOl5XGnjbjTYf+3lhx8vTTT7/kz6azLEsbGK/rpptuelmH7AYAmp+ZMQDQwNKGvEk6jHJaNpMKlPe85z3FJrvpMNXp0NEPPPBAvOUtbylmc6S9ZNLmvulQ0qeffvpmv3b6nLQs6dRTTy2+1vPPPx8333xzUUhsWESkcaTDYH/6058ulumk+2w48yVJY7jqqquKQ1unMabDaXce2nrIkCHxkY98ZKtlkw4pnb7XdASpiy66qBj/5z//+eKwz+nxq5o8eXJx5KT0ddLGumm2TPoe0r466ehLP//5z4v7pdk36eeRvt9UYqVlUSmTlM0//MM/FBsTp8IobbKcPmfOnDnFEam2hTTLqXNc6YhWaU+Z9LNK0mHLDz300LX37bw9HcI7SYdN79zLJx01q1P6GmnWVOdeOul77Pzcww47rHjeAAAV1QCAhjB9+vQ0RaU2b968tbetWrWqNn78+NorX/nKWktLS/Hxdd100021ww8/vLbjjjvW+vTpUxs2bFjtkksuqT3xxBNr77PXXnvVTj755I0+5u2331479NBDazvssENtyJAhtauuuqo2bdq04nEWLFiw9n5PPvlk8TXSY6SPHXvsscXt3/ve94rr6XxdX/3qV2uve93rar169ar179+/duaZZ9YWLVq03n3OPvvs2k477fSiMU2cOPFF3+emPPjgg7W3vOUttd69e9d23nnn4nHSWF9K+t7SY3z+85/f6Mcfe+yx2vve977a7rvvXuvRo0dt4MCBtVNOOaX29a9/fb373XzzzbV99tmn1traul4OHR0dtY997GO13XbbrRjbqFGjao8++mjxs0jf9+Z+5p1SxgcffPCLbk+fn77Ohrelr7OxU3qMdW3qfhtm3jm2jZ3W/R4AgPJa0n+qFjkAAAAAlGPPGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJBR98hs9erV8cQTT0SfPn2ipaUl98MDAAAAbBO1Wi2ee+652GOPPaJbt271L2MmT55cnFasWBGPPfZYrocFAAAAyGrhwoUxaNCgTX68pZZqm4za29tj5513jra2thgwYEDOh+6ylixZEsOHD5dZSXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bVyK08mW1Zbs8++2z069evcZYpdS5NSj/MzbVEvJjMqpFbeTKrRm7lyawauZUns2rkVp7MqpFbeTKrRm7lyayal9qWxQa+AAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYANJGOjo4YMWJEnHbaaevd3t7eHoMHD47LL7+8bmNrZHIrT2bVyK08mVUjN4DGpowBaCKtra0xY8aMmD17dtxyyy1rbx8/fnz0798/Jk6cWNfxNSq5lSezauRWnsyqkRtAY+te7wEAsHUNHTo0rrzyyuIP7pEjR0ZbW1vMnDkz5s2bFz179qz38BqW3MqTWTVyK09m1cgNoHEpYwCaUPrDe9asWXHWWWfF/PnzY8KECXHYYYfVe1gNT27lyawauZUns2rkBtCYlDEATailpSWmTJkSBx54YAwbNiwuvfTSeg+pS5BbeTKrRm7lyawauQE0yZ4xixcvjve+972x6667xo477lj8Ur/vvvu2zegAqGzatGnRu3fvWLBgQSxatKjew+ky5FaezKqRW3kyq0ZuAF28jPnTn/4Ub3jDG6JHjx5x5513xq9+9av4whe+ELvsssu2GyFARPzymV/G2Dlji3Ne2j333BPXXntt3HHHHTF8+PAYO3Zs1Gq1eg+r4cmtPJlVI7fyZFaN3ACaYJnSVVddVRwKb/r06Wtv23vvvaNLSe8GPPJIxH77RQwatP7H5s2L+MEPIt74xogjj/zbfV/xiojnn9/452wPNpcZGyez6pm98ELEww+nXQcjdtppTYYPPBC3P3BNtA1+Or419eI4+L///Lf/Lw85JKJPn4glSyJGj4445ZTY3i1btizGjBkT559/fhx//PHF7+k0i/GGG24obmPj5FaezKqRW3kyq0ZuAE1Sxtx+++0xatSoeNe73hVz586NgQMHxgc/+ME499xzo0uYOjXivPMiVq+O6NYt4qabIsaOLT5UO/vsiK98JVrS5Yjo+D//J1rb2qJl9erienF7t26x4vop0XHO+7MO+88rO6KlR6/ifNmKVVkfu3X6tOj5wfPX5FCn77+r5SazLcys8/+3iFiya4/40ytai/vMvnhI8Svrzh1/H2/9/ePFx3d5viP2SAVqp3//94gRIyJ+9KPYnl122WXFu57pCBrJkCFD4uqrr46PfvSjcdJJJxXXeTG5lSezauRWnsyqkRtA42qplZinuMMOOxTnF198cVHIpMPiXXTRRUW7fnYqMzZi+fLlxanT0qVLi9k1CxcujEE5Zwykd9332mtNEdOptTXi8cej9sQTEUcdVbwA7NT5gnBDq1q6xTEfmBZP9t0tmt3uS5+JH91wTrSu8xTZnr7/KmS2dTLrNGzGIX+7kj7e0vK387+aP+bBF3/Rb30r6wyZtP6+Lr/XNiIV5SeccELcfffdccwxx6z3sVSmr1q1Kr797W8XGzrWUyNllsitPJlVI7fyZFaN3JqXzKqRW3ky27Lc2tvbo2/fvltnZszq1avjiCOOiM9+9rPF9de97nXx4IMPbraMmTRpUlxxxRVRd2n5w7pFTNLREfHoo7Gy7b7oucHdN/XPUvfa6hjy7BPbxQvrvf/0xIteIG9P338VMts6mXWadOPC+MQ/DIqO1pa/FTB/PW/tqMWn/20TmxDOnr3dLlc69thjiz+wN2bOnDnZx9NVyK08mVUjt/JkVo3cABpbqTJmwIABcdBBB613WzpM3je+8Y3NTo9MM2k2nBmTXdp3Ii1N2nBmzL77RkePXi+aCbOpmTG11taY/ukzopaxGUxHsNp///3joYceKpaG5dKy6OCofe0TxdKRen7/XSk3mW2dzDqd8uP22OeJ5fHuK/Z90cdu/ZfH4qDf/WXjX/Tv/m5bDBUAACB/GZOOpJRepK3r4Ycfjr3S8p9N6NWrV3Gqu/RCOO0RM27cmhkxqYi58cbi9tqrdo+vH3JCnP7gd9YWMC1p34mf/GTNfTu1tkbLjTfGjvvkXV+7Y4/WqK1cXpz37lnqR7Zl0ve5QWb1+P67VG4y2/LMNqFldS1q3VrWnm9S+n93O50VAwAAdA2lXm195CMfiREjRhTLlEaPHh1tbW1x0003FacuIW3WO2pUsTQpzYhZ9yg3/3TyR+Irrzs5vnHo6uh57Jv+djSldN90RJd0hJcNPme7sJnM2ASZbVlm6QhJndm94hXRf1C/2PWeD8buz6+O017YJ/6rz+PxZLcXov/hb4wYsiIizdZLazGffDLi9NMVMQAAQHOVMUceeWTMmjWrWHr0L//yL8Xh8a677ro488wzo8tIL4w38eJ4/h5DY9WHRkXPzhkBm7nvdkUO5clsq2W2e0T8795zo0e3HsUmg++q1WLl6pXR8/wNd3oCAADoGkqvQzjllFOKE0AuPVv/VrykQmbd6wAAAF1Nt3oPAAAAAGB7oowBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGXWPOlmyZEk0ij+v7Fh7efHixbFjj9ZoJJ1ZNVJmXYHcypNZNXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bVvNy8Wmq1Wi0yGD16dNx2222RHm7lypXRSFp69Io9L/5Gcfn317wzaiuX13tIAAAAQBfV3t4effv2rX8Z02np0qXRr1+/aGtriwEDBkSjzIx5840PFpfvGndIQ86MGT58eENl1hXIrTyZVSO38mRWjdzKk1k1citPZtXIrTyZVSO38mS2Zbm9VBlTt2VK6Yc5aNCgaATLVqyKiDVlzMCBA6N3z7rF0mUy60rkVp7MqpFbeTKrRm7lyawauZUns2rkVp7MqpFbeTLbNmzgCwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMQBPp6OiIESNGxGmnnbbe7e3t7TF48OC4/PLL6za2Ria38mRWjdzKk1k1cgNobMoYgCbS2toaM2bMiNmzZ8ctt9yy9vbx48dH//79Y+LEiXUdX6OSW3kyq0Zu5cmsGrkBNLbu9R4AAFvX0KFD48orryz+4B45cmS0tbXFzJkzY968edGzZ896D69hya08mVUjt/JkVo3cABqXMgagCaU/vGfNmhVnnXVWzJ8/PyZMmBCHHXZYvYfV8ORWnsyqkVt5MqtGbgCNSRkD0IRaWlpiypQpceCBB8awYcPi0ksvrfeQugS5lSezauRWnsyqkRtAE+wZ88lPfrL4hb7u6YADDth2owOgsmnTpkXv3r1jwYIFsWjRonoPp8uQW3kyq0Zu5cmsGrkBNMEGvgcffHAsWbJk7emHP/zhthkZAJXdc889ce2118Ydd9wRw4cPj7Fjx0atVqv3sBqe3MqTWTVyK09m1cgNoEnKmO7du8fuu+++9rTbbrtFV9eyaFEc/btfxO5Ln6n3UAC22LJly2LMmDFx/vnnx/HHHx9Tp04tNm284YYb6j20hia38mRWjdzKk1k1cgNoojLmkUceiT322CP22WefOPPMM+P3v/99dGlTp8YO+70m/u/Mj8ePbjgnWqdPq/eIALbIZZddVrzrmY6gkQwZMiSuvvrquOSSS+Lxxx+v9/AaltzKk1k1citPZtXIDaBJypijjjoqZsyYEbNnzy42AkvrTt/4xjfGc889t8nPWb58eSxdunS9U8NIa2bPOy9aVq8urrbWatHzgg+uuR2gC5o7d25Mnjw5pk+fXuwP0GncuHExYsQI09M3QW7lyawauZUns2rkBtBER1M66aST1l4+9NBDi3Jmr732iq997WvFL/SNmTRpUlxxxRXRkB55JOKvRUynlo6OiEcfjRg0qG7DAqjq2GOPjVWrVm30Y3PmzMk+nq5CbuXJrBq5lSezauQG0GTLlNa18847x9ChQ+PRVF5sZnpke3v72tPChQujYey3X0S39SOotbZG7Ltv3YYEAAAANLctKmOef/75eOyxx2LAgAGbvE+vXr2ib9++650aRpr9ctNNawqYiFjV0i1WTL7erBgAAACgMcqYj370o8X607ThVzpM3jve8Y5obW2NM844I7qssWPjLw8/Gu8547NxzAemRcc576/3iAAAAIAmVmrPmEWLFhXFyx//+Md45StfGcccc0zce++9xeWurDZoUNy756H1HgYAAACwHShVxsycOXPbjQQAAABgO7BFe8YAAAAAUI4yBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGTUPepkyZIl0Sj+vLJj7eXFixfHjj1ao5F0ZtVImXUFcitPZtXIrTyZVSO38mRWjdzKk1k1citPZtXIrTyZVfNy82qp1Wq1yGD06NFx2223RXq4lStXRiNp6dEr9rz4G8Xl31/zzqitXF7vIQEAAABdVHt7e/Tt27f+ZUynpUuXRr9+/aKtrS0GDBgQjTIz5s03PlhcvmvcIQ05M2b48OENlVlXILfyZFaN3MqTWTVyK09m1citPJlVI7fyZFaN3MqT2Zbl9lJlTN2WKaUf5qBBg6IRLFuxKiLWlDEDBw6M3j3rFkuXyawrkVt5MqtGbuXJrBq5lSezauRWnsyqkVt5MqtGbuXJbNuwgS8AAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQBNpKOjI0aMGBGnnXbaere3t7fH4MGD4/LLL6/b2BqZ3MqTWTVyK09m1cgNoLEpYwCaSGtra8yYMSNmz54dt9xyy9rbx48fH/3794+JEyfWdXyNSm7lyawauZUns2rkBtDYutd7AABsXUOHDo0rr7yy+IN75MiR0dbWFjNnzox58+ZFz5496z28hiW38mRWjdzKk1k1cgNoXMoYgCaU/vCeNWtWnHXWWTF//vyYMGFCHHbYYfUeVsOTW3kyq0Zu5cmsGrkBNCZlDEATamlpiSlTpsSBBx4Yw4YNi0svvbTeQ+oS5FaezKqRW3kyq0ZuAE24Z0ya9ph+wX/4wx/eeiMCYKuYNm1a9O7dOxYsWBCLFi2q93C6DLmVJ7Nq5FaezKqRG0ATlTFpremNN94Yhx566NYdEQBb7J577olrr7027rjjjhg+fHiMHTs2arVavYfV8ORWnsyqkVt5MqtGbgBNVMY8//zzceaZZ8bNN98cu+yyS2wX0rsI3/vemnOABrZs2bIYM2ZMnH/++XH88cfH1KlTi00bb7jhhnoPraHJrTyZVSO38mRWjdwAmqyMueCCC+Lkk0+OE088MbYLU6dG7LVXxMiRa87TdYAGddlllxXveqalpMmQIUPi6quvjksuuSQef/zxeg+vYcmtPJlVI7fyZFaN3ACaqIxJh8P76U9/GpMmTXpZ91++fHksXbp0vVOXkmbCnHdexOrVa66n83HjzJABGtLcuXNj8uTJMX369GJ/gE7jxo2LESNGmJ6+CXIrT2bVyK08mVUjN4AmOprSwoUL46KLLoq77rordthhh5f1Oam0ueKKK6LLeuSRvxUxnTo6Ih59NGLQoHqNCmCjjj322Fi1atVGPzZnzpzs4+kq5FaezKqRW3kyq0ZuAE00M+b++++Pp556Kl7/+tdH9+7di1Nq3b/0pS8VlztSSbGR6ZHt7e1rT6nQ6VL22y+i2wYxtbZG7LtvvUYEAAAAbC8zY0444YSYP3/+eredc845ccABB8THPvaxaE0lxQZ69epVnLqsNPvlppvWLE1KZVP6Hm+80awYAAAAYNuXMX369IlDDjlkvdt22mmn2HXXXV90e1MZOzZi1Kg1S5PSjBhFDAAAAJCjjNmupQJGCQMAAADUu4y5++67t/RLAAAAAGw3Sh/aGgAAAIDqlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgo+5RJ0uWLIlG8eeVHWsvL168OHbs0RqNpDOrRsqsK5BbeTKrRm7lyawauZUns2rkVp7MqpFbeTKrRm7lyayal5tXS61Wq0UGo0ePjttuuy3Sw61cuTIaSUuPXrHnxd8oLv/+mndGbeXyeg8JAAAA6KLa29ujb9++9S9jOi1dujT69esXbW1tMWDAgGiUmTFvvvHB4vJd4w5pyJkxw4cPb6jMugK5lSezauRWnsyqkVt5MqtGbuXJrBq5lSezauRWnsy2LLeXKmPqtkwp/TAHDRoUjWDZilURsaaMGThwYPTuWbdYukxmXYncypNZNXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bbhg18AQAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAIBGLWOmTJkShx56aPTt27c4HX300XHnnXduu9EBAAAAbM9lzKBBg+LKK6+M+++/P+67774YOXJkvO1tb4tf/vKX226EAAAAAE2ke5k7n3rqqetd/8xnPlPMlrn33nvj4IMPjqa0aFHEI49E7LdfaqPqPRoAAABge90zpqOjI2bOnBkvvPBCsVypKU2dGrHXXhEjR645T9cBAAAAcs2MSebPn1+UL3/5y1/iFa94RcyaNSsOOuigTd5/+fLlxanT0qVLo8vMiDnvvIjVq9dcT+fjxkWMGmWGDAAAAJBvZsz+++8fDzzwQPzkJz+J888/P84+++z41a9+tcn7T5o0Kfr167f2NHjw4OgS0tKkziKmU0dHxKOP1mtEAAAAwPZYxvTs2TP23XffOPzww4ui5bDDDosvfvGLm7z/ZZddFu3t7WtPCxcujC4h7RHTbYN4Wlsj9t23XiMCAAAAtuc9YzqtXr16vWVIG+rVq9faQ2F3nrqEtBTpppvWFDBJOr/xRkuUAAAAgHx7xqRZLieddFLsueee8dxzz8Wtt94ad999d8yZMyea0tixa/aISUuT0owYRQwAAACQs4x56qmn4n3ve18sWbKk2P/l0EMPLYqYN7/5zdG0UgGjhAEAAADqUcZMdWhnAAAAgPruGQMAAADAy6eMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABl1jzpZsmRJNIo/r+xYe3nx4sWxY4/WaCSdWTVSZl2B3MqTWTVyK09m1citPJlVI7fyZFaN3MqTWTVyK09m1bzcvFpqtVotMhg9enTcdtttkR5u5cqV0UhaevSKPS/+RnH599e8M2orl9d7SAAAAEAX1d7eHn379q1/GdNp6dKl0a9fv2hra4sBAwZEo8yMefONDxaX7xp3SEPOjBk+fHhDZdYVyK08mVUjt/JkVo3cypNZNXIrT2bVyK08mVUjt/JktmW5vVQZU7dlSumHOWjQoGgEy1asiog1ZczAgQOjd8+6xdJlMutK5FaezKqRW3kyq0Zu5cmsGrmVJ7Nq5FaezKqRW3ky2zZs4AsAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAANGoZM2nSpDjyyCOjT58+8apXvSre/va3x0MPPbTtRgcAAACwPZcxc+fOjQsuuCDuvffeuOuuu2LlypXxlre8JV544YVtN0IAoLEt/mnEjFPWnAMA8JK6RwmzZ89e7/qMGTOKGTL3339/vOlNb4quqmXRojj6d7+IBbvsUe+hdF2LFkU88kjEfvtFDBpU79E0LjltGflVI7fqZPfy/HxmxOM/iPifL0ScdJWsAAC2Zhmzofb29uK8f//+0WVNnRo7nHde/N/Vq6OjpSWWHfznWHbuP0Qj+fPKjmjp0as4X7ZiVTSa1unToucHz4+W1auj1q1brLh+SnSc8/56D6vhcmvUnBo5s66Sn9yaK7NGzq6RcmtpXxix7I8RLS2xw323REu68aHbIz75tYhPXB5xxvsjdt6zrmMEAGhULbVarVblE1evXh1vfetb49lnn40f/vCHm7zf8uXLi1OnpUuXxuDBg2PhwoUxqN7vnKV3PPfaK30za29a1dItjvnAtHiy7251HVpXsfvSZ+JHN5wTres8jWT4YnLaMvKrRm7Vye6lPb7D36+9nP6UaGlpSReKcmatT65506aeFi1a1Dh/d3QhcitPZtXIrTyZVSO38mS2ZbmlySt9+/bd+kdTSnvHPPjggzFz5syX3PS3X79+a09pUA0jTT1fp4hJutdWx5Bnn6jbkLqavf/0xHovVhIZvpictoz8qpFbdbJ7aRet+GCsrLUWl4siZs2FNecdtYgDP1LH0QEANOEypQsvvDDuuOOO+P73v/+SDdlll10WF1988YtmxjSEtAdAt27rFTK11taY/ukzotZAzd/ixYtj//33L45cNXDgwGgkLYsOjtrXPlFM42+0DBspt0bOqVEz60r5ya15Mmv07Bort1Gx6snTo8e0kS/+0PS/RNzbWEt+AQC6bBmTpiGPHz8+Zs2aFXfffXfsvffeL/k5vXr1Kk4NKf1BfdNNEePGRXR0RLS2RsuNN8aO+wyJRrJjj9aorVxenPfuuUXb/Gx9KasGzbChcmvgnBo2sy6Un9yaKLMGz67hcuu+ZmZMFDvG1CJW1yK6tURcfrlNfAEANqN72aVJt956a9x2223Rp0+fePLJJ4vb0/KjHXfcMbqksWMjRo2KePTRiH339cdjFTJ8eeS0ZeRXjdyqk91L2+mVEa94VUTfgRF7vzXiF7dErHo24u/NigEA2GplzJQpU4rz4447br3bp0+fHmPGjIkuK/2B7Y/sLSPDl0dOW0Z+1citOtltXr+BER9+MKK155r9Yk78SETHiojuDTojFgCgqy5TAgBYa93iJRUyihgAgJdU+WhKAAAAAJSnjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZdY86WbJkSb0eusvpzEpm5citPJlVI7fyZFaN3MqTWTVyK09m1citPJlVI7fyZFbNy82rpVar1SKD0aNHx2233Rbp4VauXJnjIQEAAACya29vj759+9a/jOm0dOnS6NevX7S1tcWAAQNyPnSXbtaGDx8us5LkVp7MqpFbeTKrRm7lyawauZUns2rkVp7MqpFbeTLbstxeqoyp2zKl9MMcNGhQvR6+S5JZNXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bVyK08mW0bNvAFAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgagiXR0dMSIESPitNNOW+/29vb2GDx4cFx++eV1G1sjk1t5MqtGbuXJrBq5ATQ2ZQxAE2ltbY0ZM2bE7Nmz45Zbbll7+/jx46N///4xceLEuo6vUcmtPJlVI7fyZFaN3AAaW/d6DwCArWvo0KFx5ZVXFn9wjxw5Mtra2mLmzJkxb9686NmzZ72H17DkVp7MqpFbeTKrRm4AjUsZA9CE0h/es2bNirPOOivmz58fEyZMiMMOO6zew2p4citPZtXIrTyZVSM3gMakjAFoQi0tLTFlypQ48MADY9iwYXHppZfWe0hdgtzKk1k1citPZtXIDaBJ9oz5/ve/H6eeemrssccexS/3b37zm9tmZABskWnTpkXv3r1jwYIFsWjRonoPp8uQW3kyq0Zu5cmsGrkBNEEZ88ILLxRTGydPnrxtRgTAFrvnnnvi2muvjTvuuCOGDx8eY8eOjVqtVu9hNTy5lSezauRWnsyqkRtAk5QxJ510Unz605+Od7zjHdtmRABskWXLlsWYMWPi/PPPj+OPPz6mTp1abNp4ww031HtoDU1u5cmsGrmVJ7Nq5AbQuBzaGqDJXHbZZcW7nukIGsmQIUPi6quvjksuuSQef/zxeg+vYcmtPJlVI7fyZFaN3AC24zJm+fLlsXTp0vVOAGwbc+fOLZaRTp8+vdgfoNO4ceNixIgRpqdvgtzKk1k1citPZtXIDWA7P5rSpEmT4oorrtjWDwNARBx77LGxatWqjX5szpw52cfTVcitPJlVI7fyZFaN3AC285kxaXpke3v72tPChQu39UMCAAAAbL8zY3r16lWcAAAAAKhQxjz//PPx6KOPrr2+YMGCeOCBB6J///6x5557bu3xAQAAAGzfZcx9991XHBqv08UXX1ycn3322TFjxoytOzoAAACA7b2MOe644+y8DgAAANCoG/gCAAAA8DfKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJBR96iTJUuW1Ouhu5zOrGRWjtzKk1k1citPZtXIrTyZVSO38mRWjdzKk1k1citPZtW83LxaarVaLTIYPXp03HbbbZEebuXKlTkeEgAAACC79vb26Nu3b/3LmE5Lly6Nfv36RVtbWwwYMCDnQ3fpZm348OEyK0lu5cmsGrmVJ7Nq5FaezKqRW3kyq0Zu5cmsGrmVJ7Mty+2lypi6LVNKP8xBgwbV6+G7JJlVI7fyZFaN3MqTWTVyK09m1citPJlVI7fyZFaN3MqT2bZhA18AAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwCaSEdHR4wYMSJOO+209W5vb2+PwYMHx+WXX163sTUyuZUns2rkVp7MqpEbQGNTxgA0kdbW1pgxY0bMnj07brnllrW3jx8/Pvr37x8TJ06s6/galdzKk1k1citPZtXIDaCxda/3AADYuoYOHRpXXnll8Qf3yJEjo62tLWbOnBnz5s2Lnj171nt4DUtu5cmsGrmVJ7Nq5AbQuJQxAE0o/eE9a9asOOuss2L+/PkxYcKEOOyww+o9rIYnt/JkVo3cypNZNXIDaEzKGIAm1NLSElOmTIkDDzwwhg0bFpdeemm9h9QlyK08mVUjt/JkVo3cAJpoz5jJkyfHkCFDYocddoijjjqqmPIIQGOZNm1a9O7dOxYsWBCLFi2q93C6DLmVJ7Nq5FaezKqRG0ATlDFf/epX4+KLLy42/frpT39aTHMcNWpUPPXUU9tmhACUds8998S1114bd9xxRwwfPjzGjh0btVqt3sNqeHIrT2bVyK08mVUjN4AmKWOuueaaOPfcc+Occ86Jgw46KG644YaiaU+NOwD1t2zZshgzZkycf/75cfzxx8fUqVOLGYzp9zWbJrfyZFaN3MqTWTVyA2iSMmbFihVx//33x4knnvi3L9CtW3H9xz/+8bYYHwAlXXbZZcW7nukIGklaVnr11VfHJZdcEo8//ni9h9ew5FaezKqRW3kyq0ZuAE1SxjzzzDPR0dERr371q9e7PV1/8sknN/o5y5cvj6VLl653AmDbmDt3brGv1/Tp04tZi53GjRsXI0aMMD19E+RWnsyqkVt5MqtGbgDb+dGUJk2aFFdcccW2fhgAIuLYY4+NVatWbfRjc+bMyT6erkJu5cmsGrmVJ7Nq5AbQRDNjdtttt2htbY0//OEP692eru++++6bnB7Z3t6+9rRw4cItGzEAAADA9lLG9OzZMw4//PD4zne+s/a21atXF9ePPvrojX5Or169om/fvuudAAAAALZXpZcppcNan3322XHEEUcUh8e77rrr4oUXXiiOrgQAAADAVi5j3v3ud8fTTz8dEyZMKDbtfe1rXxuzZ89+0aa+AAAAAGylDXwvvPDC4gQAAADANtwzBgAAAIAto4wBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGXWPzGq1WnG+ZMmS3A/dZXVmJbNy5FaezKqRW3kyq0Zu5cmsGrmVJ7Nq5FaezKqRW3kyq6Yzr87uY1Naai91j61k8uTJxWnFihXx2GOP5XhIAAAAgOwWLlwYgwYNqn8Z02n16tUxdOjQuP/++6OlpSUaxdKlS2Pw4MFFYH379o1Gs9tuu8UzzzxT72F0OY2Ym+dac2rE3DzXmlMj5ua51pwaMTfPtebUiLl5rjWnRszNc635pIrl8MMPj4cffji6devWOMuU0mB69uwZ/fr1i0aU/gdoxP8JUnHViONqdI2cm+dac2nk3DzXmksj5+a51lwaOTfPtebSyLl5rjWXRs7Nc625pM5jc0VM3TbwveCCC+rxsF3a2972tnoPoUuSW3kyq0Zu5cmsGrmVJ7Nq5FaezKqRW3kyq0Zu5cls23Ue2ZcpNao0PSzN1mlvb9f8sU15rpGL5xq5eK6Ri+cauXiukYvn2vbLoa3/qlevXjFx4sTiHLYlzzVy8VwjF881cvFcIxfPNXLxXNt+mRkDAAAAkJGZMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBnzV5MnT44hQ4bEDjvsEEcddVS0tbXVe0g0mUmTJsWRRx4Zffr0iVe96lXx9re/PR566KF6D4vtwJVXXhktLS3x4Q9/uN5DoQktXrw43vve98auu+4aO+64YwwbNizuu+++eg+LJtPR0RH//M//HHvvvXfxPHvNa14Tn/rUp8JxKNhS3//+9+PUU0+NPfbYo/i38pvf/OZ6H0/PsQkTJsSAAQOK596JJ54YjzzySN3GS3M+11auXBkf+9jHin9Dd9ppp+I+73vf++KJJ56o65jZtpQxEfHVr341Lr744uKQYj/96U/jsMMOi1GjRsVTTz1V76HRRObOnRsXXHBB3HvvvXHXXXcVv3Tf8pa3xAsvvFDvodHE5s2bFzfeeGMceuih9R4KTehPf/pTvOENb4gePXrEnXfeGb/61a/iC1/4Quyyyy71HhpN5qqrroopU6bEl7/85fj1r39dXP/c5z4X//qv/1rvodHFpb/D0t/+6Y3ZjUnPsy996Utxww03xE9+8pPihXJ6nfCXv/wl+1hp3ufasmXLitehqXRO5//1X/9VvGn71re+tS5jJQ+Hto4oZsKkGQvpH/hk9erVMXjw4Bg/fnxceuml9R4eTerpp58uZsikkuZNb3pTvYdDE3r++efj9a9/fVx//fXx6U9/Ol772tfGddddV+9h0UTSv5E/+tGP4gc/+EG9h0KTO+WUU+LVr351TJ06de1t73znO4uZCv/xH/9R17HRPNJshVmzZhWzl5P0MinNUPjHf/zH+OhHP1rc1t7eXjwXZ8yYEe95z3vqPGKa5bm2qTfUhg8fHr/73e9izz33zDo+8tjuZ8asWLEi7r///mLKYadu3boV13/84x/XdWw0t/SPedK/f/96D4UmlWZinXzyyev9foOt6fbbb48jjjgi3vWudxXl8ute97q4+eab6z0smtCIESPiO9/5Tjz88MPF9Z///Ofxwx/+ME466aR6D40mtmDBgnjyySfX+3e0X79+xRu5XieQ47VCKm123nnneg+FbaR7bOeeeeaZYh1yarjXla7/5je/qdu4aG5p9lXavyNN7z/kkEPqPRya0MyZM4tpruldFdhWfvvb3xZLR9JS349//OPF8+1DH/pQ9OzZM84+++x6D48mm4W1dOnSOOCAA6K1tbX42+0zn/lMnHnmmfUeGk0sFTHJxl4ndH4MtoW0DC7tIXPGGWdE37596z0ctpHtvoyBes1YePDBB4t39WBrW7hwYVx00UXF3kRpU3LYlsVymhnz2c9+trieZsak321pbwVlDFvT1772tbjlllvi1ltvjYMPPjgeeOCB4k2NtITEcw1oJmlfydGjRxfL5NIbHjSv7X6Z0m677Va8w/KHP/xhvdvT9d13371u46J5XXjhhXHHHXfE9773vRg0aFC9h0MTSksv0wbkab+Y7t27F6e0N1HagDBdTu8ow9aQji5y0EEHrXfbgQceGL///e/rNiaa0z/90z8Vs2PSHh3paCNnnXVWfOQjHymOVAjbSudrAa8TyF3EpH1i0ptqZsU0t+2+jElTqQ8//PBiHfK67/Sl60cffXRdx0ZzSe12KmLSZl3f/e53i8NzwrZwwgknxPz584t3jjtPafZCms6fLqcCGraGtNQyHe1hXWlPj7322qtuY6I5pSONpD391pV+l6W/2WBbSX+rpdJl3dcJablcOqqS1wlsqyImHTr929/+duy66671HhLbmGVKEcVa9zTFNb1YSTtWp6ONpEOPnXPOOfUeGk22NClNr77tttuiT58+a9cap43g0tEgYGtJz68N9yJKh+JM/6jbo4itKc1MSBurpmVK6Q/Itra2uOmmm4oTbE2nnnpqsUdMOqJIWqb0s5/9LK655pp4//vfX++h0QRHHnz00UfX27Q3vXGRDrCQnm9pOVw6IuF+++1XlDPp0MNpedzmjoIDZZ9raabp6aefXuz3l2bQp1nMna8V0sfTBAKaj0Nb/1U6rPXnP//54kmfDv+apvOnndJha0m7oW/M9OnTY8yYMdnHw/bluOOOc2hrton0R+Nll11WvJOXXqikNzjOPffceg+LJvPcc88VL4LT7NK0DDO9GE4bW06YMMGLFLbI3XffHccff/yLbk9v1KbDV6eXShMnTixK5meffTaOOeaYuP7662Po0KF1GS/N+Vz75Cc/uclZ82lrg/R3HM1HGQMAAACQ0Xa/ZwwAAABATsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAIp//D0+slNdx2jtyAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e85e7f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAC agent's return: 24.0, min: 24.0, max: 24.0, mean: 24.0, var: 0.0\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10000\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions.cpu())\n",
    "    mat_state.append(env.state)  # s+1\n",
    "min_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().min()\n",
    "max_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().max()\n",
    "mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "mean_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "var_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().var()\n",
    "print(f\"SAC agent's return: {mat_return}, min: {min_return}, max: {max_return}, mean: {mean_return}, var: {var_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "562499c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 24.00±0.00, max: 24.00±0.00, mean: 24.00±0.00, median: 24.00±0.00\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    params[\"common\"][\"batch_size\"]=1000\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
