{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        # random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size*self.num_random,1)),\n",
    "        #                                       dtype=torch.long).to(self.device)\n",
    "        random_unif_actions = torch.arange(0, self.action_dim, device=self.device).long().repeat(batch_size).view(-1, 1)\n",
    "        random_unif_log_pi = np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 11, 'y': 18}, 'horizon': 80, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room_2', 'num': 1, 'initial': 80}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 11, \"y\": 18},\n",
    "        \"horizon\": 80,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room_2\",\n",
    "        \"num\": 1,  # 替代原来的args.env\n",
    "        \"initial\": 80\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"two_Room_80_go_explore_archive_file.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 5202 條軌跡中篩選出最好的 20 條。\n",
      "其中最好的一條獎勵為: 137\n",
      "最差的一條（在這20條中）獎勵為: 137\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=20)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 79\n",
      "拓展后状态数量: 79\n",
      "拓展后第一个状态的形状: torch.Size([1, 79])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[ 80.,  79.,  78.,  77.,  76.,  58.,  40.,  39.,  38.,  20.,   2.,   1.,\n",
       "            0.,  18.,  36.,  54.,  72.,  73.,  91.,  90., 108., 126., 144., 145.,\n",
       "          163., 164., 165., 147., 148., 130., 112., 111.,  93.,  75.,  76.,  77.,\n",
       "           78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  68.,  50.,  49.,\n",
       "           48.,  30.,  12.,  13.,  14.,  15.,  16.,  34.,  52.,  70.,  88., 106.,\n",
       "          124., 142., 160., 178., 177., 159., 158., 157., 175., 174., 156., 138.,\n",
       "          120., 121., 122., 140., 141., 142.,  -1.]]),\n",
       " tensor([[ 80.,  79.,  78.,  77.,  76.,  58.,  40.,  39.,  38.,  20.,   2.,   1.,\n",
       "            0.,  18.,  36.,  54.,  72.,  73.,  91.,  90., 108., 126., 144., 145.,\n",
       "          163., 164., 165., 147., 148., 130., 112., 111.,  93.,  75.,  76.,  77.,\n",
       "           78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  68.,  50.,  49.,\n",
       "           48.,  30.,  12.,  13.,  14.,  15.,  16.,  34.,  52.,  70.,  88., 106.,\n",
       "          124., 142., 160., 178., 177., 159., 158., 157., 175., 174., 156., 138.,\n",
       "          120., 121., 122., 140., 141., 142., 124.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 20 条轨迹到回放池（实时计算边际奖励）...\n",
      "总共添加了 1541 个转移到回放池\n",
      "完成！回放池当前大小: 1541\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[80., 79., 78., 77., 76., 58., 40., 39., 38., 20.,  2.,  1.,  0.,\n",
       "         18., 36., 54., 72., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1.]], dtype=float32),\n",
       " (1,),\n",
       " (tensor([2]),),\n",
       " array([[80., 79., 78., 77., 76., 58., 40., 39., 38., 20.,  2.,  1.,  0.,\n",
       "         18., 36., 54., 72., 73., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1.]], dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 20/20 [06:55<00:00, 20.78s/it, epoch=20, return=28.076]\n",
      "Iteration 1: 100%|██████████| 20/20 [08:26<00:00, 25.33s/it, epoch=40, return=28.000]\n",
      "Iteration 2: 100%|██████████| 20/20 [08:22<00:00, 25.15s/it, epoch=60, return=28.000]\n",
      "Iteration 3: 100%|██████████| 20/20 [08:04<00:00, 24.21s/it, epoch=80, return=28.000]\n",
      "Iteration 4: 100%|██████████| 20/20 [08:10<00:00, 24.54s/it, epoch=100, return=28.004]\n",
      "Iteration 5: 100%|██████████| 20/20 [08:29<00:00, 25.50s/it, epoch=120, return=28.012]\n",
      "Iteration 6: 100%|██████████| 20/20 [08:11<00:00, 24.56s/it, epoch=140, return=28.000]\n",
      "Iteration 7: 100%|██████████| 20/20 [08:20<00:00, 25.02s/it, epoch=160, return=28.000]\n",
      "Iteration 8: 100%|██████████| 20/20 [08:03<00:00, 24.16s/it, epoch=180, return=28.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [08:31<00:00, 25.57s/it, epoch=200, return=44.521]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "# num_random = 5\n",
    "num_random = action_dim  # CQL中的动作采样数,这里设置为动作空间的大小\n",
    "num_epochs = 200\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize(params[\"env\"][\"initial\"])\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0: \n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([30])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 12), (6, 12), (7, 12), (8, 12), (9, 12), (10, 12), (11, 12), (12, 12), (13, 30), (14, 48), (15, 66), (16, 84), (17, 102), (18, 120), (19, 138), (20, 156), (21, 174), (22, 192), (23, 192), (24, 192), (25, 192), (26, 192), (27, 192), (28, 192), (29, 174), (30, 156), (31, 138), (32, 120), (33, 102), (34, 84), (35, 66), (36, 48), (37, 30), (38, 12), (39, 12), (40, 12), (41, 12), (42, 12), (43, 12), (44, 12), (45, 12), (46, 12), (47, 12), (48, 12), (49, 12), (50, 12), (51, 12), (52, 12), (53, 30), (54, 48), (55, 66), (56, 84), (57, 102), (58, 120), (59, 138), (60, 156), (61, 174), (62, 192), (63, 192), (64, 192), (65, 192), (66, 192), (67, 192), (68, 192), (69, 192), (70, 192), (71, 192), (72, 192), (73, 192), (74, 192), (75, 192), (76, 192), (77, 192), (78, 192), (79, 192)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001]\n",
      "x [16, 15, 14, 13, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12]\n",
      "y [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 10, 10, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1100 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd1c0b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 32.60±0.92, max: 48.00±0.00, mean: 44.61±0.07, median: 46.00±0.00\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    params[\"common\"][\"batch_size\"]=1000\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb446a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
