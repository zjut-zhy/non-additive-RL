{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:24.774618Z",
     "start_time": "2025-07-07T09:54:22.272433Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu\n",
    "from subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "workspace = \"NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283ca354729c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:25.403450Z",
     "start_time": "2025-07-07T09:54:25.375216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 11, \"y\": 18},\n",
    "        \"horizon\": 80,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room_2\",\n",
    "        \"num\": 1,  # 替代原来的args.env\n",
    "        \"initial\": 80\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53557d7595a25f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:26.280367Z",
     "start_time": "2025-07-07T09:54:26.274714Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"two_Room_80_go_explore_archive_spacetime.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ade27f0c9cbc48ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:27.832399Z",
     "start_time": "2025-07-07T09:54:27.824570Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# --- Core parameter setting ---\n",
    "EXPLORATION_STEPS = 500000  # The total number of exploration steps can be adjusted as needed, and the larger the value, the more thorough the exploration\n",
    "K_STEPS = 15            # Random exploration steps starting from selected cells \n",
    "\n",
    "# --- Go-Explore main  ---\n",
    "def run_go_explore_phase1_spacetime(env=GridWorld, params=None,exploration_step=EXPLORATION_STEPS,isLoad=False, archive_path=\"go_explore_triplet_archive2.pkl\",k=K_STEPS):\n",
    "    \"\"\"\n",
    "    執行 Go-Explore 的第一階段探索（使用時空細胞）\n",
    "    \"\"\"\n",
    "    print(\"--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\")\n",
    "    \n",
    "    # 1. 初始化存檔庫 (Archive)\n",
    "    # ---【核心修改】：將細胞 Key 定義為 (時間, 狀態ID) ---\n",
    "    # 結構: { (time, state_id): {'reward': float, 'states': list, 'actions': list, 'times_selected': int} }\n",
    "    archive = {}\n",
    "    if isLoad:\n",
    "        print(\"正在從存檔庫載入已保存的時空細胞...\")\n",
    "        with open(archive_path, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        print(f\"成功載入存檔庫，包含 {len(archive)} 個細胞。\")\n",
    "\n",
    "\n",
    "    \n",
    "    env.common_params[\"batch_size\"]=1\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    initial_state_tensor = env.state.clone()\n",
    "    initial_state_id = initial_state_tensor.item()\n",
    "    \n",
    "    # ---【核心修改】：初始細胞現在是 (時間=0, 狀態) ---\n",
    "    initial_cell_key = (0, initial_state_id)\n",
    "    \n",
    "    initial_reward = calculate_submodular_reward([initial_state_tensor], env)\n",
    "    \n",
    "    archive[initial_cell_key] = {\n",
    "        'reward': initial_reward,\n",
    "        'states': [initial_state_tensor],\n",
    "        'actions': [],\n",
    "        'times_selected': 0\n",
    "    }\n",
    "    print(f\"初始時空細胞加入存檔庫: Cell {initial_cell_key}, Reward: {initial_reward}\")\n",
    "    \n",
    "    # 2. 執行 N 次探索循環\n",
    "    pbar = tqdm(total=exploration_step, desc=\"Go-Exploring (Spacetime)\")\n",
    "    for step in range(exploration_step):\n",
    "        # 2.1 選擇細胞 (選擇函數無需修改，它通用於任何 key)\n",
    "        cell_key_to_explore_from, selected_cell_data = select_cell_from_archive(archive)\n",
    "        \n",
    "        if selected_cell_data is None:\n",
    "            print(\"錯誤：存檔庫為空，無法繼續探索。\")\n",
    "            break\n",
    "            \n",
    "        archive[cell_key_to_explore_from]['times_selected'] += 1\n",
    "\n",
    "        # 2.2 前往 (Go To) 該細胞狀態\n",
    "        env.initialize(params[\"env\"][\"initial\"])\n",
    "        for action in selected_cell_data['actions']:\n",
    "            env.step(0, torch.tensor([action]))\n",
    "\n",
    "        # 2.3 從該狀態開始，隨機探索 (Explore) k 步\n",
    "        current_states = selected_cell_data['states'][:]\n",
    "        current_actions = selected_cell_data['actions'][:]\n",
    "        \n",
    "        k=random.randint(k,k)\n",
    "        \n",
    "        for _ in range(k):\n",
    "            if len(current_actions) >= params[\"env\"][\"horizon\"] - 1:\n",
    "                break\n",
    "                \n",
    "            # random_action = random.randint(0, env.action_dim - 1)\n",
    "            # env.step(0, torch.tensor([random_action]))\n",
    "            # \n",
    "            # new_state_tensor = env.state.clone()\n",
    "            \n",
    "            current_state_tensor = env.state.clone()\n",
    "            new_state_tensor=current_state_tensor # 初始化新状态张量\n",
    "            while current_state_tensor== new_state_tensor:\n",
    "                random_action = random.randint(0, env.action_dim - 1)\n",
    "                while random_action==0:\n",
    "                    random_action = random.randint(0, env.action_dim - 1)\n",
    "                    \n",
    "                env.step(0, torch.tensor([random_action]))\n",
    "                \n",
    "                new_state_tensor = env.state.clone()\n",
    "                if current_state_tensor== new_state_tensor:\n",
    "                    env.return_to_pre_step()\n",
    "            \n",
    "            current_states.append(new_state_tensor)\n",
    "            current_actions.append(random_action)\n",
    "            \n",
    "            # ---【核心修改】：更新存檔庫時使用 (時間, 狀態) 作為 Key ---\n",
    "            new_state_id = new_state_tensor.item()\n",
    "            time_step = len(current_actions) # 當前時間步 = 已執行動作的數量\n",
    "            new_cell_key = (time_step, new_state_id)\n",
    "            \n",
    "            new_reward = calculate_submodular_reward(current_states, env)\n",
    "            \n",
    "            if new_cell_key not in archive or new_reward > archive[new_cell_key]['reward']:\n",
    "                archive[new_cell_key] = {\n",
    "                    'reward': new_reward,\n",
    "                    'states': current_states[:],\n",
    "                    'actions': current_actions[:],\n",
    "                    'times_selected': 0\n",
    "                }\n",
    "            # elif new_cell_key in archive and new_reward < archive[new_cell_key]['reward']:\n",
    "            #     break\n",
    "            \n",
    "        if step% ( exploration_step / 10) == 0:\n",
    "            print(f\"探索步骤: {step / exploration_step * 100:.2f}%\")\n",
    "            print(f\"當前存檔庫大小: {len(archive)}\")\n",
    "            _best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "            _max_reward = _best_trajectory_data['reward']\n",
    "            print(f\"找到的最佳獎勵值: {_max_reward}\")\n",
    "            print(f\"對應的軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "                \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. 探索結束\n",
    "    print(\"\\n--- 探索完成 ---\")\n",
    "    if not archive:\n",
    "        print(\"錯誤：存檔庫為空！\")\n",
    "        return None\n",
    "\n",
    "    best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "    max_reward = best_trajectory_data['reward']\n",
    "    print(f\"找到的最佳獎勵值: {max_reward}\")\n",
    "    print(f\"對應的軌跡長度: {len(best_trajectory_data['states'])}\")\n",
    "\n",
    "    # 4. 保存存檔庫\n",
    "    archive_filename = archive_path\n",
    "    with open(archive_filename, \"wb\") as f:\n",
    "        pickle.dump(archive, f)\n",
    "    print(f\"完整的時空細胞存檔庫已保存至: {archive_filename}\")\n",
    "    \n",
    "    return best_trajectory_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8b247b80ee0483",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T10:59:59.633740Z",
     "start_time": "2025-07-07T10:32:26.773910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\n",
      "正在從存檔庫載入已保存的時空細胞...\n",
      "成功載入存檔庫，包含 5202 個細胞。\n",
      "初始時空細胞加入存檔庫: Cell (0, 80), Reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):   0%|          | 18/300000 [00:00<29:15, 170.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 0.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  10%|█         | 30030/300000 [02:41<29:02, 154.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 10.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  20%|██        | 60023/300000 [05:12<22:58, 174.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 20.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  30%|███       | 90035/300000 [07:55<19:20, 180.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 30.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  40%|████      | 120026/300000 [10:42<16:40, 179.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 40.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  50%|█████     | 150023/300000 [13:29<14:53, 167.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 50.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  60%|██████    | 180020/300000 [16:17<11:41, 170.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 60.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  70%|███████   | 210026/300000 [19:04<08:19, 180.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 70.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  80%|████████  | 240034/300000 [21:51<05:07, 195.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 80.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):  90%|█████████ | 270034/300000 [24:38<02:49, 177.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "探索步骤: 90.00%\n",
      "當前存檔庫大小: 5202\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime): 100%|██████████| 300000/300000 [27:31<00:00, 181.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 探索完成 ---\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 78\n",
      "完整的時空細胞存檔庫已保存至: two_Room_80_go_explore_archive_file.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params,exploration_step=300000,archive_path=\"two_Room_80_go_explore_archive_file.pkl\",isLoad=True,k=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547fbebdc9609041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:15:51.959985Z",
     "start_time": "2025-07-07T08:15:51.850113Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_excellent_trajectories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_20_trajectories \u001b[38;5;241m=\u001b[39m \u001b[43msample_excellent_trajectories\u001b[49m(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_Room_80_go_explore_archive_spacetime.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_n\u001b[39m\u001b[38;5;124m'\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_20_trajectories:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m其中最好的一條獎勵為: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_20_trajectories[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_excellent_trajectories' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\"go_explore_archive_file_two_Room_198.pkl\",method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31d60be0ee55504a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:10:28.541180Z",
     "start_time": "2025-07-07T11:10:28.158111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 範例 1: 採樣 Top 20 ---\n",
      "方法: Top-N。從 5202 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 138\n",
      "最差的一條（在這300條中）獎勵為: 134\n",
      "\n",
      "{'reward': 138, 'states': [tensor([80]), tensor([79]), tensor([78]), tensor([77]), tensor([76]), tensor([75]), tensor([57]), tensor([39]), tensor([40]), tensor([22]), tensor([4]), tensor([3]), tensor([2]), tensor([1]), tensor([0]), tensor([18]), tensor([36]), tensor([37]), tensor([55]), tensor([73]), tensor([72]), tensor([90]), tensor([108]), tensor([126]), tensor([127]), tensor([145]), tensor([163]), tensor([164]), tensor([165]), tensor([147]), tensor([148]), tensor([130]), tensor([112]), tensor([94]), tensor([95]), tensor([96]), tensor([97]), tensor([98]), tensor([80]), tensor([81]), tensor([82]), tensor([83]), tensor([84]), tensor([102]), tensor([103]), tensor([104]), tensor([122]), tensor([140]), tensor([139]), tensor([157]), tensor([175]), tensor([176]), tensor([177]), tensor([178]), tensor([160]), tensor([142]), tensor([124]), tensor([106]), tensor([88]), tensor([70]), tensor([52]), tensor([34]), tensor([16]), tensor([15]), tensor([14]), tensor([13]), tensor([12]), tensor([30]), tensor([48]), tensor([49]), tensor([50]), tensor([68]), tensor([67]), tensor([66]), tensor([84]), tensor([102]), tensor([120]), tensor([138]), tensor([156]), tensor([174])], 'actions': [3, 3, 3, 3, 3, 4, 4, 1, 4, 4, 3, 3, 3, 3, 2, 2, 1, 2, 2, 3, 2, 2, 2, 1, 2, 2, 1, 1, 4, 1, 4, 4, 4, 1, 1, 1, 1, 4, 1, 1, 1, 1, 2, 1, 1, 2, 2, 3, 2, 2, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 3, 3, 3, 3, 2, 2, 1, 1, 2, 3, 3, 2, 2, 2, 2, 2, 2], 'times_selected': 27}\n"
     ]
    }
   ],
   "source": [
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\".pkl\",method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "print(top_20_trajectories[0])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15bc57330cb4022b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:17:38.752212Z",
     "start_time": "2025-07-07T11:17:38.742360Z"
    }
   },
   "outputs": [],
   "source": [
    "#基于embedding的模仿学习\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F  # 这里导入 F\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalStateEncoder(nn.Module):\n",
    "    def __init__(self, num_states=900, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_states, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim)\n",
    "\n",
    "    def forward(self, state_seq):\n",
    "        indices = [i for i in state_seq if i >= 0]\n",
    "        if not indices:\n",
    "            return torch.zeros(self.lstm.hidden_size)\n",
    "\n",
    "        input_emb = self.embedding(torch.tensor(indices).long()).unsqueeze(1)  # [T, 1, D]\n",
    "        _, (h_n, _) = self.lstm(input_emb)\n",
    "        return h_n.squeeze(0).squeeze(0)  # [hidden_dim]\n",
    "\n",
    "def encode_temporal_state(state_seq, embed_table):\n",
    "    \"\"\"\n",
    "    输入: state_seq: Tensor[H]，如 [34, 33, -1, ..., -1]\n",
    "    输出: Tensor[embed_dim]，嵌入向量\n",
    "    \"\"\"\n",
    "    indices = [i for i in state_seq if i >= 0]  # 去除 -1\n",
    "    if not indices:\n",
    "        return torch.zeros(embed_table.embedding_dim)\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    embeds = embed_table(indices_tensor)\n",
    "    return embeds.mean(dim=0)\n",
    "def encode_temporal_state2(state_seq, encoder):\n",
    "    return encoder(state_seq)\n",
    "\n",
    "def submodular_selector_temporal(trajectories, embed_table,temporal_encoder, budget=50, lambda_div=0.5, per_traj_limit=True,horizon=40):\n",
    "    \"\"\"\n",
    "    改进版子模选择器：\n",
    "    - 使用欧几里得距离作为 diversity 惩罚项\n",
    "    - 可选启用：每条轨迹最多选一个状态（保证轨迹多样性）\n",
    "    \"\"\"\n",
    "    state_vectors = []\n",
    "    action_labels = []\n",
    "    traj_ids = []\n",
    "\n",
    "    for traj_id, traj in enumerate(trajectories):\n",
    "        states = [int(s.item()) for s in traj['states']]\n",
    "        actions = traj['actions']\n",
    "        for t, action in enumerate(actions):\n",
    "            temporal_state = [-1]*horizon\n",
    "            for h in range(t+1):\n",
    "                temporal_state[h] = states[h]\n",
    "            temporal_tensor = torch.tensor(temporal_state, dtype=torch.long)\n",
    "            # encoded = encode_temporal_state(temporal_tensor, embed_table) # 使用嵌入表编码\n",
    "            # temporal_encoder = TemporalStateEncoder() #使用lstm+embedding编码\n",
    "            vec = encode_temporal_state2(temporal_tensor, temporal_encoder)\n",
    "            state_vectors.append(vec.detach())\n",
    "            action_labels.append(action)\n",
    "            traj_ids.append(traj_id)\n",
    "\n",
    "    print(f\"Total states collected: {len(state_vectors)}\")\n",
    "    all_states = torch.stack(state_vectors)\n",
    "    all_actions = torch.tensor(action_labels, dtype=torch.long)\n",
    "    traj_ids = torch.tensor(traj_ids)\n",
    "\n",
    "    selected_indices = []\n",
    "    selected_vectors = []\n",
    "    selected_trajs = set()\n",
    "\n",
    "    for _ in range(min(budget, len(all_states))):\n",
    "        best_score, best_idx = -float(\"inf\"), -1\n",
    "\n",
    "        for i in range(len(all_states)):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            if per_traj_limit and traj_ids[i].item() in selected_trajs:\n",
    "                continue\n",
    "\n",
    "            candidate = all_states[i].unsqueeze(0)\n",
    "            reward = torch.abs(candidate).mean().item()\n",
    "\n",
    "            if selected_vectors:\n",
    "                selected_tensor = torch.stack(selected_vectors)\n",
    "                sims = ((candidate - selected_tensor)**2).sum(dim=1)  # Euclidean squared\n",
    "                diversity_penalty = -sims.mean().item()  # maximize distance\n",
    "            else:\n",
    "                diversity_penalty = 0\n",
    "\n",
    "            score = reward + lambda_div * diversity_penalty\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx == -1:\n",
    "            break\n",
    "\n",
    "        selected_indices.append(best_idx)\n",
    "        selected_vectors.append(all_states[best_idx])\n",
    "        selected_trajs.add(traj_ids[best_idx].item())\n",
    "\n",
    "    return all_states[selected_indices], all_actions[selected_indices],all_states,all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb30c7ed768116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:18:45.765877Z",
     "start_time": "2025-07-07T11:18:37.317456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 5202 條軌跡中篩選出最好的 50 條。\n",
      "Embedding(900, 16)\n",
      "Total states collected: 3866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_dim = 16\n",
    "num_states = 900\n",
    "elite_trajectories_data = sample_excellent_trajectories(\n",
    "        filepath=\"two_Room_80_go_explore_archive_file.pkl\", \n",
    "        method='top_n', \n",
    "        n=50)\n",
    "\n",
    "embed_table = torch.nn.Embedding(num_states, 16)\n",
    "temporal_encoder = TemporalStateEncoder(num_states=198, embed_dim=16, hidden_dim=32)\n",
    "print(embed_table)\n",
    "selected_states, selected_actions, all_states,all_actions = submodular_selector_temporal(\n",
    "    trajectories=elite_trajectories_data[:],\n",
    "    embed_table=embed_table,\n",
    "    temporal_encoder=temporal_encoder,\n",
    "    budget=500,\n",
    "    lambda_div=2.0,\n",
    "    per_traj_limit=True,\n",
    "    horizon=params[\"env\"][\"horizon\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
