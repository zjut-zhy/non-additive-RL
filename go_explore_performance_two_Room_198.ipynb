{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:24.774618Z",
     "start_time": "2025-07-07T09:54:22.272433Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import random\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from tqdm import tqdm\n",
    "\n",
    "from environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from visualization import Visu\n",
    "from subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "workspace = \"NM\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "283ca354729c8110",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:25.403450Z",
     "start_time": "2025-07-07T09:54:25.375216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 11, \"y\": 18},\n",
    "        \"horizon\": 80,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room_2\",\n",
    "        \"num\": 1,  # 替代原来的args.env\n",
    "        \"initial\": 80\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53557d7595a25f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:26.280367Z",
     "start_time": "2025-07-07T09:54:26.274714Z"
    }
   },
   "outputs": [],
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"two_Room_80_go_explore_archive_spacetime.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ade27f0c9cbc48ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T09:54:27.832399Z",
     "start_time": "2025-07-07T09:54:27.824570Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\n",
      "初始時空細胞加入存檔庫: Cell (0, 80), Reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime):   0%|          | 1932/2000000 [00:16<5:21:37, 103.54it/s, Archive Size=6359, Best Reward=70, Best Reward Length=80]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 121\u001b[39m\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m best_trajectory_data\n\u001b[32m    120\u001b[39m \u001b[38;5;66;03m# --- 運行最終升級版的 Go-Explore ---\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m best_found_trajectory = \u001b[43mrun_go_explore_phase1_spacetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mrun_go_explore_phase1_spacetime\u001b[39m\u001b[34m(env, params)\u001b[39m\n\u001b[32m     45\u001b[39m archive[cell_key_to_explore_from][\u001b[33m'\u001b[39m\u001b[33mtimes_selected\u001b[39m\u001b[33m'\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# 2.2 前往 (Go To) 該細胞狀態\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43menv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minitial\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m selected_cell_data[\u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     50\u001b[39m     env.step(\u001b[32m0\u001b[39m, torch.tensor([action]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Users\\34162\\Documents\\GitHub\\non-additive-RL\\environment.py:733\u001b[39m, in \u001b[36mGridWorld.initialize\u001b[39m\u001b[34m(self, initial)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minitialize\u001b[39m(\u001b[38;5;28mself\u001b[39m,initial=\u001b[32m34\u001b[39m):\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# self.state = 0*self.dPi_init.multinomial(\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m#     self.common_params[\"batch_size\"], replacement=True)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m     \u001b[38;5;28mself\u001b[39m.state = initial*\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcommon_params\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbatch_size\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Core parameter setting ---\n",
    "EXPLORATION_STEPS = 2000000  # The total number of exploration steps can be adjusted as needed, and the larger the value, the more thorough the exploration\n",
    "K_STEPS = params[\"env\"][\"horizon\"]             # Random exploration steps starting from selected cells\n",
    "\n",
    "# --- Go-Explore main  ---\n",
    "def run_go_explore_phase1_spacetime(env=GridWorld, params=None):\n",
    "    \"\"\"\n",
    "    執行 Go-Explore 的第一階段探索（使用時空細胞）\n",
    "    \"\"\"\n",
    "    print(\"--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\")\n",
    "    \n",
    "    # 1. 初始化存檔庫 (Archive)\n",
    "    # ---【核心修改】：將細胞 Key 定義為 (時間, 狀態ID) ---\n",
    "    # 結構: { (time, state_id): {'reward': float, 'states': list, 'actions': list, 'times_selected': int} }\n",
    "    archive = {}\n",
    "    \n",
    "    env.common_params[\"batch_size\"]=1\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    initial_state_tensor = env.state.clone()\n",
    "    initial_state_id = initial_state_tensor.item()\n",
    "    \n",
    "    # ---【核心修改】：初始細胞現在是 (時間=0, 狀態) ---\n",
    "    initial_cell_key = (0, initial_state_id)\n",
    "    \n",
    "    initial_reward = calculate_submodular_reward([initial_state_tensor], env)\n",
    "    \n",
    "    archive[initial_cell_key] = {\n",
    "        'reward': initial_reward,\n",
    "        'states': [initial_state_tensor],\n",
    "        'actions': [],\n",
    "        'times_selected': 0\n",
    "    }\n",
    "    print(f\"初始時空細胞加入存檔庫: Cell {initial_cell_key}, Reward: {initial_reward}\")\n",
    "    \n",
    "    # 2. 執行 N 次探索循環\n",
    "    pbar = tqdm(total=EXPLORATION_STEPS, desc=\"Go-Exploring (Spacetime)\")\n",
    "    for step in range(EXPLORATION_STEPS):\n",
    "        # 2.1 選擇細胞 (選擇函數無需修改，它通用於任何 key)\n",
    "        cell_key_to_explore_from, selected_cell_data = select_cell_from_archive(archive)\n",
    "        \n",
    "        if selected_cell_data is None:\n",
    "            print(\"錯誤：存檔庫為空，無法繼續探索。\")\n",
    "            break\n",
    "            \n",
    "        archive[cell_key_to_explore_from]['times_selected'] += 1\n",
    "\n",
    "        # 2.2 前往 (Go To) 該細胞狀態\n",
    "        env.initialize(params[\"env\"][\"initial\"])\n",
    "        for action in selected_cell_data['actions']:\n",
    "            env.step(0, torch.tensor([action]))\n",
    "\n",
    "        # 2.3 從該狀態開始，隨機探索 (Explore) k 步\n",
    "        current_states = selected_cell_data['states'][:]\n",
    "        current_actions = selected_cell_data['actions'][:]\n",
    "        \n",
    "        # k_STEPS=random.randint(5,K_STEPS)\n",
    "        k_STEPS = K_STEPS\n",
    "        \n",
    "        for _ in range(k_STEPS):\n",
    "            if len(current_actions) >= params[\"env\"][\"horizon\"] - 1:\n",
    "                break\n",
    "                \n",
    "            random_action = random.randint(0, env.action_dim - 1)\n",
    "            env.step(0, torch.tensor([random_action]))\n",
    "            \n",
    "            new_state_tensor = env.state.clone()\n",
    "            \n",
    "            current_states.append(new_state_tensor)\n",
    "            current_actions.append(random_action)\n",
    "            \n",
    "            # ---【核心修改】：更新存檔庫時使用 (時間, 狀態) 作為 Key ---\n",
    "            new_state_id = new_state_tensor.item()\n",
    "            time_step = len(current_actions) # 當前時間步 = 已執行動作的數量\n",
    "            new_cell_key = (time_step, new_state_id)\n",
    "            \n",
    "            new_reward = calculate_submodular_reward(current_states, env)\n",
    "            \n",
    "            if new_cell_key not in archive or new_reward > archive[new_cell_key]['reward']:\n",
    "                archive[new_cell_key] = {\n",
    "                    'reward': new_reward,\n",
    "                    'states': current_states[:],\n",
    "                    'actions': current_actions[:],\n",
    "                    'times_selected': 0\n",
    "                }\n",
    "\n",
    "        pbar.update(1)    \n",
    "\n",
    "        if step % 1 == 0:\n",
    "            # print(f\"探索步骤: {step / EXPLORATION_STEPS * 100:.2f}%\")\n",
    "            # print(f\"當前存檔庫大小: {len(archive)}\")\n",
    "            _best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "            _max_reward = _best_trajectory_data['reward']\n",
    "            # print(f\"找到的最佳獎勵值: {_max_reward}\")\n",
    "            # print(f\"對應的軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "            # print(f\"探索步骤: {step / EXPLORATION_STEPS * 100:.2f}%, 當前存檔庫大小: {len(archive)}, 最佳獎勵值: {_max_reward}, 軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "            pbar.set_postfix({\"Archive Size\": len(archive), \"Best Reward\": _max_reward,\"Best Reward Length\":len(_best_trajectory_data['states'])})\n",
    "                \n",
    "        # pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. 探索結束\n",
    "    print(\"\\n--- 探索完成 ---\")\n",
    "    if not archive:\n",
    "        print(\"錯誤：存檔庫為空！\")\n",
    "        return None\n",
    "\n",
    "    best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "    max_reward = best_trajectory_data['reward']\n",
    "    print(f\"找到的最佳獎勵值: {max_reward}\")\n",
    "    print(f\"對應的軌跡長度: {len(best_trajectory_data['states'])}\")\n",
    "\n",
    "    # 4. 保存存檔庫\n",
    "    archive_filename = \"go_explore_archive_spacetime_.pkl\"\n",
    "    with open(archive_filename, \"wb\") as f:\n",
    "        pickle.dump(archive, f)\n",
    "    print(f\"完整的時空細胞存檔庫已保存至: {archive_filename}\")\n",
    "    \n",
    "    return best_trajectory_data\n",
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8a99cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\n",
      "初始時空細胞加入存檔庫: Cell (0, 80), Reward: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Go-Exploring (Spacetime): 100%|██████████| 500000/500000 [1:45:24<00:00, 79.05it/s, Archive Size=10300, Best Reward=137, Best Reward Length=80]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 探索完成 ---\n",
      "找到的最佳獎勵值: 137\n",
      "對應的軌跡長度: 80\n",
      "完整的時空細胞存檔庫已保存至: go_explore_archive_spacetime_.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# --- Core parameter setting ---\n",
    "EXPLORATION_STEPS = 500000  # The total number of exploration steps can be adjusted as needed, and the larger the value, the more thorough the exploration\n",
    "K_STEPS = 200             # Random exploration steps starting from selected cells \n",
    "\n",
    "# --- Go-Explore main  ---\n",
    "def run_go_explore_phase1_spacetime(env=GridWorld, params=None):\n",
    "    \"\"\"\n",
    "    執行 Go-Explore 的第一階段探索（使用時空細胞）\n",
    "    \"\"\"\n",
    "    print(\"--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\")\n",
    "    \n",
    "    # 1. 初始化存檔庫 (Archive)\n",
    "    # ---【核心修改】：將細胞 Key 定義為 (時間, 狀態ID) ---\n",
    "    # 結構: { (time, state_id): {'reward': float, 'states': list, 'actions': list, 'times_selected': int} }\n",
    "    archive = {}\n",
    "    \n",
    "    env.common_params[\"batch_size\"]=1\n",
    "    env.initialize(params[\"env\"][\"initial\"])\n",
    "    initial_state_tensor = env.state.clone()\n",
    "    initial_state_id = initial_state_tensor.item()\n",
    "    \n",
    "    # ---【核心修改】：初始細胞現在是 (時間=0, 狀態) ---\n",
    "    initial_cell_key = (0, initial_state_id)\n",
    "    \n",
    "    initial_reward = calculate_submodular_reward([initial_state_tensor], env)\n",
    "    \n",
    "    archive[initial_cell_key] = {\n",
    "        'reward': initial_reward,\n",
    "        'states': [initial_state_tensor],\n",
    "        'actions': [],\n",
    "        'times_selected': 0\n",
    "    }\n",
    "    print(f\"初始時空細胞加入存檔庫: Cell {initial_cell_key}, Reward: {initial_reward}\")\n",
    "    \n",
    "    # 2. 執行 N 次探索循環\n",
    "    pbar = tqdm(total=EXPLORATION_STEPS, desc=\"Go-Exploring (Spacetime)\")\n",
    "    for step in range(EXPLORATION_STEPS):\n",
    "        # 2.1 選擇細胞 (選擇函數無需修改，它通用於任何 key)\n",
    "        cell_key_to_explore_from, selected_cell_data = select_cell_from_archive(archive)\n",
    "        \n",
    "        if selected_cell_data is None:\n",
    "            print(\"錯誤：存檔庫為空，無法繼續探索。\")\n",
    "            break\n",
    "            \n",
    "        archive[cell_key_to_explore_from]['times_selected'] += 1\n",
    "\n",
    "        # 2.2 前往 (Go To) 該細胞狀態\n",
    "        env.initialize(params[\"env\"][\"initial\"])\n",
    "        for action in selected_cell_data['actions']:\n",
    "            env.step(0, torch.tensor([action]))\n",
    "\n",
    "        # 2.3 從該狀態開始，隨機探索 (Explore) k 步\n",
    "        current_states = selected_cell_data['states'][:]\n",
    "        current_actions = selected_cell_data['actions'][:]\n",
    "        \n",
    "        k_STEPS=random.randint(5,K_STEPS)\n",
    "        \n",
    "        for _ in range(k_STEPS):\n",
    "            if len(current_actions) >= params[\"env\"][\"horizon\"] - 1:\n",
    "                break\n",
    "                \n",
    "            random_action = random.randint(0, env.action_dim - 1)\n",
    "            env.step(0, torch.tensor([random_action]))\n",
    "            \n",
    "            new_state_tensor = env.state.clone()\n",
    "            \n",
    "            current_states.append(new_state_tensor)\n",
    "            current_actions.append(random_action)\n",
    "            \n",
    "            # ---【核心修改】：更新存檔庫時使用 (時間, 狀態) 作為 Key ---\n",
    "            new_state_id = new_state_tensor.item()\n",
    "            time_step = len(current_actions) # 當前時間步 = 已執行動作的數量\n",
    "            new_cell_key = (time_step, new_state_id)\n",
    "            \n",
    "            new_reward = calculate_submodular_reward(current_states, env)\n",
    "            \n",
    "            if new_cell_key not in archive or new_reward > archive[new_cell_key]['reward']:\n",
    "                archive[new_cell_key] = {\n",
    "                    'reward': new_reward,\n",
    "                    'states': current_states[:],\n",
    "                    'actions': current_actions[:],\n",
    "                    'times_selected': 0\n",
    "                }\n",
    "\n",
    "        pbar.update(1)    \n",
    "\n",
    "        if step % 1 == 0:\n",
    "            # print(f\"探索步骤: {step / EXPLORATION_STEPS * 100:.2f}%\")\n",
    "            # print(f\"當前存檔庫大小: {len(archive)}\")\n",
    "            _best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "            _max_reward = _best_trajectory_data['reward']\n",
    "            # print(f\"找到的最佳獎勵值: {_max_reward}\")\n",
    "            # print(f\"對應的軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "            # print(f\"探索步骤: {step / EXPLORATION_STEPS * 100:.2f}%, 當前存檔庫大小: {len(archive)}, 最佳獎勵值: {_max_reward}, 軌跡長度: {len(_best_trajectory_data['states'])}\")\n",
    "            pbar.set_postfix({\"Archive Size\": len(archive), \"Best Reward\": _max_reward,\"Best Reward Length\":len(_best_trajectory_data['states'])})\n",
    "                \n",
    "        # pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. 探索結束\n",
    "    print(\"\\n--- 探索完成 ---\")\n",
    "    if not archive:\n",
    "        print(\"錯誤：存檔庫為空！\")\n",
    "        return None\n",
    "\n",
    "    best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "    max_reward = best_trajectory_data['reward']\n",
    "    print(f\"找到的最佳獎勵值: {max_reward}\")\n",
    "    print(f\"對應的軌跡長度: {len(best_trajectory_data['states'])}\")\n",
    "\n",
    "    # 4. 保存存檔庫\n",
    "    archive_filename = \"go_explore_archive_spacetime_.pkl\"\n",
    "    with open(archive_filename, \"wb\") as f:\n",
    "        pickle.dump(archive, f)\n",
    "    print(f\"完整的時空細胞存檔庫已保存至: {archive_filename}\")\n",
    "    \n",
    "    return best_trajectory_data\n",
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd74611",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params,exploration_step=300000,archive_path=\"two_Room_80_go_explore_archive_file.pkl\",isLoad=True,k=40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "547fbebdc9609041",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T08:15:51.959985Z",
     "start_time": "2025-07-07T08:15:51.850113Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_excellent_trajectories' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m top_20_trajectories \u001b[38;5;241m=\u001b[39m \u001b[43msample_excellent_trajectories\u001b[49m(filepath\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtwo_Room_80_go_explore_archive_spacetime.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m,method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtop_n\u001b[39m\u001b[38;5;124m'\u001b[39m, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m top_20_trajectories:\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m其中最好的一條獎勵為: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_20_trajectories[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_excellent_trajectories' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\"go_explore_archive_file_two_Room_198.pkl\",method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31d60be0ee55504a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:10:28.541180Z",
     "start_time": "2025-07-07T11:10:28.158111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 範例 1: 採樣 Top 20 ---\n",
      "Error: Archive file not found '.pkl'\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m其中最好的一條獎勵為: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_20_trajectories[\u001b[32m0\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m最差的一條（在這300條中）獎勵為: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtop_20_trajectories[-\u001b[32m1\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mreward\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtop_20_trajectories\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(filepath=\".pkl\",method='top_n', n=100)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這300條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "print(top_20_trajectories[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15bc57330cb4022b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:17:38.752212Z",
     "start_time": "2025-07-07T11:17:38.742360Z"
    }
   },
   "outputs": [],
   "source": [
    "#基于embedding的模仿学习\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F  # 这里导入 F\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalStateEncoder(nn.Module):\n",
    "    def __init__(self, num_states=900, embed_dim=16, hidden_dim=32):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_states, embed_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embed_dim, hidden_size=hidden_dim)\n",
    "\n",
    "    def forward(self, state_seq):\n",
    "        indices = [i for i in state_seq if i >= 0]\n",
    "        if not indices:\n",
    "            return torch.zeros(self.lstm.hidden_size)\n",
    "\n",
    "        input_emb = self.embedding(torch.tensor(indices).long()).unsqueeze(1)  # [T, 1, D]\n",
    "        _, (h_n, _) = self.lstm(input_emb)\n",
    "        return h_n.squeeze(0).squeeze(0)  # [hidden_dim]\n",
    "\n",
    "def encode_temporal_state(state_seq, embed_table):\n",
    "    \"\"\"\n",
    "    输入: state_seq: Tensor[H]，如 [34, 33, -1, ..., -1]\n",
    "    输出: Tensor[embed_dim]，嵌入向量\n",
    "    \"\"\"\n",
    "    indices = [i for i in state_seq if i >= 0]  # 去除 -1\n",
    "    if not indices:\n",
    "        return torch.zeros(embed_table.embedding_dim)\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.long)\n",
    "    embeds = embed_table(indices_tensor)\n",
    "    return embeds.mean(dim=0)\n",
    "def encode_temporal_state2(state_seq, encoder):\n",
    "    return encoder(state_seq)\n",
    "\n",
    "def submodular_selector_temporal(trajectories, embed_table,temporal_encoder, budget=50, lambda_div=0.5, per_traj_limit=True,horizon=40):\n",
    "    \"\"\"\n",
    "    改进版子模选择器：\n",
    "    - 使用欧几里得距离作为 diversity 惩罚项\n",
    "    - 可选启用：每条轨迹最多选一个状态（保证轨迹多样性）\n",
    "    \"\"\"\n",
    "    state_vectors = []\n",
    "    action_labels = []\n",
    "    traj_ids = []\n",
    "\n",
    "    for traj_id, traj in enumerate(trajectories):\n",
    "        states = [int(s.item()) for s in traj['states']]\n",
    "        actions = traj['actions']\n",
    "        for t, action in enumerate(actions):\n",
    "            temporal_state = [-1]*horizon\n",
    "            for h in range(t+1):\n",
    "                temporal_state[h] = states[h]\n",
    "            temporal_tensor = torch.tensor(temporal_state, dtype=torch.long)\n",
    "            # encoded = encode_temporal_state(temporal_tensor, embed_table) # 使用嵌入表编码\n",
    "            # temporal_encoder = TemporalStateEncoder() #使用lstm+embedding编码\n",
    "            vec = encode_temporal_state2(temporal_tensor, temporal_encoder)\n",
    "            state_vectors.append(vec.detach())\n",
    "            action_labels.append(action)\n",
    "            traj_ids.append(traj_id)\n",
    "\n",
    "    print(f\"Total states collected: {len(state_vectors)}\")\n",
    "    all_states = torch.stack(state_vectors)\n",
    "    all_actions = torch.tensor(action_labels, dtype=torch.long)\n",
    "    traj_ids = torch.tensor(traj_ids)\n",
    "\n",
    "    selected_indices = []\n",
    "    selected_vectors = []\n",
    "    selected_trajs = set()\n",
    "\n",
    "    for _ in range(min(budget, len(all_states))):\n",
    "        best_score, best_idx = -float(\"inf\"), -1\n",
    "\n",
    "        for i in range(len(all_states)):\n",
    "            if i in selected_indices:\n",
    "                continue\n",
    "            if per_traj_limit and traj_ids[i].item() in selected_trajs:\n",
    "                continue\n",
    "\n",
    "            candidate = all_states[i].unsqueeze(0)\n",
    "            reward = torch.abs(candidate).mean().item()\n",
    "\n",
    "            if selected_vectors:\n",
    "                selected_tensor = torch.stack(selected_vectors)\n",
    "                sims = ((candidate - selected_tensor)**2).sum(dim=1)  # Euclidean squared\n",
    "                diversity_penalty = -sims.mean().item()  # maximize distance\n",
    "            else:\n",
    "                diversity_penalty = 0\n",
    "\n",
    "            score = reward + lambda_div * diversity_penalty\n",
    "\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_idx = i\n",
    "\n",
    "        if best_idx == -1:\n",
    "            break\n",
    "\n",
    "        selected_indices.append(best_idx)\n",
    "        selected_vectors.append(all_states[best_idx])\n",
    "        selected_trajs.add(traj_ids[best_idx].item())\n",
    "\n",
    "    return all_states[selected_indices], all_actions[selected_indices],all_states,all_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bb30c7ed768116",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T11:18:45.765877Z",
     "start_time": "2025-07-07T11:18:37.317456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 5202 條軌跡中篩選出最好的 50 條。\n",
      "Embedding(900, 16)\n",
      "Total states collected: 3866\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embed_dim = 16\n",
    "num_states = 900\n",
    "elite_trajectories_data = sample_excellent_trajectories(\n",
    "        filepath=\"two_Room_80_go_explore_archive_file.pkl\", \n",
    "        method='top_n', \n",
    "        n=50)\n",
    "\n",
    "embed_table = torch.nn.Embedding(num_states, 16)\n",
    "temporal_encoder = TemporalStateEncoder(num_states=198, embed_dim=16, hidden_dim=32)\n",
    "print(embed_table)\n",
    "selected_states, selected_actions, all_states,all_actions = submodular_selector_temporal(\n",
    "    trajectories=elite_trajectories_data[:],\n",
    "    embed_table=embed_table,\n",
    "    temporal_encoder=temporal_encoder,\n",
    "    budget=500,\n",
    "    lambda_div=2.0,\n",
    "    per_traj_limit=True,\n",
    "    horizon=params[\"env\"][\"horizon\"]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
