{
 "cells": [
  {
   "cell_type": "code",
   "id": "4ad12547a9074574",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T10:45:54.912067Z",
     "start_time": "2025-06-24T10:45:54.901791Z"
    }
   },
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils.environment import GridWorld\n",
    "from utils.network import append_state\n",
    "from utils.network import policy as agent_net\n",
    "from utils.visualization import Visu\n",
    "from utils.subpo import calculate_submodular_reward, compute_subpo_advantages\n",
    "\n",
    "\n",
    "workspace = \"NM\""
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T10:45:56.547023Z",
     "start_time": "2025-06-24T10:45:56.523475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": \"env_data.pkl\",\n",
    "        \"cov_module\": \"Matern\",\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\"\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.05,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 300\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_1\" \n",
    "\n",
    "params['env']['num'] = 1\n",
    "# start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "#     mode=params[\"visu\"][\"wb\"],\n",
    "#     config=params\n",
    "# )\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "    \n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "# plt, fig = visu.stiener_grid( items_loc=env.items_loc, init=34)\n",
    "# wandb.log({\"chart\": wandb.Image(fig)})\n",
    "# plt.close()\n",
    "# Hori_TransitionMatrix = torch.zeros(node_size*H, node_size*H)\n",
    "# for node in env.horizon_transition_graph.nodes:\n",
    "#     connected_edges = env.horizon_transition_graph.edges(node)\n",
    "#     for u, v in connected_edges:\n",
    "#         Hori_TransitionMatrix[u[0]*node_size+u[1], v[0]*node_size + v[1]] = 1.0\n",
    "env.get_horizon_transition_matrix()\n",
    "# policy = Policy(TransitionMatrix=TransitionMatrix, Hori_TransitionMatrix=Hori_TransitionMatrix, ActionTransitionMatrix=env.Hori_ActionTransitionMatrix[:, :, :, 0],\n",
    "#                 agent_param=params[\"agent\"], env_param=params[\"env\"])\n"
   ],
   "id": "283ca354729c8110",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T01:16:10.126660Z",
     "start_time": "2025-06-25T01:16:10.120683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def select_cell_from_archive(archive):\n",
    "    \"\"\"\n",
    "    Select a cell from the archive for exploration.\n",
    "    Cells with the fewest selection counts are prioritized.\n",
    "    \"\"\"\n",
    "    if not archive:\n",
    "        return None, None\n",
    "\n",
    "    # Find the minimum selection count\n",
    "    min_times_selected = float('inf')\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] < min_times_selected:\n",
    "            min_times_selected = archive[cell_id]['times_selected']\n",
    "    \n",
    "    # Find all cells with the minimum selection count\n",
    "    least_visited_cells = []\n",
    "    for cell_id in archive:\n",
    "        if archive[cell_id]['times_selected'] == min_times_selected:\n",
    "            least_visited_cells.append(cell_id)\n",
    "            \n",
    "    #  Randomly select one of these cells\n",
    "    selected_cell_id = random.choice(least_visited_cells)\n",
    "    \n",
    "    return selected_cell_id, archive[selected_cell_id]\n",
    "\n",
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ],
   "id": "cdb61fd47a1af3d2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T11:46:44.294670Z",
     "start_time": "2025-06-24T11:21:06.341931Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "# --- Core parameter setting ---\n",
    "EXPLORATION_STEPS = 1000000  # The total number of exploration steps can be adjusted as needed, and the larger the value, the more thorough the exploration\n",
    "K_STEPS = 15               # Random exploration steps starting from selected cells \n",
    "\n",
    "# --- Go-Explore main  ---\n",
    "def run_go_explore_phase1_spacetime(env=GridWorld, params=None):\n",
    "    \"\"\"\n",
    "    執行 Go-Explore 的第一階段探索（使用時空細胞）\n",
    "    \"\"\"\n",
    "    print(\"--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\")\n",
    "    \n",
    "    # 1. 初始化存檔庫 (Archive)\n",
    "    # ---【核心修改】：將細胞 Key 定義為 (時間, 狀態ID) ---\n",
    "    # 結構: { (time, state_id): {'reward': float, 'states': list, 'actions': list, 'times_selected': int} }\n",
    "    archive = {}\n",
    "    \n",
    "    env.common_params[\"batch_size\"]=1\n",
    "    env.initialize()\n",
    "    initial_state_tensor = env.state.clone()\n",
    "    initial_state_id = initial_state_tensor.item()\n",
    "    \n",
    "    # ---【核心修改】：初始細胞現在是 (時間=0, 狀態) ---\n",
    "    initial_cell_key = (0, initial_state_id)\n",
    "    \n",
    "    initial_reward = calculate_submodular_reward([initial_state_tensor], env)\n",
    "    \n",
    "    archive[initial_cell_key] = {\n",
    "        'reward': initial_reward,\n",
    "        'states': [initial_state_tensor],\n",
    "        'actions': [],\n",
    "        'times_selected': 0\n",
    "    }\n",
    "    print(f\"初始時空細胞加入存檔庫: Cell {initial_cell_key}, Reward: {initial_reward}\")\n",
    "    \n",
    "    # 2. 執行 N 次探索循環\n",
    "    pbar = tqdm(total=EXPLORATION_STEPS, desc=\"Go-Exploring (Spacetime)\")\n",
    "    for step in range(EXPLORATION_STEPS):\n",
    "        # 2.1 選擇細胞 (選擇函數無需修改，它通用於任何 key)\n",
    "        cell_key_to_explore_from, selected_cell_data = select_cell_from_archive(archive)\n",
    "        \n",
    "        if selected_cell_data is None:\n",
    "            print(\"錯誤：存檔庫為空，無法繼續探索。\")\n",
    "            break\n",
    "            \n",
    "        archive[cell_key_to_explore_from]['times_selected'] += 1\n",
    "\n",
    "        # 2.2 前往 (Go To) 該細胞狀態\n",
    "        env.initialize()\n",
    "        for action in selected_cell_data['actions']:\n",
    "            env.step(0, torch.tensor([action]))\n",
    "\n",
    "        # 2.3 從該狀態開始，隨機探索 (Explore) k 步\n",
    "        current_states = selected_cell_data['states'][:]\n",
    "        current_actions = selected_cell_data['actions'][:]\n",
    "        \n",
    "        for _ in range(K_STEPS):\n",
    "            if len(current_actions) >= params[\"env\"][\"horizon\"] - 1:\n",
    "                break\n",
    "                \n",
    "            random_action = random.randint(0, env.action_dim - 1)\n",
    "            env.step(0, torch.tensor([random_action]))\n",
    "            \n",
    "            new_state_tensor = env.state.clone()\n",
    "            \n",
    "            current_states.append(new_state_tensor)\n",
    "            current_actions.append(random_action)\n",
    "            \n",
    "            # ---【核心修改】：更新存檔庫時使用 (時間, 狀態) 作為 Key ---\n",
    "            new_state_id = new_state_tensor.item()\n",
    "            time_step = len(current_actions) # 當前時間步 = 已執行動作的數量\n",
    "            new_cell_key = (time_step, new_state_id)\n",
    "            \n",
    "            new_reward = calculate_submodular_reward(current_states, env)\n",
    "            \n",
    "            if new_cell_key not in archive or new_reward > archive[new_cell_key]['reward']:\n",
    "                archive[new_cell_key] = {\n",
    "                    'reward': new_reward,\n",
    "                    'states': current_states[:],\n",
    "                    'actions': current_actions[:],\n",
    "                    'times_selected': 0\n",
    "                }\n",
    "        \n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "\n",
    "    # 3. 探索結束\n",
    "    print(\"\\n--- 探索完成 ---\")\n",
    "    if not archive:\n",
    "        print(\"錯誤：存檔庫為空！\")\n",
    "        return None\n",
    "\n",
    "    best_trajectory_data = max(archive.values(), key=lambda x: x['reward'])\n",
    "    max_reward = best_trajectory_data['reward']\n",
    "    print(f\"找到的最佳獎勵值: {max_reward}\")\n",
    "    print(f\"對應的軌跡長度: {len(best_trajectory_data['states'])}\")\n",
    "\n",
    "    # 4. 保存存檔庫\n",
    "    archive_filename = \"go_explore_archive_spacetime.pkl\"\n",
    "    with open(archive_filename, \"wb\") as f:\n",
    "        pickle.dump(archive, f)\n",
    "    print(f\"完整的時空細胞存檔庫已保存至: {archive_filename}\")\n",
    "    \n",
    "    return best_trajectory_data\n",
    "\n",
    "# --- 運行最終升級版的 Go-Explore ---\n",
    "best_found_trajectory = run_go_explore_phase1_spacetime(env, params)\n",
    "\n"
   ],
   "id": "6beefe3c0acb7741",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Go-Explore Phase 1: Spacetime Exploration 開始 ---\n",
      "初始時空細胞加入存檔庫: Cell (0, 34), Reward: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Go-Exploring (Spacetime):   0%|          | 0/1000000 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cb47ff3cd784434bcb54b1b2d7b528e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 探索完成 ---\n",
      "找到的最佳獎勵值: 68\n",
      "對應的軌跡長度: 40\n",
      "完整的時空細胞存檔庫已保存至: go_explore_archive_spacetime.pkl\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-25T02:06:08.538726Z",
     "start_time": "2025-06-25T02:06:08.078718Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(best_found_trajectory)\n",
    "print(calculate_submodular_reward(best_found_trajectory['states'],env))\n",
    "# --- 使用範例 ---\n",
    "\n",
    "# 範例1：獲取獎勵最高的 20 條軌跡\n",
    "print(\"--- 範例 1: 採樣 Top 20 ---\")\n",
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=300)\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")\n",
    "\n",
    "# 範例2：獲取獎勵排名前 5% 的軌跡\n",
    "print(\"--- 範例 2: 採樣 Top 5% ---\")\n",
    "top_5_percent_trajectories = sample_excellent_trajectories(method='top_p', p=0.05)\n",
    "if top_5_percent_trajectories:\n",
    "    # 打印其中一條軌跡的詳細信息以供檢查\n",
    "    sample_traj_data = top_5_percent_trajectories[0]\n",
    "    print(f\"抽樣檢查最好的一條軌跡：獎勵={sample_traj_data['reward']}, 長度={len(sample_traj_data['states'])}\\n\")\n",
    "\n",
    "# 範例3：獲取所有獎勵值大於等於 45 的軌跡\n",
    "print(\"--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\")\n",
    "high_reward_trajectories = sample_excellent_trajectories(method='threshold', threshold=45)\n",
    "if high_reward_trajectories:\n",
    "    print(f\"所有高分軌跡的平均獎勵為: {np.mean([d['reward'] for d in high_reward_trajectories]):.2f}\\n\")\n"
   ],
   "id": "31d60be0ee55504a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'reward': 68, 'states': [tensor([34]), tensor([35]), tensor([36]), tensor([37]), tensor([38]), tensor([24]), tensor([10]), tensor([11]), tensor([12]), tensor([26]), tensor([40]), tensor([54]), tensor([68]), tensor([82]), tensor([81]), tensor([80]), tensor([66]), tensor([52]), tensor([51]), tensor([37]), tensor([36]), tensor([35]), tensor([34]), tensor([33]), tensor([32]), tensor([31]), tensor([30]), tensor([16]), tensor([2]), tensor([1]), tensor([0]), tensor([14]), tensor([28]), tensor([42]), tensor([56]), tensor([70]), tensor([71]), tensor([72]), tensor([58]), tensor([44])], 'actions': [1, 1, 1, 1, 4, 4, 1, 1, 2, 2, 2, 2, 2, 3, 3, 4, 4, 3, 4, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, 2, 2, 2, 2, 1, 1, 4, 4], 'times_selected': 361}\n",
      "68\n",
      "--- 範例 1: 採樣 Top 20 ---\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 300 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 60\n",
      "\n",
      "--- 範例 2: 採樣 Top 5% ---\n",
      "方法: Top-P。從 2312 條軌跡中篩選出最好的前 5.0% (115 條)。\n",
      "抽樣檢查最好的一條軌跡：獎勵=68, 長度=40\n",
      "\n",
      "--- 範例 3: 採樣獎勵 >= 45 的軌跡 ---\n",
      "方法: Threshold。從 2312 條軌跡中篩選出 804 條獎勵不低於 45 的軌跡。\n",
      "所有高分軌跡的平均獎勵為: 56.11\n",
      "\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-24T05:39:17.568219Z",
     "start_time": "2025-06-24T05:37:25.078931Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Mean: 16.84 | Max: 32.00 | Median: 16.00 | Min: 7.00 | Entropy: 1.5830 | Loss: -0.0802\n",
      "Epoch 1 | Mean: 19.65 | Max: 31.00 | Median: 21.00 | Min: 8.00 | Entropy: 1.5573 | Loss: -0.0649\n",
      "Epoch 2 | Mean: 22.50 | Max: 34.00 | Median: 23.00 | Min: 10.00 | Entropy: 1.5602 | Loss: -0.0949\n",
      "Epoch 3 | Mean: 23.14 | Max: 35.00 | Median: 23.00 | Min: 12.00 | Entropy: 1.5839 | Loss: -0.1523\n",
      "Epoch 4 | Mean: 22.84 | Max: 32.00 | Median: 22.00 | Min: 17.00 | Entropy: 1.5485 | Loss: -0.2059\n",
      "Epoch 5 | Mean: 23.82 | Max: 34.00 | Median: 24.00 | Min: 14.00 | Entropy: 1.5358 | Loss: -0.2533\n",
      "Epoch 6 | Mean: 24.75 | Max: 33.00 | Median: 25.00 | Min: 18.00 | Entropy: 1.5549 | Loss: -0.2875\n",
      "Epoch 7 | Mean: 25.57 | Max: 33.00 | Median: 26.00 | Min: 14.00 | Entropy: 1.5556 | Loss: -0.3120\n",
      "Epoch 8 | Mean: 26.03 | Max: 35.00 | Median: 26.00 | Min: 14.00 | Entropy: 1.5302 | Loss: -0.3111\n",
      "Epoch 9 | Mean: 25.78 | Max: 33.00 | Median: 26.00 | Min: 10.00 | Entropy: 1.5429 | Loss: -0.3103\n",
      "Epoch 10 | Mean: 25.53 | Max: 36.00 | Median: 25.00 | Min: 15.00 | Entropy: 1.5764 | Loss: -0.3213\n",
      "Epoch 11 | Mean: 25.66 | Max: 35.00 | Median: 26.00 | Min: 12.00 | Entropy: 1.5751 | Loss: -0.3175\n",
      "Epoch 12 | Mean: 25.44 | Max: 36.00 | Median: 26.00 | Min: 12.00 | Entropy: 1.5753 | Loss: -0.3120\n",
      "Epoch 13 | Mean: 26.02 | Max: 36.00 | Median: 26.00 | Min: 12.00 | Entropy: 1.5881 | Loss: -0.3258\n",
      "Epoch 14 | Mean: 25.74 | Max: 36.00 | Median: 26.00 | Min: 13.00 | Entropy: 1.5972 | Loss: -0.3465\n",
      "Epoch 15 | Mean: 25.59 | Max: 35.00 | Median: 26.00 | Min: 10.00 | Entropy: 1.6022 | Loss: -0.3692\n",
      "Epoch 16 | Mean: 26.06 | Max: 34.00 | Median: 26.00 | Min: 11.00 | Entropy: 1.6053 | Loss: -0.3797\n",
      "Epoch 17 | Mean: 25.94 | Max: 35.00 | Median: 26.00 | Min: 8.00 | Entropy: 1.6012 | Loss: -0.4050\n",
      "Epoch 18 | Mean: 26.27 | Max: 36.00 | Median: 27.00 | Min: 12.00 | Entropy: 1.5911 | Loss: -0.4322\n",
      "Epoch 19 | Mean: 26.38 | Max: 35.00 | Median: 27.00 | Min: 15.00 | Entropy: 1.5757 | Loss: -0.4346\n",
      "Epoch 20 | Mean: 27.07 | Max: 34.00 | Median: 28.00 | Min: 15.00 | Entropy: 1.5734 | Loss: -0.4522\n",
      "Epoch 21 | Mean: 27.01 | Max: 37.00 | Median: 27.00 | Min: 14.00 | Entropy: 1.5751 | Loss: -0.4546\n",
      "Epoch 22 | Mean: 26.96 | Max: 35.00 | Median: 27.00 | Min: 13.00 | Entropy: 1.5730 | Loss: -0.4688\n",
      "Epoch 23 | Mean: 27.00 | Max: 34.00 | Median: 27.00 | Min: 16.00 | Entropy: 1.5710 | Loss: -0.4589\n",
      "Epoch 24 | Mean: 27.30 | Max: 37.00 | Median: 27.00 | Min: 19.00 | Entropy: 1.5736 | Loss: -0.4630\n",
      "Epoch 25 | Mean: 27.73 | Max: 39.00 | Median: 28.00 | Min: 17.00 | Entropy: 1.5762 | Loss: -0.4872\n",
      "Epoch 26 | Mean: 27.74 | Max: 39.00 | Median: 28.00 | Min: 15.00 | Entropy: 1.5850 | Loss: -0.4787\n",
      "Epoch 27 | Mean: 28.11 | Max: 40.00 | Median: 28.00 | Min: 19.00 | Entropy: 1.5969 | Loss: -0.4895\n",
      "Epoch 28 | Mean: 27.94 | Max: 36.00 | Median: 28.00 | Min: 15.00 | Entropy: 1.5913 | Loss: -0.4991\n",
      "Epoch 29 | Mean: 27.46 | Max: 36.00 | Median: 28.00 | Min: 17.00 | Entropy: 1.5767 | Loss: -0.4944\n",
      "Epoch 30 | Mean: 27.61 | Max: 37.00 | Median: 28.00 | Min: 12.00 | Entropy: 1.5637 | Loss: -0.4963\n",
      "Epoch 31 | Mean: 27.87 | Max: 36.00 | Median: 28.00 | Min: 15.00 | Entropy: 1.5797 | Loss: -0.5202\n",
      "Epoch 32 | Mean: 28.15 | Max: 39.00 | Median: 28.00 | Min: 16.00 | Entropy: 1.5870 | Loss: -0.5123\n",
      "Epoch 33 | Mean: 28.06 | Max: 36.00 | Median: 28.00 | Min: 16.00 | Entropy: 1.6002 | Loss: -0.5137\n",
      "Epoch 34 | Mean: 28.06 | Max: 36.00 | Median: 28.00 | Min: 14.00 | Entropy: 1.6027 | Loss: -0.5101\n",
      "Epoch 35 | Mean: 28.17 | Max: 39.00 | Median: 29.00 | Min: 14.00 | Entropy: 1.5950 | Loss: -0.5237\n",
      "Epoch 36 | Mean: 28.41 | Max: 40.00 | Median: 29.00 | Min: 17.00 | Entropy: 1.5808 | Loss: -0.5075\n",
      "Epoch 37 | Mean: 28.61 | Max: 42.00 | Median: 29.00 | Min: 15.00 | Entropy: 1.5706 | Loss: -0.5141\n",
      "Epoch 38 | Mean: 28.61 | Max: 41.00 | Median: 28.00 | Min: 17.00 | Entropy: 1.5587 | Loss: -0.4920\n",
      "Epoch 39 | Mean: 29.17 | Max: 39.00 | Median: 29.00 | Min: 15.00 | Entropy: 1.5746 | Loss: -0.5237\n",
      "Epoch 40 | Mean: 28.98 | Max: 39.00 | Median: 29.00 | Min: 18.00 | Entropy: 1.5937 | Loss: -0.5508\n",
      "Epoch 41 | Mean: 29.47 | Max: 37.00 | Median: 30.00 | Min: 20.00 | Entropy: 1.6033 | Loss: -0.5920\n",
      "Epoch 42 | Mean: 29.92 | Max: 39.00 | Median: 30.00 | Min: 21.00 | Entropy: 1.5940 | Loss: -0.6017\n",
      "Epoch 43 | Mean: 30.24 | Max: 40.00 | Median: 31.00 | Min: 20.00 | Entropy: 1.5883 | Loss: -0.6080\n",
      "Epoch 44 | Mean: 30.18 | Max: 39.00 | Median: 30.00 | Min: 22.00 | Entropy: 1.5743 | Loss: -0.6148\n",
      "Epoch 45 | Mean: 29.72 | Max: 36.00 | Median: 30.00 | Min: 21.00 | Entropy: 1.5495 | Loss: -0.6105\n",
      "Epoch 46 | Mean: 30.16 | Max: 38.00 | Median: 30.00 | Min: 20.00 | Entropy: 1.5693 | Loss: -0.6094\n",
      "Epoch 47 | Mean: 30.50 | Max: 40.00 | Median: 31.00 | Min: 17.00 | Entropy: 1.5823 | Loss: -0.6141\n",
      "Epoch 48 | Mean: 30.87 | Max: 43.00 | Median: 31.00 | Min: 20.00 | Entropy: 1.5632 | Loss: -0.5913\n",
      "Epoch 49 | Mean: 31.71 | Max: 43.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5259 | Loss: -0.5627\n",
      "Epoch 50 | Mean: 31.75 | Max: 43.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5281 | Loss: -0.5538\n",
      "Epoch 51 | Mean: 31.20 | Max: 45.00 | Median: 31.00 | Min: 20.00 | Entropy: 1.5731 | Loss: -0.5876\n",
      "Epoch 52 | Mean: 30.65 | Max: 39.00 | Median: 31.00 | Min: 18.00 | Entropy: 1.5364 | Loss: -0.6100\n",
      "Epoch 53 | Mean: 30.94 | Max: 43.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5537 | Loss: -0.6122\n",
      "Epoch 54 | Mean: 30.27 | Max: 40.00 | Median: 30.00 | Min: 20.00 | Entropy: 1.5851 | Loss: -0.5919\n",
      "Epoch 55 | Mean: 30.77 | Max: 46.00 | Median: 31.00 | Min: 21.00 | Entropy: 1.5660 | Loss: -0.5731\n",
      "Epoch 56 | Mean: 30.46 | Max: 43.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5732 | Loss: -0.5734\n",
      "Epoch 57 | Mean: 30.43 | Max: 42.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5950 | Loss: -0.5876\n",
      "Epoch 58 | Mean: 30.67 | Max: 44.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5983 | Loss: -0.6265\n",
      "Epoch 59 | Mean: 31.02 | Max: 38.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5822 | Loss: -0.6522\n",
      "Epoch 60 | Mean: 31.24 | Max: 38.00 | Median: 32.00 | Min: 21.00 | Entropy: 1.5648 | Loss: -0.6647\n",
      "Epoch 61 | Mean: 30.87 | Max: 39.00 | Median: 32.00 | Min: 15.00 | Entropy: 1.5533 | Loss: -0.6627\n",
      "Epoch 62 | Mean: 30.78 | Max: 38.00 | Median: 31.00 | Min: 24.00 | Entropy: 1.5480 | Loss: -0.6737\n",
      "Epoch 63 | Mean: 31.32 | Max: 39.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5531 | Loss: -0.6710\n",
      "Epoch 64 | Mean: 31.49 | Max: 40.00 | Median: 32.00 | Min: 15.00 | Entropy: 1.5557 | Loss: -0.6733\n",
      "Epoch 65 | Mean: 31.96 | Max: 40.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5664 | Loss: -0.6768\n",
      "Epoch 66 | Mean: 31.64 | Max: 44.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5794 | Loss: -0.6820\n",
      "Epoch 67 | Mean: 31.30 | Max: 41.00 | Median: 32.00 | Min: 21.00 | Entropy: 1.5869 | Loss: -0.6780\n",
      "Epoch 68 | Mean: 31.31 | Max: 40.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5969 | Loss: -0.6948\n",
      "Epoch 69 | Mean: 31.17 | Max: 44.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.6007 | Loss: -0.6918\n",
      "Epoch 70 | Mean: 30.89 | Max: 38.00 | Median: 31.00 | Min: 24.00 | Entropy: 1.6000 | Loss: -0.6857\n",
      "Epoch 71 | Mean: 30.69 | Max: 38.00 | Median: 31.00 | Min: 24.00 | Entropy: 1.5970 | Loss: -0.6919\n",
      "Epoch 72 | Mean: 30.52 | Max: 38.00 | Median: 31.00 | Min: 24.00 | Entropy: 1.5961 | Loss: -0.6943\n",
      "Epoch 73 | Mean: 30.50 | Max: 37.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5926 | Loss: -0.6900\n",
      "Epoch 74 | Mean: 30.53 | Max: 36.00 | Median: 31.00 | Min: 23.00 | Entropy: 1.5971 | Loss: -0.6880\n",
      "Epoch 75 | Mean: 30.53 | Max: 40.00 | Median: 31.00 | Min: 21.00 | Entropy: 1.5996 | Loss: -0.6744\n",
      "Epoch 76 | Mean: 31.03 | Max: 41.00 | Median: 31.00 | Min: 24.00 | Entropy: 1.5976 | Loss: -0.6680\n",
      "Epoch 77 | Mean: 31.45 | Max: 48.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5907 | Loss: -0.6630\n",
      "Epoch 78 | Mean: 31.27 | Max: 42.00 | Median: 31.00 | Min: 22.00 | Entropy: 1.5870 | Loss: -0.6552\n",
      "Epoch 79 | Mean: 31.37 | Max: 45.00 | Median: 31.00 | Min: 20.00 | Entropy: 1.5873 | Loss: -0.6659\n",
      "Epoch 80 | Mean: 31.15 | Max: 44.00 | Median: 31.00 | Min: 21.00 | Entropy: 1.5921 | Loss: -0.6718\n",
      "Epoch 81 | Mean: 31.24 | Max: 43.00 | Median: 32.00 | Min: 21.00 | Entropy: 1.5939 | Loss: -0.6943\n",
      "Epoch 82 | Mean: 31.51 | Max: 38.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5910 | Loss: -0.6944\n",
      "Epoch 83 | Mean: 31.42 | Max: 39.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5887 | Loss: -0.6970\n",
      "Epoch 84 | Mean: 32.01 | Max: 42.00 | Median: 32.00 | Min: 25.00 | Entropy: 1.5885 | Loss: -0.7007\n",
      "Epoch 85 | Mean: 32.37 | Max: 45.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5881 | Loss: -0.7000\n",
      "Epoch 86 | Mean: 32.44 | Max: 42.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5938 | Loss: -0.6914\n",
      "Epoch 87 | Mean: 32.00 | Max: 40.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5973 | Loss: -0.6876\n",
      "Epoch 88 | Mean: 32.02 | Max: 41.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5998 | Loss: -0.6931\n",
      "Epoch 89 | Mean: 31.66 | Max: 41.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.6008 | Loss: -0.6896\n",
      "Epoch 90 | Mean: 31.39 | Max: 43.00 | Median: 32.00 | Min: 18.00 | Entropy: 1.6002 | Loss: -0.6678\n",
      "Epoch 91 | Mean: 31.78 | Max: 46.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5946 | Loss: -0.6497\n",
      "Epoch 92 | Mean: 31.49 | Max: 47.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5911 | Loss: -0.6337\n",
      "Epoch 93 | Mean: 31.46 | Max: 45.00 | Median: 32.00 | Min: 25.00 | Entropy: 1.5879 | Loss: -0.6489\n",
      "Epoch 94 | Mean: 31.31 | Max: 42.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5960 | Loss: -0.6402\n",
      "Epoch 95 | Mean: 31.83 | Max: 47.00 | Median: 32.00 | Min: 25.00 | Entropy: 1.5997 | Loss: -0.6302\n",
      "Epoch 96 | Mean: 32.10 | Max: 46.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5911 | Loss: -0.6249\n",
      "Epoch 97 | Mean: 32.26 | Max: 45.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5946 | Loss: -0.6364\n",
      "Epoch 98 | Mean: 32.25 | Max: 45.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.6050 | Loss: -0.6565\n",
      "Epoch 99 | Mean: 31.70 | Max: 38.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.6034 | Loss: -0.6805\n",
      "Epoch 100 | Mean: 31.65 | Max: 40.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5931 | Loss: -0.6956\n",
      "Epoch 101 | Mean: 31.54 | Max: 39.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5811 | Loss: -0.7006\n",
      "Epoch 102 | Mean: 31.40 | Max: 40.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5801 | Loss: -0.7015\n",
      "Epoch 103 | Mean: 31.59 | Max: 41.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5911 | Loss: -0.7044\n",
      "Epoch 104 | Mean: 31.78 | Max: 41.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5915 | Loss: -0.6986\n",
      "Epoch 105 | Mean: 32.05 | Max: 46.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5744 | Loss: -0.6717\n",
      "Epoch 106 | Mean: 32.12 | Max: 44.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5688 | Loss: -0.6744\n",
      "Epoch 107 | Mean: 32.29 | Max: 43.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5761 | Loss: -0.6726\n",
      "Epoch 108 | Mean: 32.16 | Max: 43.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5933 | Loss: -0.6827\n",
      "Epoch 109 | Mean: 32.33 | Max: 48.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.6046 | Loss: -0.6992\n",
      "Epoch 110 | Mean: 32.36 | Max: 40.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5945 | Loss: -0.7022\n",
      "Epoch 111 | Mean: 32.02 | Max: 40.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5766 | Loss: -0.6963\n",
      "Epoch 112 | Mean: 32.48 | Max: 40.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5785 | Loss: -0.7017\n",
      "Epoch 113 | Mean: 32.60 | Max: 40.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5830 | Loss: -0.6862\n",
      "Epoch 114 | Mean: 33.11 | Max: 46.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5525 | Loss: -0.6620\n",
      "Epoch 115 | Mean: 33.94 | Max: 47.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5021 | Loss: -0.6407\n",
      "Epoch 116 | Mean: 33.67 | Max: 47.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5001 | Loss: -0.6331\n",
      "Epoch 117 | Mean: 33.69 | Max: 46.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5274 | Loss: -0.6377\n",
      "Epoch 118 | Mean: 33.09 | Max: 46.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5676 | Loss: -0.6645\n",
      "Epoch 119 | Mean: 32.62 | Max: 43.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5946 | Loss: -0.6849\n",
      "Epoch 120 | Mean: 32.08 | Max: 42.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5990 | Loss: -0.7030\n",
      "Epoch 121 | Mean: 32.00 | Max: 40.00 | Median: 32.00 | Min: 21.00 | Entropy: 1.5911 | Loss: -0.7172\n",
      "Epoch 122 | Mean: 31.49 | Max: 40.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5810 | Loss: -0.6951\n",
      "Epoch 123 | Mean: 31.85 | Max: 44.00 | Median: 32.00 | Min: 25.00 | Entropy: 1.5837 | Loss: -0.6899\n",
      "Epoch 124 | Mean: 31.81 | Max: 40.00 | Median: 32.00 | Min: 27.00 | Entropy: 1.5912 | Loss: -0.6816\n",
      "Epoch 125 | Mean: 32.16 | Max: 44.00 | Median: 32.00 | Min: 27.00 | Entropy: 1.5989 | Loss: -0.6672\n",
      "Epoch 126 | Mean: 32.64 | Max: 47.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5922 | Loss: -0.6574\n",
      "Epoch 127 | Mean: 32.79 | Max: 47.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5815 | Loss: -0.6416\n",
      "Epoch 128 | Mean: 33.41 | Max: 48.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5697 | Loss: -0.6368\n",
      "Epoch 129 | Mean: 32.69 | Max: 45.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5641 | Loss: -0.6195\n",
      "Epoch 130 | Mean: 32.75 | Max: 44.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5656 | Loss: -0.6164\n",
      "Epoch 131 | Mean: 33.16 | Max: 44.00 | Median: 32.00 | Min: 27.00 | Entropy: 1.5766 | Loss: -0.6322\n",
      "Epoch 132 | Mean: 32.90 | Max: 45.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5872 | Loss: -0.6552\n",
      "Epoch 133 | Mean: 32.71 | Max: 45.00 | Median: 32.00 | Min: 20.00 | Entropy: 1.5926 | Loss: -0.6629\n",
      "Epoch 134 | Mean: 32.48 | Max: 46.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5942 | Loss: -0.6798\n",
      "Epoch 135 | Mean: 32.05 | Max: 42.00 | Median: 32.00 | Min: 25.00 | Entropy: 1.5908 | Loss: -0.6871\n",
      "Epoch 136 | Mean: 32.56 | Max: 43.00 | Median: 32.00 | Min: 24.00 | Entropy: 1.5845 | Loss: -0.7106\n",
      "Epoch 137 | Mean: 32.17 | Max: 40.00 | Median: 32.00 | Min: 23.00 | Entropy: 1.5772 | Loss: -0.7215\n",
      "Epoch 138 | Mean: 32.00 | Max: 40.00 | Median: 32.00 | Min: 22.00 | Entropy: 1.5753 | Loss: -0.7170\n",
      "Epoch 139 | Mean: 32.12 | Max: 40.00 | Median: 32.00 | Min: 26.00 | Entropy: 1.5806 | Loss: -0.7261\n"
     ]
    }
   ],
   "execution_count": 8,
   "source": [
    "\n",
    "# Agent's policy\n",
    "if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "    agent = agent_net(2, env.action_dim)\n",
    "else:\n",
    "    agent = agent_net(H-1, env.action_dim)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=params[\"alg\"][\"lr\"])\n",
    "\n",
    "for t_eps in range(epochs):\n",
    "    mat_action = []\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    marginal_return = []\n",
    "    mat_done = []\n",
    "    \n",
    "    # Used to store data for all trajectories within a batch (new)\n",
    "    batch_trajectories = [[] for _ in range(params[\"common\"][\"batch_size\"])]\n",
    "    batch_log_probs = [[] for _ in range(params[\"common\"][\"batch_size\"])]\n",
    "    \n",
    "    # print(t_eps)\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    # print(torch.mean(env.weighted_traj_return([init_state])))\n",
    "    # print(t_eps, \" \", mat_state, \" \", env.weighted_traj_return(mat_state))\n",
    "    list_batch_state = []\n",
    "    for h_iter in range(H-1):\n",
    "        if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "            batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "            # append time index to the state\n",
    "            batch_state = torch.cat(\n",
    "                [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "        else:\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "        action_prob = agent(batch_state)\n",
    "        # action_prob = pi_h_s_a[h_iter, mat_state[-1]]\n",
    "        # policy_dist = Categorical(torch.nn.Softmax()(action_prob))\n",
    "        policy_dist = Categorical(action_prob)\n",
    "        actions = policy_dist.sample()\n",
    "        \n",
    "        # TODO： Record log_deb for each trajectory in the batch (new)\n",
    "        log_probs = policy_dist.log_prob(actions)\n",
    "        for i in range(params[\"common\"][\"batch_size\"]):\n",
    "            batch_log_probs[i].append(log_probs[i])\n",
    "        \n",
    "        mat_action.append(actions)\n",
    "        \n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "        # print(t_eps, \" \", mat_state, \" \", env.weighted_traj_return(mat_state))\n",
    "        # mat_return.append(env.batched_marginal_coverage(\n",
    "        #     mat_state, [init_state]))\n",
    "        \n",
    "        # TODO：Record the status for each trajectory in the batch\n",
    "        for i in range(params[\"common\"][\"batch_size\"]):\n",
    "            # The shape of env.state is (batch_size, 1)，, and we take the state of the i sample \n",
    "            batch_trajectories[i].append(env.state[i])\n",
    "            \n",
    "        mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "        if h_iter ==0:\n",
    "            marginal_return.append(mat_return[h_iter])\n",
    "        else:\n",
    "            # if params[\"alg\"][\"type\"]==\"SRL\":\n",
    "            marginal_return.append(mat_return[h_iter])\n",
    "            # else:\n",
    "            # marginal_return.append(mat_return[h_iter] - mat_return[h_iter-1])\n",
    "        list_batch_state.append(batch_state)\n",
    "        # mat_return.append(env.weighted_traj_return(\n",
    "        #     mat_state) - env.weighted_traj_return([init_state]))\n",
    "\n",
    "    ###################\n",
    "    # Compute gradients\n",
    "    ###################\n",
    "    # --- Gradient calculation and update stage ---\n",
    "    all_advantages = []\n",
    "    final_rewards = []\n",
    "    # 1. Calculate the corresponding SUBPO advantage function for each trajectory in the batch\n",
    "    for i in range(params[\"common\"][\"batch_size\"]):\n",
    "        # Construct a single trajectory, and add an initial state\n",
    "        single_trajectory = [init_state[i]] + batch_trajectories[i]\n",
    "        \n",
    "        # 計算這條軌跡的最終獎勵，用於監控\n",
    "        final_rewards.append(calculate_submodular_reward(single_trajectory, env))\n",
    "\n",
    "        # 計算正確的 SUBPO 優勢函數\n",
    "        advantages = compute_subpo_advantages(single_trajectory, env)\n",
    "        all_advantages.extend(advantages[:-1]) \n",
    "\n",
    "\n",
    "    all_advantages = torch.tensor(all_advantages, dtype=torch.float32)\n",
    "\n",
    "    # Convert log_debs from a list of lists to a flat tensor\n",
    "    flat_log_probs = torch.cat([torch.stack(lp) for lp in batch_log_probs])\n",
    "\n",
    "    # 標準化優勢函數\n",
    "    if len(all_advantages) > 1:\n",
    "        all_advantages = (all_advantages - all_advantages.mean()) / (all_advantages.std() + 1e-9)\n",
    "\n",
    "    # 2. 計算損失函數\n",
    "    ent_coef = params[\"alg\"][\"ent_coef\"]\n",
    "    # 移除熵的衰減，使用固定的係數以保持探索\n",
    "    entropy_term = policy_dist.entropy().mean() \n",
    "    \n",
    "    loss = -1 * (torch.mean(flat_log_probs * all_advantages) + ent_coef * entropy_term)\n",
    "\n",
    "    # 3. 更新梯度\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    # --- 打印日誌 ---\n",
    "    obj_mean = torch.tensor(final_rewards).float().mean()\n",
    "    obj_max = torch.tensor(final_rewards).float().max()\n",
    "    obj_median = torch.tensor(final_rewards).float().median()\n",
    "    obj_min = torch.tensor(final_rewards).float().min()\n",
    "\n",
    "    if t_eps % 1 == 0:\n",
    "        print(f\"Epoch {t_eps} | Mean: {obj_mean:.2f} | Max: {obj_max:.2f} | Median: {obj_median:.2f} | Min: {obj_min:.2f} | Entropy: {entropy_term:.4f} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "    \n",
    "# wandb.finish()"
   ],
   "id": "6726e4739a3f5731"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-24T05:30:22.085127Z",
     "start_time": "2025-06-24T05:30:16.523999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params[\"common\"][\"batch_size\"]=10000\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "env.get_horizon_transition_matrix()\n",
    "env.initialize()\n",
    "init_state = env.state\n",
    "mat_action = []\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "marginal_return = []\n",
    "mat_state.append(env.state)\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = env.state.reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    action_prob = agent(batch_state)\n",
    "    policy_dist = Categorical(action_prob)\n",
    "    actions = policy_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_action.append(actions)\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    # print(\"Action \", actions, \" state \", env.state,\" mat return \", mat_return[-1])\n",
    "obj = env.weighted_traj_return(mat_state).float()\n",
    "print( \" mean \", obj.mean(), \" max \",\n",
    "          obj.max(), \" median \", obj.median(), \" min \", obj.min(), \" ent \", policy_dist.entropy().mean().detach())\n",
    "max_index = torch.argmax(obj)\n",
    "print(\"Max index \", max_index)\n",
    "for i in range(len(mat_state)-1):\n",
    "    print(\"State \", i, \" \", mat_state[i][max_index], \" Action \", mat_action[i][max_index], \" Return \", mat_return[i][max_index])"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " mean  tensor(34.9254)  max  tensor(50.)  median  tensor(36.)  min  tensor(18.)  ent  tensor(1.5026)\n",
      "Max index  tensor(3885)\n",
      "State  0   tensor(34)  Action  tensor(1)  Return  tensor(6)\n",
      "State  1   tensor(35)  Action  tensor(1)  Return  tensor(8)\n",
      "State  2   tensor(36)  Action  tensor(1)  Return  tensor(10)\n",
      "State  3   tensor(37)  Action  tensor(1)  Return  tensor(12)\n",
      "State  4   tensor(38)  Action  tensor(1)  Return  tensor(14)\n",
      "State  5   tensor(39)  Action  tensor(4)  Return  tensor(16)\n",
      "State  6   tensor(25)  Action  tensor(1)  Return  tensor(18)\n",
      "State  7   tensor(26)  Action  tensor(2)  Return  tensor(19)\n",
      "State  8   tensor(40)  Action  tensor(2)  Return  tensor(21)\n",
      "State  9   tensor(54)  Action  tensor(2)  Return  tensor(23)\n",
      "State  10   tensor(68)  Action  tensor(2)  Return  tensor(25)\n",
      "State  11   tensor(82)  Action  tensor(4)  Return  tensor(25)\n",
      "State  12   tensor(68)  Action  tensor(3)  Return  tensor(27)\n",
      "State  13   tensor(67)  Action  tensor(3)  Return  tensor(29)\n",
      "State  14   tensor(66)  Action  tensor(4)  Return  tensor(29)\n",
      "State  15   tensor(52)  Action  tensor(1)  Return  tensor(29)\n",
      "State  16   tensor(53)  Action  tensor(3)  Return  tensor(29)\n",
      "State  17   tensor(52)  Action  tensor(3)  Return  tensor(29)\n",
      "State  18   tensor(51)  Action  tensor(3)  Return  tensor(29)\n",
      "State  19   tensor(50)  Action  tensor(3)  Return  tensor(29)\n",
      "State  20   tensor(49)  Action  tensor(3)  Return  tensor(29)\n",
      "State  21   tensor(48)  Action  tensor(3)  Return  tensor(30)\n",
      "State  22   tensor(47)  Action  tensor(3)  Return  tensor(31)\n",
      "State  23   tensor(46)  Action  tensor(4)  Return  tensor(33)\n",
      "State  24   tensor(32)  Action  tensor(2)  Return  tensor(33)\n",
      "State  25   tensor(46)  Action  tensor(3)  Return  tensor(35)\n",
      "State  26   tensor(45)  Action  tensor(2)  Return  tensor(36)\n",
      "State  27   tensor(59)  Action  tensor(3)  Return  tensor(38)\n",
      "State  28   tensor(58)  Action  tensor(3)  Return  tensor(40)\n",
      "State  29   tensor(57)  Action  tensor(4)  Return  tensor(42)\n",
      "State  30   tensor(43)  Action  tensor(4)  Return  tensor(44)\n",
      "State  31   tensor(29)  Action  tensor(3)  Return  tensor(46)\n",
      "State  32   tensor(28)  Action  tensor(0)  Return  tensor(46)\n",
      "State  33   tensor(28)  Action  tensor(1)  Return  tensor(46)\n",
      "State  34   tensor(29)  Action  tensor(3)  Return  tensor(46)\n",
      "State  35   tensor(28)  Action  tensor(2)  Return  tensor(47)\n",
      "State  36   tensor(42)  Action  tensor(2)  Return  tensor(48)\n",
      "State  37   tensor(56)  Action  tensor(2)  Return  tensor(50)\n",
      "State  38   tensor(70)  Action  tensor(0)  Return  tensor(50)\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a2c430c2405f5441"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
