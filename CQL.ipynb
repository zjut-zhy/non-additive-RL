{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections \n",
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c02f561",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetDiscrete(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNetDiscrete, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        logits = self.fc_out(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        return action, log_prob, probs\n",
    "\n",
    "class QValueNetDiscrete(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNetDiscrete, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)\n",
    "\n",
    "class CQLDiscrete:\n",
    "    ''' 离散动作CQL算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim,\n",
    "                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,\n",
    "                 device, beta, num_random):\n",
    "        self.action_dim = action_dim\n",
    "        self.device = device\n",
    "        \n",
    "        self.actor = PolicyNetDiscrete(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic_1 = QValueNetDiscrete(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic_2 = QValueNetDiscrete(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNetDiscrete(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_2 = QValueNetDiscrete(state_dim, hidden_dim, action_dim).to(device)\n",
    "        \n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        \n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(), lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(), lr=critic_lr)\n",
    "        \n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float).to(device)\n",
    "        self.log_alpha.requires_grad = True\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=alpha_lr)\n",
    "        \n",
    "        self.target_entropy = target_entropy\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.beta = beta\n",
    "        self.num_random = num_random\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        action, _, _ = self.actor(state)\n",
    "        return action.item()\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(), net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) + \n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'], dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions'], dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'], dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'], dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        \n",
    "        # 计算下一状态的价值\n",
    "        with torch.no_grad():\n",
    "            next_actions, next_log_probs, next_probs = self.actor(next_states)\n",
    "            next_q1 = self.target_critic_1(next_states)\n",
    "            next_q2 = self.target_critic_2(next_states)\n",
    "            next_q = torch.min(next_q1, next_q2)\n",
    "            \n",
    "            # 计算期望价值 E[Q(s',a')] - α*E[log π(a'|s')]\n",
    "            next_value = torch.sum(next_probs * (next_q - self.log_alpha.exp() * torch.log(next_probs + 1e-8)), dim=1, keepdim=True)\n",
    "            td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "\n",
    "        # 当前Q值\n",
    "        current_q1 = self.critic_1(states).gather(1, actions.unsqueeze(1))\n",
    "        current_q2 = self.critic_2(states).gather(1, actions.unsqueeze(1))\n",
    "        \n",
    "        # SAC损失\n",
    "        critic_1_loss = F.mse_loss(current_q1, td_target.detach())\n",
    "        critic_2_loss = F.mse_loss(current_q2, td_target.detach())\n",
    "\n",
    "        # CQL额外部分 - 离散动作版本\n",
    "        batch_size = states.shape[0]\n",
    "        \n",
    "        # 获取所有Q值\n",
    "        q1_all = self.critic_1(states)  # [batch_size, action_dim]\n",
    "        q2_all = self.critic_2(states)  # [batch_size, action_dim]\n",
    "        \n",
    "        # 当前策略的动作概率\n",
    "        _, _, curr_probs = self.actor(states)\n",
    "        curr_log_probs = torch.log(curr_probs + 1e-8)\n",
    "        \n",
    "        # 均匀分布的log概率\n",
    "        uniform_log_prob = -np.log(self.action_dim)\n",
    "        \n",
    "        # CQL损失项1: logsumexp over all actions\n",
    "        # 这里我们对所有动作计算，不需要额外采样\n",
    "        q1_logsumexp = torch.logsumexp(q1_all - curr_log_probs.detach(), dim=1).mean()\n",
    "        q2_logsumexp = torch.logsumexp(q2_all - curr_log_probs.detach(), dim=1).mean()\n",
    "        \n",
    "        # CQL损失项2: 数据集中实际动作的Q值\n",
    "        q1_dataset = current_q1.mean()\n",
    "        q2_dataset = current_q2.mean()\n",
    "        \n",
    "        # 组合CQL损失\n",
    "        qf1_loss = critic_1_loss + self.beta * (q1_logsumexp - q1_dataset)\n",
    "        qf2_loss = critic_2_loss + self.beta * (q2_logsumexp - q2_dataset)\n",
    "\n",
    "        # 更新Q网络\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward()\n",
    "        self.critic_1_optimizer.step()\n",
    "        \n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward()\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        _, log_probs, probs = self.actor(states)\n",
    "        q1_new = self.critic_1(states)\n",
    "        q2_new = self.critic_2(states)\n",
    "        q_new = torch.min(q1_new, q2_new)\n",
    "        \n",
    "        # 策略损失：最大化 E[Q(s,a)] - α*E[log π(a|s)]\n",
    "        actor_loss = torch.sum(probs * (self.log_alpha.exp() * torch.log(probs + 1e-8) - q_new), dim=1).mean()\n",
    "        \n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        entropy = -torch.sum(probs * torch.log(probs + 1e-8), dim=1).mean()\n",
    "        alpha_loss = (entropy.detach() - self.target_entropy) * self.log_alpha.exp()\n",
    "        \n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        # 软更新目标网络\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 1000000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 50, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "None  mean  tensor(19.5897)  max  tensor(36.)  median  tensor(21.)  min  tensor(6.)  ent  tensor(1.5753)\n",
      "None  mean  tensor(21.3150)  max  tensor(32.)  median  tensor(21.)  min  tensor(14.)  ent  tensor(1.0201)\n",
      "None  mean  tensor(24.7990)  max  tensor(36.)  median  tensor(25.)  min  tensor(16.)  ent  tensor(1.1896)\n",
      "None  mean  tensor(26.6470)  max  tensor(43.)  median  tensor(27.)  min  tensor(9.)  ent  tensor(1.2414)\n",
      "None  mean  tensor(28.9817)  max  tensor(50.)  median  tensor(28.)  min  tensor(10.)  ent  tensor(1.1752)\n",
      "None  mean  tensor(32.5450)  max  tensor(51.)  median  tensor(34.)  min  tensor(14.)  ent  tensor(0.9818)\n",
      "None  mean  tensor(31.9210)  max  tensor(46.)  median  tensor(34.)  min  tensor(14.)  ent  tensor(0.7728)\n",
      "None  mean  tensor(31.3343)  max  tensor(47.)  median  tensor(32.)  min  tensor(14.)  ent  tensor(0.7193)\n",
      "None  mean  tensor(31.6260)  max  tensor(50.)  median  tensor(31.)  min  tensor(15.)  ent  tensor(0.7416)\n",
      "None  mean  tensor(32.8493)  max  tensor(50.)  median  tensor(32.)  min  tensor(16.)  ent  tensor(0.7963)\n",
      "None  mean  tensor(34.8247)  max  tensor(52.)  median  tensor(36.)  min  tensor(16.)  ent  tensor(0.8347)\n",
      "None  mean  tensor(36.6410)  max  tensor(51.)  median  tensor(38.)  min  tensor(16.)  ent  tensor(0.8170)\n",
      "None  mean  tensor(37.7767)  max  tensor(50.)  median  tensor(40.)  min  tensor(16.)  ent  tensor(0.7546)\n",
      "None  mean  tensor(37.9300)  max  tensor(50.)  median  tensor(40.)  min  tensor(14.)  ent  tensor(0.6615)\n",
      "None  mean  tensor(37.4970)  max  tensor(48.)  median  tensor(39.)  min  tensor(14.)  ent  tensor(0.5709)\n",
      "None  mean  tensor(37.2877)  max  tensor(49.)  median  tensor(39.)  min  tensor(15.)  ent  tensor(0.5270)\n",
      "None  mean  tensor(37.7970)  max  tensor(48.)  median  tensor(39.)  min  tensor(14.)  ent  tensor(0.5311)\n",
      "None  mean  tensor(37.9127)  max  tensor(49.)  median  tensor(39.)  min  tensor(15.)  ent  tensor(0.5566)\n",
      "None  mean  tensor(38.2113)  max  tensor(48.)  median  tensor(39.)  min  tensor(15.)  ent  tensor(0.5714)\n",
      "None  mean  tensor(37.8327)  max  tensor(47.)  median  tensor(39.)  min  tensor(15.)  ent  tensor(0.5576)\n",
      "None  mean  tensor(38.3950)  max  tensor(48.)  median  tensor(39.)  min  tensor(16.)  ent  tensor(0.5206)\n",
      "None  mean  tensor(39.5140)  max  tensor(48.)  median  tensor(41.)  min  tensor(16.)  ent  tensor(0.4716)\n",
      "None  mean  tensor(39.7450)  max  tensor(48.)  median  tensor(41.)  min  tensor(15.)  ent  tensor(0.4682)\n",
      "None  mean  tensor(39.6683)  max  tensor(48.)  median  tensor(40.)  min  tensor(17.)  ent  tensor(0.5011)\n",
      "None  mean  tensor(39.3060)  max  tensor(48.)  median  tensor(40.)  min  tensor(15.)  ent  tensor(0.5252)\n",
      "None  mean  tensor(39.5463)  max  tensor(47.)  median  tensor(40.)  min  tensor(20.)  ent  tensor(0.5249)\n",
      "None  mean  tensor(39.6087)  max  tensor(46.)  median  tensor(40.)  min  tensor(21.)  ent  tensor(0.5061)\n",
      "None  mean  tensor(39.8677)  max  tensor(47.)  median  tensor(40.)  min  tensor(17.)  ent  tensor(0.4747)\n",
      "None  mean  tensor(39.9810)  max  tensor(47.)  median  tensor(40.)  min  tensor(17.)  ent  tensor(0.4626)\n",
      "None  mean  tensor(40.1133)  max  tensor(48.)  median  tensor(41.)  min  tensor(14.)  ent  tensor(0.4740)\n",
      "None  mean  tensor(39.9110)  max  tensor(48.)  median  tensor(40.)  min  tensor(18.)  ent  tensor(0.4985)\n",
      "None  mean  tensor(40.2223)  max  tensor(48.)  median  tensor(42.)  min  tensor(24.)  ent  tensor(0.5089)\n",
      "None  mean  tensor(40.4827)  max  tensor(47.)  median  tensor(42.)  min  tensor(23.)  ent  tensor(0.5054)\n",
      "None  mean  tensor(40.7717)  max  tensor(48.)  median  tensor(43.)  min  tensor(24.)  ent  tensor(0.4918)\n",
      "None  mean  tensor(40.7067)  max  tensor(46.)  median  tensor(42.)  min  tensor(20.)  ent  tensor(0.4798)\n",
      "None  mean  tensor(41.0900)  max  tensor(47.)  median  tensor(43.)  min  tensor(24.)  ent  tensor(0.4681)\n",
      "None  mean  tensor(40.7890)  max  tensor(48.)  median  tensor(42.)  min  tensor(24.)  ent  tensor(0.4797)\n",
      "None  mean  tensor(40.2260)  max  tensor(48.)  median  tensor(41.)  min  tensor(26.)  ent  tensor(0.4913)\n",
      "None  mean  tensor(40.5753)  max  tensor(47.)  median  tensor(42.)  min  tensor(24.)  ent  tensor(0.4914)\n",
      "None  mean  tensor(40.9533)  max  tensor(48.)  median  tensor(43.)  min  tensor(26.)  ent  tensor(0.4814)\n",
      "None  mean  tensor(41.0183)  max  tensor(46.)  median  tensor(43.)  min  tensor(24.)  ent  tensor(0.4758)\n",
      "None  mean  tensor(40.8977)  max  tensor(47.)  median  tensor(43.)  min  tensor(25.)  ent  tensor(0.4777)\n",
      "None  mean  tensor(41.0067)  max  tensor(46.)  median  tensor(43.)  min  tensor(23.)  ent  tensor(0.4801)\n",
      "None  mean  tensor(40.7603)  max  tensor(47.)  median  tensor(43.)  min  tensor(26.)  ent  tensor(0.4876)\n",
      "None  mean  tensor(40.7467)  max  tensor(47.)  median  tensor(42.)  min  tensor(21.)  ent  tensor(0.4899)\n",
      "None  mean  tensor(40.5707)  max  tensor(48.)  median  tensor(42.)  min  tensor(22.)  ent  tensor(0.4822)\n",
      "None  mean  tensor(40.7180)  max  tensor(46.)  median  tensor(42.)  min  tensor(23.)  ent  tensor(0.4699)\n",
      "None  mean  tensor(40.6427)  max  tensor(48.)  median  tensor(42.)  min  tensor(24.)  ent  tensor(0.4661)\n",
      "None  mean  tensor(40.2957)  max  tensor(45.)  median  tensor(41.)  min  tensor(17.)  ent  tensor(0.4715)\n",
      "None  mean  tensor(40.1553)  max  tensor(46.)  median  tensor(40.)  min  tensor(26.)  ent  tensor(0.4776)\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 50,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "# start a new wandb run to track this script\n",
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"code-\" + params[\"env\"][\"node_weight\"],\n",
    "    mode=params[\"visu\"][\"wb\"],\n",
    "    config=params\n",
    ")\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n",
    "\n",
    "# Agent's policy\n",
    "if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "    agent = agent_net(2, env.action_dim)\n",
    "else:\n",
    "    agent = agent_net(H-1, env.action_dim)\n",
    "optim = torch.optim.Adam(agent.parameters(), lr=params[\"alg\"][\"lr\"])\n",
    "\n",
    "for t_eps in range(epochs):\n",
    "    mat_action = []\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    marginal_return = []\n",
    "    mat_done = []\n",
    "    # print(t_eps)\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "\n",
    "    list_batch_state = []\n",
    "    for h_iter in range(H-1):\n",
    "        if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "            batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "            # append time index to the state\n",
    "            batch_state = torch.cat(\n",
    "                [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "        else:\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "        action_prob = agent(batch_state)\n",
    "\n",
    "        policy_dist = Categorical(action_prob)\n",
    "        actions = policy_dist.sample()\n",
    "        mat_action.append(actions)\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "        mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "        if h_iter ==0:\n",
    "            marginal_return.append(mat_return[h_iter])\n",
    "        else:\n",
    "            marginal_return.append(mat_return[h_iter] - mat_return[h_iter-1])\n",
    "        list_batch_state.append(batch_state)\n",
    "\n",
    "        #将数据转换放入回放池里\n",
    "        if h_iter < H-2:\n",
    "            next_state_batch = append_state(mat_state, H-1)\n",
    "            done = 0\n",
    "        else:\n",
    "            next_state_batch = batch_state\n",
    "            done = 1\n",
    "        batch_size = batch_state.shape[0]\n",
    "        for i in range(batch_size):\n",
    "            state = batch_state[i]\n",
    "            action = actions[i]\n",
    "            reward = marginal_return[-1][i]\n",
    "            next_state = next_state_batch[i]\n",
    "            replay_buffer.add(state, action, reward, next_state, done)\n",
    "\n",
    "    states_visited = torch.vstack(list_batch_state).float()\n",
    " \n",
    "    policy_dist = Categorical(agent(states_visited))\n",
    "    log_prob = policy_dist.log_prob(torch.hstack(mat_action))\n",
    "    batch_return = torch.hstack(marginal_return)/MAX_Ret\n",
    "\n",
    "    # - 2*policy_dist.entropy().mean()\n",
    "    J_obj = -1*(torch.mean(log_prob*batch_return) + params[\"alg\"][\"ent_coef\"] *\n",
    "                policy_dist.entropy().mean()/(t_eps+1))\n",
    "    optim.zero_grad()\n",
    "    J_obj.backward()\n",
    "    optim.step()\n",
    "\n",
    "    obj = env.weighted_traj_return(mat_state).float()\n",
    "    print(visu.JPi_optimal, \" mean \", obj.mean(), \" max \",\n",
    "          obj.max(), \" median \", obj.median(), \" min \", obj.min(), \" ent \", policy_dist.entropy().mean().detach())\n",
    "\n",
    "    wandb.log({\"opt\": MAX_Ret, \"mean\": obj.mean(),\n",
    "               \"max\": obj.max(), \"median\": obj.median(), \"min \": obj.min(), \" ent \": policy_dist.entropy().mean().detach()})\n",
    "\n",
    "    a = 1\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b068f5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.size()  # 查看回放池的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:29<00:00,  2.99s/it, epoch=10, return=26.834]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:29<00:00,  2.91s/it, epoch=20, return=28.000]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:29<00:00,  2.90s/it, epoch=30, return=28.000]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:29<00:00,  2.91s/it, epoch=40, return=28.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:28<00:00,  2.89s/it, epoch=50, return=28.000]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:29<00:00,  2.90s/it, epoch=60, return=28.000]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:28<00:00,  2.89s/it, epoch=70, return=28.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:29<00:00,  2.91s/it, epoch=80, return=28.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:28<00:00,  2.89s/it, epoch=90, return=28.000]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:29<00:00,  2.92s/it, epoch=100, return=28.000]\n"
     ]
    }
   ],
   "source": [
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 64\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "num_random = 5\n",
    "num_epochs = 100\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQLDiscrete(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                actions,_,_ = agent.actor(batch_state)\n",
    "                env.step(h_iter, actions)\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])\n",
    "            return_list.append(mat_return)\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38450667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e466386",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class PolicyNetContinuous(torch.nn.Module):\n",
    "#     def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "#         super(PolicyNetContinuous, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "#         self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
    "#         self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
    "#         self.action_bound = action_bound\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         mu = self.fc_mu(x)\n",
    "#         std = F.softplus(self.fc_std(x))\n",
    "#         dist = Normal(mu, std)\n",
    "#         normal_sample = dist.rsample()  # rsample()是重参数化采样\n",
    "#         log_prob = dist.log_prob(normal_sample)\n",
    "#         action = torch.tanh(normal_sample)\n",
    "#         # 计算tanh_normal分布的对数概率密度\n",
    "#         log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n",
    "#         action = action * self.action_bound\n",
    "#         return action, log_prob\n",
    "# class QValueNetContinuous(torch.nn.Module):\n",
    "#     def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "#         super(QValueNetContinuous, self).__init__()\n",
    "#         self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "#         self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "#     def forward(self, x, a):\n",
    "#         cat = torch.cat([x, a], dim=1)\n",
    "#         x = F.relu(self.fc1(cat))\n",
    "#         x = F.relu(self.fc2(x))\n",
    "#         return self.fc_out(x)\n",
    "# class CQL:\n",
    "#     ''' CQL算法 '''\n",
    "#     def __init__(self, state_dim, hidden_dim, action_dim, action_bound,\n",
    "#                  actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,\n",
    "#                  device, beta, num_random):\n",
    "#         self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim,\n",
    "#                                          action_bound).to(device)\n",
    "#         self.critic_1 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "#                                             action_dim).to(device)\n",
    "#         self.critic_2 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "#                                             action_dim).to(device)\n",
    "#         self.target_critic_1 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "#                                                    action_dim).to(device)\n",
    "#         self.target_critic_2 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "#                                                    action_dim).to(device)\n",
    "#         self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "#         self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "#         self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "#                                                 lr=actor_lr)\n",
    "#         self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "#                                                    lr=critic_lr)\n",
    "#         self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "#                                                    lr=critic_lr)\n",
    "#         self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "#         self.log_alpha.requires_grad = True  #对alpha求梯度\n",
    "#         self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "#                                                     lr=alpha_lr)\n",
    "#         self.target_entropy = target_entropy  # 目标熵的大小\n",
    "#         self.gamma = gamma\n",
    "#         self.tau = tau\n",
    "\n",
    "#         self.beta = beta  # CQL损失函数中的系数\n",
    "#         self.num_random = num_random  # CQL中的动作采样数\n",
    "\n",
    "#     def take_action(self, state):\n",
    "#         state = torch.tensor([state], dtype=torch.float).to(device)\n",
    "#         action = self.actor(state)[0]\n",
    "#         return [action.item()]\n",
    "\n",
    "#     def soft_update(self, net, target_net):\n",
    "#         for param_target, param in zip(target_net.parameters(),\n",
    "#                                        net.parameters()):\n",
    "#             param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "#                                     param.data * self.tau)\n",
    "\n",
    "#     def update(self, transition_dict):\n",
    "#         states = torch.tensor(transition_dict['states'],\n",
    "#                               dtype=torch.float).to(device)\n",
    "#         actions = torch.tensor(transition_dict['actions'],\n",
    "#                                dtype=torch.float).view(-1, 1).to(device)\n",
    "#         rewards = torch.tensor(transition_dict['rewards'],\n",
    "#                                dtype=torch.float).view(-1, 1).to(device)\n",
    "#         next_states = torch.tensor(transition_dict['next_states'],\n",
    "#                                    dtype=torch.float).to(device)\n",
    "#         dones = torch.tensor(transition_dict['dones'],\n",
    "#                              dtype=torch.float).view(-1, 1).to(device)\n",
    "#         rewards = (rewards + 8.0) / 8.0  # 对倒立摆环境的奖励进行重塑\n",
    "\n",
    "#         next_actions, log_prob = self.actor(next_states)\n",
    "#         entropy = -log_prob\n",
    "#         q1_value = self.target_critic_1(next_states, next_actions)\n",
    "#         q2_value = self.target_critic_2(next_states, next_actions)\n",
    "#         next_value = torch.min(q1_value,\n",
    "#                                q2_value) + self.log_alpha.exp() * entropy\n",
    "#         td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "#         critic_1_loss = torch.mean(\n",
    "#             F.mse_loss(self.critic_1(states, actions), td_target.detach()))\n",
    "#         critic_2_loss = torch.mean(\n",
    "#             F.mse_loss(self.critic_2(states, actions), td_target.detach()))\n",
    "\n",
    "#         # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "#         batch_size = states.shape[0]  # 获取批次大小\n",
    "#         random_unif_actions = torch.rand(\n",
    "#             [batch_size * self.num_random, actions.shape[-1]],\n",
    "#             dtype=torch.float).uniform_(-1, 1).to(device)   # 生成均匀分布的随机动作，-1，1之间\n",
    "#         random_unif_log_pi = np.log(0.5**next_actions.shape[-1])  #均匀概率密度\n",
    "#         tmp_states = states.unsqueeze(1).repeat(1, self.num_random,\n",
    "#                                                 1).view(-1, states.shape[-1])  # 扩展状态维度\n",
    "#         tmp_next_states = next_states.unsqueeze(1).repeat(\n",
    "#             1, self.num_random, 1).view(-1, next_states.shape[-1])   #扩展下一个状态维度\n",
    "#         random_curr_actions, random_curr_log_pi = self.actor(tmp_states)    # 当前随机动作和对数概率\n",
    "#         random_next_actions, random_next_log_pi = self.actor(tmp_next_states)   # 下一个随机动作和对数概率\n",
    "#         q1_unif = self.critic_1(tmp_states, random_unif_actions).view(\n",
    "#             -1, self.num_random, 1)     # 当前随机动作的Q值\n",
    "#         q2_unif = self.critic_2(tmp_states, random_unif_actions).view(\n",
    "#             -1, self.num_random, 1)\n",
    "#         q1_curr = self.critic_1(tmp_states, random_curr_actions).view(\n",
    "#             -1, self.num_random, 1)     # 当前动作的Q值\n",
    "#         q2_curr = self.critic_2(tmp_states, random_curr_actions).view(\n",
    "#             -1, self.num_random, 1)\n",
    "#         q1_next = self.critic_1(tmp_states, random_next_actions).view(\n",
    "#             -1, self.num_random, 1)     # 下一个动作的Q值（使用当前状态）\n",
    "#         q2_next = self.critic_2(tmp_states, random_next_actions).view(\n",
    "#             -1, self.num_random, 1)\n",
    "#         q1_cat = torch.cat([\n",
    "#             q1_unif - random_unif_log_pi,\n",
    "#             q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "#             q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "#         ],dim=1)        #三种Q值的拼接\n",
    "#         q2_cat = torch.cat([\n",
    "#             q2_unif - random_unif_log_pi,\n",
    "#             q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "#             q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "#         ],dim=1)\n",
    "\n",
    "#         qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()      # 对拼接的Q值进行logsumexp操作\n",
    "#         qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "#         qf1_loss_2 = self.critic_1(states, actions).mean()      # 计算当前动作的Q值\n",
    "#         qf2_loss_2 = self.critic_2(states, actions).mean()\n",
    "#         qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)        # CQL损失函数\n",
    "#         qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "#         self.critic_1_optimizer.zero_grad()\n",
    "#         qf1_loss.backward(retain_graph=True)        #保留图\n",
    "#         self.critic_1_optimizer.step()\n",
    "#         self.critic_2_optimizer.zero_grad()\n",
    "#         qf2_loss.backward(retain_graph=True)\n",
    "#         self.critic_2_optimizer.step()\n",
    "\n",
    "#         # 更新策略网络\n",
    "#         new_actions, log_prob = self.actor(states)\n",
    "#         entropy = -log_prob\n",
    "#         q1_value = self.critic_1(states, new_actions)\n",
    "#         q2_value = self.critic_2(states, new_actions)\n",
    "#         actor_loss = torch.mean(-self.log_alpha.exp() * entropy -\n",
    "#                                 torch.min(q1_value, q2_value))\n",
    "#         self.actor_optimizer.zero_grad()\n",
    "#         actor_loss.backward()\n",
    "#         self.actor_optimizer.step()\n",
    "\n",
    "#         # 更新alpha值\n",
    "#         alpha_loss = torch.mean(\n",
    "#             (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "#         self.log_alpha_optimizer.zero_grad()\n",
    "#         alpha_loss.backward()\n",
    "#         self.log_alpha_optimizer.step()\n",
    "\n",
    "#         self.soft_update(self.critic_1, self.target_critic_1)\n",
    "#         self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98386d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
