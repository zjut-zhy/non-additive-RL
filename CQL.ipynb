{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size, self.num_random)),\n",
    "                                              dtype=torch.long).to(self.device)\n",
    "        random_unif_log_pi =np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (4,),\n",
       " (tensor([2]),),\n",
       " array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         14., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:00<00:00,  6.02s/it, epoch=10, return=28.810]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:58<00:00,  5.90s/it, epoch=20, return=34.125]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:04<00:00,  6.43s/it, epoch=30, return=53.526]\n",
      "Iteration 3: 100%|██████████| 10/10 [01:02<00:00,  6.24s/it, epoch=40, return=57.837]\n",
      "Iteration 4: 100%|██████████| 10/10 [01:00<00:00,  6.06s/it, epoch=50, return=58.602]\n",
      "Iteration 5: 100%|██████████| 10/10 [01:01<00:00,  6.17s/it, epoch=60, return=58.506]\n",
      "Iteration 6: 100%|██████████| 10/10 [01:01<00:00,  6.10s/it, epoch=70, return=57.994]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=80, return=58.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [01:02<00:00,  6.21s/it, epoch=90, return=63.840]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=100, return=64.000]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "num_random = 5\n",
    "num_epochs = 100\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 13), (38, 13), (39, 13)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "798d452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_cql_model(agent, save_dir=\"./saved_models\", model_name=None, \n",
    "                   include_optimizers=True, additional_info=None):\n",
    "    \"\"\"\n",
    "    保存CQL模型的完整状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        save_dir: 保存目录\n",
    "        model_name: 模型名称，如果为None则使用时间戳\n",
    "        include_optimizers: 是否保存优化器状态\n",
    "        additional_info: 额外信息字典（如训练轮数、超参数等）\n",
    "    \n",
    "    Returns:\n",
    "        str: 保存的文件路径\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 生成模型名称\n",
    "    if model_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"cql_model_{timestamp}\"\n",
    "    \n",
    "    # 准备保存的状态字典\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_1_state_dict': agent.critic_1.state_dict(),\n",
    "        'critic_2_state_dict': agent.critic_2.state_dict(),\n",
    "        'target_critic_1_state_dict': agent.target_critic_1.state_dict(),\n",
    "        'target_critic_2_state_dict': agent.target_critic_2.state_dict(),\n",
    "        'log_alpha': agent.log_alpha,\n",
    "        \n",
    "        # 模型超参数\n",
    "        'hyperparameters': {\n",
    "            'gamma': agent.gamma,\n",
    "            'tau': agent.tau,\n",
    "            'target_entropy': agent.target_entropy,\n",
    "            'beta': agent.beta,\n",
    "            'num_random': agent.num_random,\n",
    "            'action_dim': agent.action_dim,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 可选：保存优化器状态\n",
    "    if include_optimizers:\n",
    "        checkpoint.update({\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_1_optimizer_state_dict': agent.critic_1_optimizer.state_dict(),\n",
    "            'critic_2_optimizer_state_dict': agent.critic_2_optimizer.state_dict(),\n",
    "            'log_alpha_optimizer_state_dict': agent.log_alpha_optimizer.state_dict(),\n",
    "        })\n",
    "    \n",
    "    # 添加额外信息\n",
    "    if additional_info:\n",
    "        checkpoint['additional_info'] = additional_info\n",
    "    \n",
    "    # 保存时间戳\n",
    "    checkpoint['save_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # 保存文件\n",
    "    file_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(checkpoint, file_path)\n",
    "    \n",
    "    print(f\"模型已保存到: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def load_cql_model(agent, file_path, load_optimizers=True):\n",
    "    \"\"\"\n",
    "    加载CQL模型状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        file_path: 模型文件路径\n",
    "        load_optimizers: 是否加载优化器状态\n",
    "    \n",
    "    Returns:\n",
    "        dict: 额外信息（如果有的话）\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {file_path}\")\n",
    "    \n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(file_path, map_location=agent.device)\n",
    "    \n",
    "    # 加载网络参数\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    agent.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])\n",
    "    agent.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])\n",
    "    agent.target_critic_1.load_state_dict(checkpoint['target_critic_1_state_dict'])\n",
    "    agent.target_critic_2.load_state_dict(checkpoint['target_critic_2_state_dict'])\n",
    "    agent.log_alpha = checkpoint['log_alpha']\n",
    "    \n",
    "    # 可选：加载优化器状态\n",
    "    if load_optimizers and 'actor_optimizer_state_dict' in checkpoint:\n",
    "        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        agent.critic_1_optimizer.load_state_dict(checkpoint['critic_1_optimizer_state_dict'])\n",
    "        agent.critic_2_optimizer.load_state_dict(checkpoint['critic_2_optimizer_state_dict'])\n",
    "        agent.log_alpha_optimizer.load_state_dict(checkpoint['log_alpha_optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"模型已从 {file_path} 加载\")\n",
    "    print(f\"保存时间: {checkpoint.get('save_timestamp', '未知')}\")\n",
    "    \n",
    "    # 返回超参数和额外信息\n",
    "    return {\n",
    "        'hyperparameters': checkpoint.get('hyperparameters', {}),\n",
    "        'additional_info': checkpoint.get('additional_info', {})\n",
    "    }\n",
    "\n",
    "# 使用示例：\n",
    "# 保存模型\n",
    "additional_info = {\n",
    "    'epoch': i_epoch,\n",
    "    'return_list': return_list,\n",
    "    'best_return': max(return_list) if return_list else 0,\n",
    "    'training_params': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_trains_per_epoch': num_trains_per_epoch,\n",
    "        'buffer_size': buffer_size\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存当前训练的模型\n",
    "# save_path = save_cql_model(\n",
    "#     agent, \n",
    "#     save_dir=\"./saved_models\",\n",
    "#     model_name=\"cql_model_64\",\n",
    "#     include_optimizers=True,\n",
    "#     additional_info=additional_info\n",
    "# )\n",
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a16c1b",
   "metadata": {},
   "source": [
    "下面开始进行微调数据，这里我尝试采用更优质的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "611dd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 10 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 10 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=10)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=10)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "1cc0ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 20 条轨迹到回放池（实时计算边际奖励）...\n",
      "总共添加了 777 个转移到回放池\n",
      "完成！回放池当前大小: 777\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6bad08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303999b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:01<00:00,  6.15s/it, epoch=10, return=64.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [01:01<00:00,  6.16s/it, epoch=20, return=64.000]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=30, return=64.000]\n",
      "Iteration 3: 100%|██████████| 10/10 [01:01<00:00,  6.16s/it, epoch=40, return=64.000]\n",
      "Iteration 4:  30%|███       | 3/10 [00:18<00:44,  6.30s/it]"
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ed209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetContinuous(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound):\n",
    "        super(PolicyNetContinuous, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
    "        self.action_bound = action_bound\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        mu = self.fc_mu(x)\n",
    "        std = F.softplus(self.fc_std(x))\n",
    "        dist = Normal(mu, std)\n",
    "        normal_sample = dist.rsample()  # rsample()是重参数化采样\n",
    "        log_prob = dist.log_prob(normal_sample)\n",
    "        action = torch.tanh(normal_sample)\n",
    "        # 计算tanh_normal分布的对数概率密度\n",
    "        log_prob = log_prob - torch.log(1 - torch.tanh(action).pow(2) + 1e-7)\n",
    "        action = action * self.action_bound\n",
    "        return action, log_prob\n",
    "class QValueNetContinuous(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNetContinuous, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_out = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, a):\n",
    "        cat = torch.cat([x, a], dim=1)\n",
    "        x = F.relu(self.fc1(cat))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc_out(x)\n",
    "class CQL:\n",
    "    ''' CQL算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, action_bound,\n",
    "                 actor_lr, critic_lr, alpha_lr, target_entropy, tau, gamma,\n",
    "                 device, beta, num_random):\n",
    "        self.actor = PolicyNetContinuous(state_dim, hidden_dim, action_dim,\n",
    "                                         action_bound).to(device)\n",
    "        self.critic_1 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "                                            action_dim).to(device)\n",
    "        self.critic_2 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "                                            action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "                                                   action_dim).to(device)\n",
    "        self.target_critic_2 = QValueNetContinuous(state_dim, hidden_dim,\n",
    "                                                   action_dim).to(device)\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  #对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(device)\n",
    "        action = self.actor(state)[0]\n",
    "        return [action.item()]\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(device)\n",
    "        actions = torch.tensor(transition_dict['actions'],\n",
    "                               dtype=torch.float).view(-1, 1).to(device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(device)\n",
    "        rewards = (rewards + 8.0) / 8.0  # 对倒立摆环境的奖励进行重塑\n",
    "\n",
    "        next_actions, log_prob = self.actor(next_states)\n",
    "        entropy = -log_prob\n",
    "        q1_value = self.target_critic_1(next_states, next_actions)\n",
    "        q2_value = self.target_critic_2(next_states, next_actions)\n",
    "        next_value = torch.min(q1_value,\n",
    "                               q2_value) + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(self.critic_1(states, actions), td_target.detach()))\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(self.critic_2(states, actions), td_target.detach()))\n",
    "\n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]  # 获取批次大小\n",
    "        random_unif_actions = torch.rand(\n",
    "            [batch_size * self.num_random, actions.shape[-1]],\n",
    "            dtype=torch.float).uniform_(-1, 1).to(device)   # 生成均匀分布的随机动作，-1，1之间\n",
    "        random_unif_log_pi = np.log(0.5**next_actions.shape[-1])  #均匀概率密度\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random,\n",
    "                                                1).view(-1, states.shape[-1])  # 扩展状态维度\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(\n",
    "            1, self.num_random, 1).view(-1, next_states.shape[-1])   #扩展下一个状态维度\n",
    "        random_curr_actions, random_curr_log_pi = self.actor(tmp_states)    # 当前随机动作和对数概率\n",
    "        random_next_actions, random_next_log_pi = self.actor(tmp_next_states)   # 下一个随机动作和对数概率\n",
    "        q1_unif = self.critic_1(tmp_states, random_unif_actions).view(\n",
    "            -1, self.num_random, 1)     # 当前随机动作的Q值\n",
    "        q2_unif = self.critic_2(tmp_states, random_unif_actions).view(\n",
    "            -1, self.num_random, 1)\n",
    "        q1_curr = self.critic_1(tmp_states, random_curr_actions).view(\n",
    "            -1, self.num_random, 1)     # 当前动作的Q值\n",
    "        q2_curr = self.critic_2(tmp_states, random_curr_actions).view(\n",
    "            -1, self.num_random, 1)\n",
    "        q1_next = self.critic_1(tmp_states, random_next_actions).view(\n",
    "            -1, self.num_random, 1)     # 下一个动作的Q值（使用当前状态）\n",
    "        q2_next = self.critic_2(tmp_states, random_next_actions).view(\n",
    "            -1, self.num_random, 1)\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)        #三种Q值的拼接\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()      # 对拼接的Q值进行logsumexp操作\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states, actions).mean()      # 计算当前动作的Q值\n",
    "        qf2_loss_2 = self.critic_2(states, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)        # CQL损失函数\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)        #保留图\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        new_actions, log_prob = self.actor(states)\n",
    "        entropy = -log_prob\n",
    "        q1_value = self.critic_1(states, new_actions)\n",
    "        q2_value = self.critic_2(states, new_actions)\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy -\n",
    "                                torch.min(q1_value, q2_value))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98386d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
