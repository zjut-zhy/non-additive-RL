{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size, self.num_random)),\n",
    "                                              dtype=torch.long).to(self.device)\n",
    "        random_unif_log_pi =np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (4,),\n",
       " (tensor([2]),),\n",
       " array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         14., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:00<00:00,  6.02s/it, epoch=10, return=28.810]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:58<00:00,  5.90s/it, epoch=20, return=34.125]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:04<00:00,  6.43s/it, epoch=30, return=53.526]\n",
      "Iteration 3: 100%|██████████| 10/10 [01:02<00:00,  6.24s/it, epoch=40, return=57.837]\n",
      "Iteration 4: 100%|██████████| 10/10 [01:00<00:00,  6.06s/it, epoch=50, return=58.602]\n",
      "Iteration 5: 100%|██████████| 10/10 [01:01<00:00,  6.17s/it, epoch=60, return=58.506]\n",
      "Iteration 6: 100%|██████████| 10/10 [01:01<00:00,  6.10s/it, epoch=70, return=57.994]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=80, return=58.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [01:02<00:00,  6.21s/it, epoch=90, return=63.840]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=100, return=64.000]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "num_random = 5\n",
    "num_epochs = 100\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 13), (38, 13), (39, 13)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "798d452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_cql_model(agent, save_dir=\"./saved_models\", model_name=None, \n",
    "                   include_optimizers=True, additional_info=None):\n",
    "    \"\"\"\n",
    "    保存CQL模型的完整状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        save_dir: 保存目录\n",
    "        model_name: 模型名称，如果为None则使用时间戳\n",
    "        include_optimizers: 是否保存优化器状态\n",
    "        additional_info: 额外信息字典（如训练轮数、超参数等）\n",
    "    \n",
    "    Returns:\n",
    "        str: 保存的文件路径\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 生成模型名称\n",
    "    if model_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"cql_model_{timestamp}\"\n",
    "    \n",
    "    # 准备保存的状态字典\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_1_state_dict': agent.critic_1.state_dict(),\n",
    "        'critic_2_state_dict': agent.critic_2.state_dict(),\n",
    "        'target_critic_1_state_dict': agent.target_critic_1.state_dict(),\n",
    "        'target_critic_2_state_dict': agent.target_critic_2.state_dict(),\n",
    "        'log_alpha': agent.log_alpha,\n",
    "        \n",
    "        # 模型超参数\n",
    "        'hyperparameters': {\n",
    "            'gamma': agent.gamma,\n",
    "            'tau': agent.tau,\n",
    "            'target_entropy': agent.target_entropy,\n",
    "            'beta': agent.beta,\n",
    "            'num_random': agent.num_random,\n",
    "            'action_dim': agent.action_dim,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 可选：保存优化器状态\n",
    "    if include_optimizers:\n",
    "        checkpoint.update({\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_1_optimizer_state_dict': agent.critic_1_optimizer.state_dict(),\n",
    "            'critic_2_optimizer_state_dict': agent.critic_2_optimizer.state_dict(),\n",
    "            'log_alpha_optimizer_state_dict': agent.log_alpha_optimizer.state_dict(),\n",
    "        })\n",
    "    \n",
    "    # 添加额外信息\n",
    "    if additional_info:\n",
    "        checkpoint['additional_info'] = additional_info\n",
    "    \n",
    "    # 保存时间戳\n",
    "    checkpoint['save_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # 保存文件\n",
    "    file_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(checkpoint, file_path)\n",
    "    \n",
    "    print(f\"模型已保存到: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def load_cql_model(agent, file_path, load_optimizers=True):\n",
    "    \"\"\"\n",
    "    加载CQL模型状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        file_path: 模型文件路径\n",
    "        load_optimizers: 是否加载优化器状态\n",
    "    \n",
    "    Returns:\n",
    "        dict: 额外信息（如果有的话）\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {file_path}\")\n",
    "    \n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(file_path, map_location=agent.device)\n",
    "    \n",
    "    # 加载网络参数\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    agent.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])\n",
    "    agent.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])\n",
    "    agent.target_critic_1.load_state_dict(checkpoint['target_critic_1_state_dict'])\n",
    "    agent.target_critic_2.load_state_dict(checkpoint['target_critic_2_state_dict'])\n",
    "    agent.log_alpha = checkpoint['log_alpha']\n",
    "    \n",
    "    # 可选：加载优化器状态\n",
    "    if load_optimizers and 'actor_optimizer_state_dict' in checkpoint:\n",
    "        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        agent.critic_1_optimizer.load_state_dict(checkpoint['critic_1_optimizer_state_dict'])\n",
    "        agent.critic_2_optimizer.load_state_dict(checkpoint['critic_2_optimizer_state_dict'])\n",
    "        agent.log_alpha_optimizer.load_state_dict(checkpoint['log_alpha_optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"模型已从 {file_path} 加载\")\n",
    "    print(f\"保存时间: {checkpoint.get('save_timestamp', '未知')}\")\n",
    "    \n",
    "    # 返回超参数和额外信息\n",
    "    return {\n",
    "        'hyperparameters': checkpoint.get('hyperparameters', {}),\n",
    "        'additional_info': checkpoint.get('additional_info', {})\n",
    "    }\n",
    "\n",
    "# 使用示例：\n",
    "# 保存模型\n",
    "additional_info = {\n",
    "    'epoch': i_epoch,\n",
    "    'return_list': return_list,\n",
    "    'best_return': max(return_list) if return_list else 0,\n",
    "    'training_params': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_trains_per_epoch': num_trains_per_epoch,\n",
    "        'buffer_size': buffer_size\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存当前训练的模型\n",
    "# save_path = save_cql_model(\n",
    "#     agent, \n",
    "#     save_dir=\"./saved_models\",\n",
    "#     model_name=\"cql_model_64\",\n",
    "#     include_optimizers=True,\n",
    "#     additional_info=additional_info\n",
    "# )\n",
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a16c1b",
   "metadata": {},
   "source": [
    "下面开始进行微调数据，这里我尝试采用更优质的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "611dd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 67\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=20)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=20)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "1cc0ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 40 条轨迹到回放池（实时计算边际奖励）...\n",
      "总共添加了 1552 个转移到回放池\n",
      "完成！回放池当前大小: 1552\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "ca520b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均奖励: 64.0\n",
      "完成！回放池当前大小: 2371\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10      #采样的batch大小\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "\n",
    "    env.step(h_iter, actions.cpu())\n",
    "\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    if h_iter == 0:\n",
    "        reward = mat_return[-1]\n",
    "    else:\n",
    "        reward = mat_return[-1]-mat_return[-2]\n",
    "\n",
    "    if h_iter == H-2:\n",
    "        next_state = batch_state\n",
    "        done = 1\n",
    "    else:\n",
    "        next_state = append_state(mat_state, H-1)\n",
    "        done = 0\n",
    "\n",
    "    for j in range(params[\"common\"][\"batch_size\"]):\n",
    "        replay_buffer.add(batch_state[j],actions[j],reward[j],next_state[j],done)\n",
    "print(f\"平均奖励: {mat_return[-1].float().mean()}\")\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "6bad08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 640\n",
    "# new_state=expand_trajectory_states(mat_state, H)\n",
    "# new_state[-1],new_state[-2],new_state[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "303999b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:00<00:00,  6.01s/it, epoch=10, return=58.890]\n",
      "Iteration 1:  60%|██████    | 6/10 [00:42<00:28,  7.03s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[384]\u001b[39m\u001b[32m, line 40\u001b[39m\n\u001b[32m     32\u001b[39m     b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n\u001b[32m     33\u001b[39m     transition_dict = {\n\u001b[32m     34\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m'\u001b[39m: b_s,\n\u001b[32m     35\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m: b_a,\n\u001b[32m   (...)\u001b[39m\u001b[32m     38\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdones\u001b[39m\u001b[33m'\u001b[39m: b_d\n\u001b[32m     39\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i_epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     43\u001b[39m     pbar.set_postfix({\n\u001b[32m     44\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     45\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m % (num_epochs / \u001b[32m10\u001b[39m * i + i_epoch + \u001b[32m1\u001b[39m),\n\u001b[32m     46\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     47\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33m'\u001b[39m % np.mean(return_list[-\u001b[32m10\u001b[39m:])\n\u001b[32m     48\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 162\u001b[39m, in \u001b[36mCQL.update\u001b[39m\u001b[34m(self, transition_dict)\u001b[39m\n\u001b[32m    160\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_1_optimizer.step()\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_2_optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mqf2_loss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_2_optimizer.step()\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# 更新策略网络\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f21ed209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "e98386d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 24), (28, 38), (29, 52), (30, 53), (31, 54), (32, 40), (33, 26), (34, 12), (35, 12), (36, 26), (37, 27), (38, 27), (39, 27)]\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 3, 3, 2, 1, 0, 0, 1, 1, 1, 1]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)\n",
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6bd4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed8ad36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
