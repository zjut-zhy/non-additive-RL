{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size, self.num_random)),\n",
    "                                              dtype=torch.long).to(self.device)\n",
    "        random_unif_log_pi =np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (4,),\n",
       " (tensor([2]),),\n",
       " array([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28.,\n",
       "         14., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:00<00:00,  6.02s/it, epoch=10, return=28.810]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:58<00:00,  5.90s/it, epoch=20, return=34.125]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:04<00:00,  6.43s/it, epoch=30, return=53.526]\n",
      "Iteration 3: 100%|██████████| 10/10 [01:02<00:00,  6.24s/it, epoch=40, return=57.837]\n",
      "Iteration 4: 100%|██████████| 10/10 [01:00<00:00,  6.06s/it, epoch=50, return=58.602]\n",
      "Iteration 5: 100%|██████████| 10/10 [01:01<00:00,  6.17s/it, epoch=60, return=58.506]\n",
      "Iteration 6: 100%|██████████| 10/10 [01:01<00:00,  6.10s/it, epoch=70, return=57.994]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=80, return=58.000]\n",
      "Iteration 8: 100%|██████████| 10/10 [01:02<00:00,  6.21s/it, epoch=90, return=63.840]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:01<00:00,  6.12s/it, epoch=100, return=64.000]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "num_random = 5\n",
    "num_epochs = 100\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64])"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 13), (38, 13), (39, 13)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABGMAAAJdCAYAAACWDbrjAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjMsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvZiW1igAAAAlwSFlzAAAPYQAAD2EBqD+naQAARcFJREFUeJzt3Qu8VXWdN/7v4XDVAEExboIX0NTUTKQ/1ZSWyOOk5UyF01CZOmmK5eRTU2aj+aSZ0+TTNMrNMZ2L16lpdOZ5FMs0y0cHtWzyEoKaAULeEFIJEPb/9Vt4CBCUteD89jr7vN+v12afs89l/87nrH1Y+7N/67faGo1GIwAAAADIokeeuwEAAAAgUcYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAJXdfvvt0dbWVlwDALB1lDEAUBNXXnllUWzce++962/7v//3/8ZXvvKVaLZp06YV46ubhx9+OP7H//gf8YY3vCEGDx4cH/vYx+Lpp5/Oct8PPfRQ8bv59a9/Hc02ffr0+PCHPxyjRo0qtqFPfOITm/28xYsXxxe/+MU4/PDDo3///q9ZpN1yyy1x0kknxZvf/OZob2+P3XffvZN/CgDoPpQxAFBjqYw577zzalvGvOtd74oVK1YU17ktXLiwuN/58+fH1772tfjc5z4X/+f//J+YOHFirFq1KksZk343dShjLrroovjRj34U+++/f/Ts2XOLnzd37tzicxctWhQHHHDAa37Pq6++urgMHDgwhg8f3gmjBoDua8v/WwMALanRaMTvf//76Nev3zZ/rx49ekTfvn2jGVIB8+KLL8Z9991XzAhJxo8fX5QxqTg6+eSTS32/l19+OdauXRu9e/eOZko/04477ljqa3784x+vnxWTZgltySGHHBLPPvtsMYvou9/9bjGb5rXyveyyy6JXr15x9NFHxwMPPFBqTADAlpkZAwA1lQ41ufTSS4u305PsjkuHVBx861vfKmZDpELkjW98Y5xyyimxdOnSjb5POrwkPZmePXt2jBs3rihhZs6cWXzsiiuuiPe85z2x6667Rp8+fWK//fYrDnnZ9OsffPDB4gl/xxgOO+yw11wz5l//9V+LJ/7pvnbZZZf46Ec/WszG2PTnS8VBuv3YY48t3h4yZEgxw2XNmjWvm8/3vve94ufqKGKSI444Ivbee++4/vrrX/Nr02yWNO6//du/LTLca6+9ip8/zXZJfvWrX8WHPvShorRI2abcbrzxxvVfn8qejiIjHfLTkUtHDuntzR1elrLc8BCijkPTUrannXZa8XsYOXJk8bGUcTpEKI0p3ccOO+wQI0aMiL/5m7951fcdPXr0RtvGlqRDk9LPtDXSbJhUxAAA25+ZMQBQU6lYefLJJ+MHP/hB/PM///NmP56ezJ9wwgnxmc98Jh5//PG45JJL4uc//3nceeedGz2RToenfOQjHym+5pOf/GTss88+xe2peEllzvvf//7i8Jb/+I//KEqBVPRMnTq1+JxUVnz6058uypKzzz67uC0VP1vSMaZDDz00Lrzwwvjtb38bf/d3f1eMKY1tp512Wv+5qXSZNGlSvO1tbyuKkR/+8IfxzW9+syhHTj311C3eRypwnnrqqaIk2VSaHZMO79oaqYxKs4TSLJpUxqSiIhVP73jHO4riI62vkmappHInFUapAPqTP/mT4vColPm3v/3t+NKXvhT77rtv8f06rstKmaci6pxzzilmxnRIxVpaE+dP//RPY/LkycVsli984QvFIUZHHXVUpfsCAJpPGQMANTVhwoRilkcqY9LMkg399Kc/jX/4h3+Iq666Kv78z/98/e1pBkV68p5mpmx4e1pX5eabby6Kjw2lGRkbHq50+umnF19/8cUXry9jUgnx5S9/ef0Ml9eyevXqoixIMzruuOOO9YcwvfOd7yxmsfzv//2/N1oDJxUhxx13XPz1X/918f6nPvWpeOtb3xqXX375a5YxaSHaZNiwYa/6WLrtueeei5UrVxYFy+utO5OySUXIhrNr0mybe+65Z/3Xp7Ik/QzpZ0tlzJ577hl/9Ed/VJQx6bCojplCVaUS6NZbby0Wyt1QKuP+6Z/+qViYOEkL6qZZMCkfZQwAdF0OUwKALiiVLWlh1VQEPPPMM+sv6dCgNIPltttu2+jz99hjj1cVMcmGRcyyZcuK7/Hud787HnvsseL9stKZoNKMlVRebLiWzPve975405veVCywu6lUwGwolRzp/l9LWjQ42VzZ0nG/HZ/zWj74wQ9uVMSkEicthJtmofzud79bn2taZyXlN2/evFcdbrU9pNlKmxYxSfpdbliApfVs0syf18sHAKg3M2MAoAtKpUAqS9IaI5uTCpFNy5jNSYcOnXvuuXHXXXfFSy+9tNHH0vdPhU8ZTzzxRHHdcRjUhlIZk2b0bFqcbFiGJIMGDXrVujdbKpHS7JdNpdk2G37Oa9k0lzRLJi1wnGbqdMzW2Vy26RCm7WlLv5+0fsyma8GkfP77v/97u94/AJCXMgYAuqC0pksqYtJhSpuzacGxuWLi0Ucfjfe+971FSZIOS9ptt92KmRdpvZV0OFG6j862udkgW6Pj8KSOw5U2lG5Lh/283iFKm8ul42dOiwhvbiZRMmbMmKhqSwsTb6k42lI+qTACALouZQwA1NiWzpCTFrhNi92mhWarnqI6LdabZpakswRteEaiTQ9xeq1xbCqtZ9KxYHA6S9OG0m0dH99WaWZKKpzSYVGbmjNnTrzlLW+p9H3TWjBJWvw4rR3zWl4rkzR75fnnn9/otlWrVm22PAIAuh9rxgBAjaUz+SSbPrFPa5qkWRZf/epXX/U1L7/88qs+/7VmXWw4yyIdmpTOMLS5cWzN90xnN0ozdmbMmLHRIUQ33XRTPPzww8XaMdtLWu/lP//zP2PBggXrb0uL4D7yyCPrTztdVhp7Wow3nfp7c8XJ008//bq/m46yLC1gvKFZs2Zt1Sm7AYDWZ2YMANRYWpA3SadRTofNpALlz/7sz4pFdtNpqtOpo++///448sgji9kcaS2ZtLhvOpX0hz70odf83ulr0mFJxxxzTPG9XnjhhbjsssuKQmLTIiKNI50G+/zzzy8O00mfs+nMlySN4aKLLipObZ3GmE6n3XFq69133z0++9nPbrds0iml08+aziB1xhlnFOP/xje+UZz2Od1/VZdeemlx5qT0fdLCumm2TPoZ0ro66exLv/jFL4rPS7Nv0u8j/bypxEqHRaVMUjZ/8Rd/USxMnAqjtMhy+prZs2cXZ6TqDGmWU8e40hmt0poy6XeVpNOWH3jgges/t+P2dArvJJ02vWMtn3TWrA7pe6RZUx1r6aSfseNrDzrooGK7AQAqagAAtXDFFVekKSqNe+65Z/1tL7/8cuPTn/50Y8iQIY22trbi4xuaNWtW45BDDmn069ev0b9//8YBBxzQ+Ku/+qvGk08+uf5zRo8e3Xjf+9632fu88cYbGwceeGCjb9++jd13371x0UUXNb7zne8U9/P444+v/7wlS5YU3yPdR/rYu9/97uL22267rXg/XW/ouuuuaxx88MGNPn36NAYPHtyYMmVKY+HChRt9zvHHH9/YcccdXzWmc88991U/55Y88MADjSOPPLKxww47NHbaaafiftJYX0/62dJ9fOMb39jsxx999NHGxz/+8cbQoUMbvXr1aowYMaJx9NFHN7773e9u9HmXXXZZY88992y0t7dvlMOaNWsaX/jCFxq77LJLMbZJkyY15s+fX/wu0s/9Wr/zDinj/fff/1W3p69P32fT29L32dwl3ceGtvR5m2beMbbNXTb8GQCA8trSP1WLHAAAAADKsWYMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyKhnZLZ27dp48skno3///tHW1pb77gEAAAA6RaPRiN/97ncxfPjw6NGjR/PLmEsvvbS4rFq1Kh599NFcdwsAAACQ1YIFC2LkyJFb/HhbI9U2GS1btix22mmnmDNnTgwbNiznXXdZixcvjvHjx8usJLmVJ7Nq5FaezKqRW3kyq0Zu5cmsGrmVJ7Nq5FaezLYtt+effz4GDhxYn8OUOg5NSr/M12qJeDWZVSO38mRWjdzKk1k1citPZtXIrTyZVSO38mRWjdzKk1k1r7csiwV8AQAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBaEELFiyIE088MYYPHx69e/eO0aNHxxlnnBHPPvtss4dWWzKrRm7lyawauZUnM4D6UsYAtJjHHnssxo0bF/PmzYtrrrkm5s+fHzNmzIhbb701JkyYEM8991yzh1g7MqtGbuXJrBq5lSczgHrr2ewBALB9TZ06tXgF9JZbbol+/foVt40aNSoOPvjg2GuvveLss8+O6dOnN3uYtSKzauRWnsyqkVt5MgOoNzNjAFpIeqVz9uzZcdppp63f+e4wdOjQmDJlSlx33XXRaDSaNsa6kVk1citPZtXIrTyZAdSfMgaghaTp6Gnnet99993sx9PtS5cujaeffjr72OpKZtXIrTyZVSO38mQG0IJlzKJFi+KjH/1o7LzzzkXTfsABB8S9997bOaMDoBKvdpYns2rkVp7MqpFbeTIDaJEyJjXo73jHO6JXr15x0003xUMPPRTf/OY3Y9CgQZ03QgC22pgxY6KtrS0efvjhzX483Z7+Zg8ZMiT72OpKZtXIrTyZVSO38mQG0GJlzEUXXRS77bZbXHHFFTF+/PjYY4894sgjjywWAQOg+dKsxYkTJ8a0adNixYoVG31syZIlcdVVV8Vxxx1X7KSzjsyqkVt5MqtGbuXJDKDFypgbb7yxOEXehz/84dh1112L1dgvu+yyzhsdAKVdcsklsXLlypg0aVLccccdsWDBgrj55puLHfMRI0bEBRdc0Owh1o7MqpFbeTKrRm7lyQyghcqYxx57rDgF3tixY4sV2k899dT4zGc+E//4j/+4xa9J/wksX758owsAnSf9jU5ree25554xefLkYvbiySefHIcffnjcddddMXjw4GYPsXZkVo3cypNZNXIrT2YA9dazzCevXbu2mBnzta99rXg/zYx54IEHYsaMGXH88cdv9msuvPDCOO+887bPaAHYKqNHj44rr7yy2cPoUmRWjdzKk1k1citPZgAtMjNm2LBhsd9++73q1Hi/+c1vtvg1Z511Vixbtmz9JU2RBAAAAOiuSs2MSWdSmjt37ka3PfLII0XrviV9+vQpLgAAAACUnBnz2c9+Nu6+++7iMKX58+fH1VdfHbNmzYqpU6d23ggBAAAAumsZc+ihh8b3v//9uOaaa+LNb35zfPWrX41vfetbMWXKlM4bIQAAAEB3PUwpOfroo4sLAAAAAJ08MwYAAACAbaOMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABn1jCZZvHhxs+66y+nISmblyK08mVUjt/JkVo3cypNZNXIrT2bVyK08mVUjt/JkVs3W5tXWaDQakcHkyZPjhhtuiHR3q1evznGXAAAAANktW7YsBgwY0PwypsPy5ctj4MCBMWfOnBg2bFjOu+7Szdr48eNlVpLcypNZNXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bbltvrlTFNO0wp/TJHjhzZrLvvkmRWjdzKk1k1citPZtXIrTyZVSO38mRWjdzKk1k1citPZp3DAr4AAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgC0oAULFsSJJ54Yw4cPj969e8fo0aPjjDPOiGeffbbZQ6stmVUjt/JkVo3cypMZQH0pYwBazGOPPRbjxo2LefPmxTXXXBPz58+PGTNmxK233hoTJkyI5557rtlDrB2ZVSO38mRWjdzKkxlAvfVs9gAA2L6mTp1avAJ6yy23RL9+/YrbRo0aFQcffHDstddecfbZZ8f06dObPcxakVk1citPZtXIrTyZAdSbmTEALSS90jl79uw47bTT1u98dxg6dGhMmTIlrrvuumg0Gk0bY93IrBq5lSezauRWnswA6k8ZA9BC0nT0tHO97777bvbj6falS5fG008/nX1sdSWzauRWnsyqkVt5MgNosTLmK1/5SrS1tW10edOb3tR5owOgEq92liezauRWnsyqkVt5MgNooZkx+++/fyxevHj95ac//WnnjAyA0saMGVMU5Q8//PBmP55uHzRoUAwZMiT72OpKZtXIrTyZVSO38mQG0IJlTM+ePYtjTTsuu+yyS3R5CxdG3Hbbumten7zKkRcZ7bzzzjFx4sSYNm1arFixYqOPLVmyJK666qo47rjjip101pFZNXIrT2bVyK08mXUS+3RbR06bJxe29WxK6RjU4cOHR9++fYvT4l144YXFyuxd1uWXR5x8csTatdHo0SNWTZsea044Mepkxeo10darT3H90qqXmzqW9iu+E71POzXaapxXnXLbUl79erXbAaLTXHLJJfH2t789Jk2aFOeff37sscce8eCDD8bnP//5GDFiRFxwwQXNHmLtyKwauZUns2rkVp7Mtv0Qr7QPWcd94Drs425JnXKqU26vl4vnBt1TW6PEwaQ33XRTvPDCC7HPPvsUhyidd955sWjRonjggQeif//+m/2alStXFpcOy5cvj9122y0WLFgQI0eOjKZKreTo0UUR0+Hlth7xzk99J5YMaIEZP9vZ0OXPxJ0zToj2DTYZeVXLa+QBY+NfPzWhdn90Fy5cWJ/HZxdSx9yeeOKJOPfcc+Pmm28uzqqRZjIee+yxxW3pFdNmk1k1citPZtXIrTyZtU5u6enRh2bcFfc9sbR43z7w1pFT9VzGjR7kuUEL6cht2bJlMWDAgO0zM+aoo45a//aBBx4Yb3vb22L06NFx/fXXx0knnbTZr0kzZ1JpU0vz5m1UxCQ9G2tj9+ef7NZ/MLZkj6VPbvRHJJFXtbzufmKXopXfoXfpyWmwVdLf5iuvvLLZw+hSZFaN3MqTWTVyK09m1aR9tI4iJrEPvHXkVD2Xe59Y6rlBN7RNv+2ddtop9t5775g/f/4WP+ess86KM88881UzY2ph7NiIHj02KmQa7e1xxfkfiUaNmr80+yjNRpo7d24xrbRZ2hbuH43rv1xMr6tzXnXJbUt5/Xqn4dnHAgBAefd++YjY8al67QM3ex+3qz5XaFZur5XLi7sOi3Hn/zDbWGihMiYdsvToo4/Gxz72sS1+Tp8+fYpLLaU/CrNmReOUU6JtzZpiutiaS6dFvz13jzpJxxA2Vq8srpvalqZcZs2KOOWUiDVrItrbo23mzNrlVZvcNpPXqkunxZLHu+8rAwAAXckOvdvX7evWaB+46fu4W1KznGqT22vk0qjZmj/kVWor/NznPhfHHHNMMeXxySefLI43bW9vj4985CPRZZ10Uvz+8PfGCV++ppix8KMTpjR7RPWWDkebNCkizYYaM2ZdocVW57Vm16ER58xu9qgAACjDPvDWkdPmyYVtLWPSQjSpeHn22WdjyJAh8c53vjPuvvvu4u2uLE2bu3vUgc0eRteR/nj4A1ItL+03AEDXZB9468hp8+TCtpQx1157bZlPBwAAAGATPTa9AQAAAIDOo4wBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGfWMJlm8eHHUxYrVa9a/vWjRoujXqz3qpCOrOmXWFdQxN9taa5JbeTKrRm7lyawauZUns9bJzf5aa6pjbra11rS1ebU1Go1Gp48mIiZPnhw33HBDpLtbvXp11Elbrz4x6szvFW//5uIPRmP1ymYPiRZlWwMAqDf7a+RiW2tty5YtiwEDBjR/Zsz1119fXC9fvjwGDhwYc+bMiWHDhkVdGsmJMx8o3p47d24tG8nx48fXKrOuoI652dZak9zKk1k1citPZtXIrTyZtU5u9tdaUx1zs621po7canuYUvpljhw5MurgpVUvR8S6B8GIESNih95Ni6XLZNaV1Ck321prk1t5MqtGbuXJrBq5lSezrp+b/bXWVqfcbGvdmwV8AQAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBaEELFiyIE088MYYPHx69e/eO0aNHxxlnnBHPPvtss4dWWzKrRm7lyawauZUnM4D6UsYAtJjHHnssxo0bF/PmzYtrrrkm5s+fHzNmzIhbb701JkyYEM8991yzh1g7MqtGbuXJrBq5lSczgHrr2ewBALB9TZ06tXgF9JZbbol+/foVt40aNSoOPvjg2GuvveLss8+O6dOnN3uYtSKzauRWnsyqkVt5MgOoNzNjAFpIeqVz9uzZcdppp63f+e4wdOjQmDJlSlx33XXRaDSaNsa6kVk1citPZtXIrTyZAdSfMgaghaTp6Gnnet99993sx9PtS5cujaeffjr72OpKZtXIrTyZVSO38mQG0OJlzNe//vVoa2uLv/zLv9x+IwJgm3m1szyZVSO38mRWjdzKkxlAC5Yx99xzT8ycOTMOPPDA7TsiACobM2ZMUZI//PDDm/14un3QoEExZMiQ7GOrK5lVI7fyZFaN3MqTGUCLljEvvPBCcazpZZddVvwh7zIWLoy47bZ119CZbGs0yc477xwTJ06MadOmxYoVKzb62JIlS+Kqq66K4447rthJZx2ZVSO38mRWjdzKk9k2WLgwetx+ewxd/kyzR0Kr8fyA7VHGpNXZ3/e+98URRxwRXcbll0eMHh3xnvesu07vQ2ewrdFkl1xySaxcuTImTZoUd9xxRyxYsCBuvvnmYsd8xIgRccEFFzR7iLUjs2rkVp7MqpFbeTKrvg/Xd9LEuHPGCTH5F7c0e0S0Cs8P2B5lzLXXXhs/+9nP4sILL9yqz0//CSxfvnyjS3apfTz55Ii1a9e9n65POUUryfZnW6MGxo4dG/fee2/sueeeMXny5OIUpieffHIcfvjhcdddd8XgwYObPcTakVk1citPZtXIrTyZbds+XHujEV+bfUm02YdjW3l+wBb0jBJSo37GGWfED37wg+jbt+9WfU0qbc4777xoqnnz/rDxd1izJmL+/IiRI5s1KlqRbY2aGD16dFx55ZXNHkaXIrNq5FaezKqRW3ky27Z9uJ6NtfHyo49G7Ll704ZFiz8/2HVos0ZFV5sZc99998VTTz0Vb33rW6Nnz57F5cc//nF8+9vfLt5ekzaqTZx11lmxbNmy9ZdU6GQ3dmxEj01+1Pb2tLpZ/rHQ2mxrAAAtsQ/3cluPaOy1V9OGRIvw/IDtUca8973vjV/+8pdx//33r7+MGzeuWMw3vd2eNqpN9OnTJwYMGLDRJbs0I2HWrHUbfZKuZ840U4Htz7YGANDl9+FSEfOlSadHwz4c28rzA7bHYUr9+/ePN7/5zRvdtuOOOxYrtm96e+2cdFLEpEnrpoOlFtLGT2exrQEAdNl9uN8/PDcO+/7CWDJgl/hKs8dEa/D8gG0tY7q8tNHb8MnBtgYA0PWMHBlrdx0aS26d3eyR0Go8P2B7lzG33377tn4LAAAAgG6j9KmtAQAAAKhOGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAy6hlNsnjx4qiLFavXrH970aJF0a9Xe9RJR1Z1yqwrqGNutrXWJLfyZFaN3MqTWTVyK09mrZOb/bXWVMfcbGutaWvzams0Go1OH01ETJ48OW644YZId7d69eqok7ZefWLUmd8r3v7NxR+MxuqVzR4SLcq2BgBQb/bXyMW21tqWLVsWAwYMaP7MmOuvv764Xr58eQwcODDmzJkTw4YNi7o0khNnPlC8PXfu3Fo2kuPHj69VZl1BHXOzrbUmuZUns2rkVp7MqpFbeTJrndzsr7WmOuZmW2tNHbnV9jCl9MscOXJk1MFLq16OiHUPghEjRsQOvZsWS5fJrCupU262tdYmt/JkVo3cypNZNXIrT2ZdPzf7a62tTrnZ1ro3C/gCAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAAHUtY6ZPnx4HHnhgDBgwoLhMmDAhbrrpps4bHQAAAEB3LmNGjhwZX//61+O+++6Le++9N97znvfEBz7wgXjwwQc7b4QAAAAALaRUGXPMMcfEH//xH8fYsWNj7733jgsuuCDe8IY3xN133x0tZeHCiNtuW3cNnc32BgBQf/bZ2Ba2H7bXmjFr1qyJa6+9Nl588cXicKVW0X7FdyJGj454z3vWXV9+ebOHRCtL25ftDQCg3uyzsS1sP2yPMuaXv/xlMRumT58+8alPfSq+//3vx3777bfFz1+5cmUsX758o0tdDV3+TPQ+7dSItWvX3ZCuTzlFe0mnaEvb1ckn294AAOrMPhvbwvbD9ipj9tlnn7j//vvjv/7rv+LUU0+N448/Ph566KEtfv6FF14YAwcOXH/Zbbfdoq72WPpktHU8SDqsWRMxf36zhkQLa0vble0NAKDe5s2zz0Z1th+2VxnTu3fvGDNmTBxyyCFF0XLQQQfF3/3d323x888666xYtmzZ+suCBQuirh4fNDwaPTaJpL09YsyYZg2JFtZI25XtDQCg3saOtc9GdbYftveaMR3Wrl1bHIq0Jelwpo5TYXdc6mrJgF1i1bTp6x4cSbqeOTOdRqrZQ6MFNdJ2NWuW7Q0AoM7ss7EtbD9sQc8oIc1yOeqoo2LUqFHxu9/9Lq6++uq4/fbbY/bs2dEq1pxwYsT7/njdtLHUVnqQ0JlOOili0iTbGwBAndlnY1vYftjWMuapp56Kj3/847F48eJi/ZcDDzywKGImTpwYLSU9ODxAyMX2BgBQf/bZ2Ba2H7aljLncKbgAAAAAmrtmDAAAAABbTxkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMuoZTbJ48eKoixWr16x/e9GiRdGvV3vUSUdWdcqsK6hjbra11iS38mRWjdzKk1k1citPZq2Tm/211lTH3GxrrWlr82prNBqNTh9NREyePDluuOGGSHe3evXqqJO2Xn1i1JnfK97+zcUfjMbqlc0eEi3KtgYAUG/218jFttbali1bFgMGDGj+zJjrr7++uF6+fHkMHDgw5syZE8OGDYu6NJITZz5QvD137txaNpLjx4+vVWZdQR1zs621JrmVJ7Nq5FaezKqRW3kya53c7K+1pjrmZltrTR251fYwpfTLHDlyZNTBS6tejoh1D4IRI0bEDr2bFkuXyawrqVNutrXWJrfyZFaN3MqTWTVyK09mXT83+2utrU652da6Nwv4AgAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAB1LWMuvPDCOPTQQ6N///6x6667xrHHHhtz587tvNEBAAAAdOcy5sc//nFMnTo17r777vjBD34Qq1evjiOPPDJefPHFzhshAFBvi34WceXR664BAHhdPaOEm2++eaP3r7zyymKGzH333Rfvete7ouUsXBgxb17E2LERI0c2ezS0wLbUNnqPZo+k9R5zHqdbl9H/+3/r3n772+WU3HNPxE9+EvFHfxRx6KHNHk3XtOFj77+vjfj1TyL++7qIEW997c+1/W2d7pxZd/7Zc5P1xuyvURcem91CqTJmU8uWLSuuBw8eHC3n8ssjTj45Yu3aiB49ImbNijjppGaPii6+LfXt0SMmH3l6XH/QkfHSqjVRNytWr4m2Xn2K65dWvZz1vtuv+E70Pu3UaFu7Nho9esSqadNjzQknrv94o7FufBvq849XxBs+M3X917zw7Utj5fEnRG5LX1odPXYYWFz3fWFl1EmR0adPi7YUYMqxrS1e+PtpTcmpLpm94ZS/iD5X/0u0pTwiYuWffzRemPkP0RXUZVsrtqu/Pj3a+jai0dYjGicPKabarv3ld2Pl/pPXPWB32DkaA3d73cd2K/9dq6rZmTUztzr87N1lW6tL1nXJbcM8Ntxfg+w8D+022hqNV/bQS1q7dm28//3vj+effz5++tOfbvHzVq5cWVw6LF++PHbbbbdYsGBBjKxJy5f+8O93zuzi7Yf+16TY4aklEaNHr3sAdGhvj/j1r5vSTC5cuLB2mXUFtcgttdqbbEsvt/WId37qO7FkwC7NGVMNDV3+TNw544Ro3+DP0evlVOVrupsio+mfiPZNbl/T1iPe0U1zOuDJR+LGfz6zKGI6pC3o/R+7OH45fO8mjqzrWP/YO6f/+tvSrkRbW1usbUT02CDc/++pb3ucltSd/7Z15589N1lvfR4/+taU2KH3Nr1+3Zr7uF1QHXPzPLQ1deSWJq8MGDBg+59NKa0d88ADD8S11177uov+Dhw4cP0lDar20pSwDR8AyZo1EfPnN2tEdFWb2ZZ6NtbG7s8/2bQh1dEeS5/caAdoa3Kq8jXdTZHRZm5v78Y5jV/44EZFTJLeP3TRQ00aUdez/rH3by9FrFn3GExFTNJRxKxutMcZq07zOK2gO2fWnX/23GS9dXlM6vO76Ndrc/+TQifxPLRbqTQz5vTTT48bbrgh7rjjjthjj9c+ptLMmG2nkWytmTGN9vb4/SPzo1HD3+WiRYtin332Kc6SNmLEiGz327ZwYfQdu1cxNXhzOaVDusad/8Pi9p/81eHRr3d79Fi0MAbtt/ervmbpg3Nj7Yi82S5+8sl4y8EHx/0//3kMGz486qLIaN+x6w9R6pCmoy996JHsOdUhs/b77omdDvujV82Mef72n8SaQ+q/dkwdtrWNHntDe0Sc8oZXfc6KE38UjaEHve5ju5X/rlVVh8yalVtdfvbusK3VKes65LalPOLxx6Othi8k12IftwuqY26eh3bvmTGl5tyl3ubTn/50fP/734/bb7/9dYuYpE+fPsWlS0kbWjo275RT1jWR6QEwc6bFk9gu21LbzJnRb8/do47Sqz+N1SuL66xTclMeW5nTzm/ovW5s++y12a8ZnG7P7Pc79Iq1Ly2LQTv0il3eUKO/dymLyy6L+OQn163hkfToEW2zZjUlp1pk9u53Rhx/fMQ//uP6m9qOPz4Gpdu7gFpsaxs+9ooqK+lYgadYOSb69WyPSI/TEo/tlvu7VlUNMmtabjX52bvFtlajrGuR2xbyiBoWMbQ4z0O7lZ5lD026+uqri1kx/fv3jyVLlhS3p8OP+vXrFy0lLZI0adK6KWFjxngAUJ1tqfNyku3WZ3TXXevenzBBTldemf5Di7jzzoh3vMPZlLZlu3rg7ohffCFi0G4Rb/14xM/+KWL5oogdh7z6cz1Ot153zqw7/+y5yXpj8qAubIvdRqkyZvr06cX1YYcdttHtV1xxRXziE5+IlpM2fBs/24NtqfNyku3rS/l8+MPNHkW9pAJGCbNtisfehyKOOCaivXdaOCbikBMi1qyK6LnJrB2P0/K6c2bd+WfPTdYbkwd1YVvsFkofpgQAsN6GxUsqZDYtYgAA2H5nUwIAAACgPGUMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyKhnNMnixYujLlasXrP+7UWLFkW/Xu1RJx1Z1SmzrkBurZOZx2jrkVk1citPZtXIrTyZVSO38mTWOrnZx21NW5tXW6PRaHT6aCJi8uTJccMNN0S6u9WrV0edtPXqE6PO/F7x9m8u/mA0Vq9s9pCADXiMAgDQauzjtrZly5bFgAEDmj8z5vrrry+uly9fHgMHDow5c+bEsGHDoi6N5MSZDxRvz507t5aN5Pjx42uVWVcgt9bJzGO09cisGrmVJ7Nq5FaezKqRW3kya53c7OO2po7canuYUvpljhw5MurgpVUvR8S6B8GIESNih95Ni6XLZNaVyK3rZ+Yx2rpkVo3cypNZNXIrT2bVyK08mXX93Ozjdm8W8AUAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBqAFLViwIE488cQYPnx49O7dO0aPHh1nnHFGPPvss80eWm3JrBq5lSezauRWnswA6ksZA9BiHnvssRg3blzMmzcvrrnmmpg/f37MmDEjbr311pgwYUI899xzzR5i7cisGrmVJ7Nq5FaezADqrWezBwDA9jV16tTiFdBbbrkl+vXrV9w2atSoOPjgg2OvvfaKs88+O6ZPn97sYdaKzKqRW3kyq0Zu5ckMoN7MjAFoIemVztmzZ8dpp522fue7w9ChQ2PKlClx3XXXRaPRaNoY60Zm1citPJlVI7fyZAZQf8oYgBaSpqOnnet99913sx9Pty9dujSefvrp7GOrK5lVI7fyZFaN3MqTGUALljF33HFHHHPMMcVCYG1tbfHv//7vnTMyACrzamd5MqtGbuXJrBq5lSczgBYqY1588cU46KCD4tJLL+2cEQFQ2ZgxY4qi/OGHH97sx9PtgwYNiiFDhmQfW13JrBq5lSezauRWnswAWrCMOeqoo+L888+PP/mTP4lW0bZwYUx44r9j6PJnmj0UoIqFCyNuu23ddTe38847x8SJE2PatGmxYsWKjT62ZMmSuOqqq+K4444rdtJZR2bVyK08mVUjt/JkBjVmv5VXWDPm8suj79i94pprvxR3zjgh2q/4TrNHBJRx+eURo0dHvOc9667T+93cJZdcEitXroxJkyYVh5YuWLAgbr755mLHfMSIEXHBBRc0e4i1I7Nq5FaezKqRW3kyg/rvt3ru2b11ehmT/hNYvnz5RpfaSG3kySdH29q1xbvtjUb0nnqalhK6ilcew/HKY7i4PuWUbv8YHjt2bNx7772x5557xuTJk4tTmJ588slx+OGHx1133RWDBw9u9hBrR2bVyK08mVUjt/JkBvXfb03PPR2d0X317Ow7uPDCC+O8886LWpo37w8Phle0rVkTMX9+xMiRTRsWUP0xHB7DhdGjR8eVV17Z7GF0KTKrRm7lyawauZUnM6j/c8/dn38ylgzYpWnDooVnxpx11lmxbNmy9Zc0RbI2xo6N6LFxBI329rTqWdOGBGzbYzg8hgEAqJstPPf89U7DmzYkWryM6dOnTwwYMGCjS22kV85nzVpXwETEy209YtWl07r9K+rQZbzyGC4KmCRdz5zpMQwAQO33W9NzT7Niuq/Shym98MILMT8dAvCKxx9/PO6///7iuNNRo0ZFl3PSSfH7w98bJ3z5mqKV/NEJU5o9IqCMk06KmDRp3aFJaUaMIgYAgC6w37pm16ER58xu9qjoKmVMWggsLfzV4cwzzyyujz/++C57TGpj5Mi4e9SBzR4GUFUqYJQwAAB0pf3WVS83ezR0pTLmsMMOi0aj0TmjAQAAAGhxnb5mDAAAAAB/oIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGSljAAAAADJSxgAAAABkpIwBAAAAyEgZAwAAAJCRMgYAAAAgI2UMAAAAQEbKGAAAAICMlDEAAAAAGfWMJlm8eHHUxYrVa9a/vWjRoujXqz3qpCOrOmXWFcitdTLzGG09MqtGbuXJrBq5lSezauRWnsxaJzf7uK1pa/NqazQajU4fTURMnjw5brjhhkh3t3r16qiTtl59YtSZ3yve/s3FH4zG6pXNHhKwAY9RAABajX3c1rZs2bIYMGBA82fGXH/99cX18uXLY+DAgTFnzpwYNmxY1KWRnDjzgeLtuXPn1rKRHD9+fK0y6wrk1jqZeYy2HplVI7fyZFaN3MqTWTVyK09mrZObfdzW1JFbbQ9TSr/MkSNHRh28tOrliFj3IBgxYkTs0LtpsXSZzLoSuXX9zDxGW5fMqpFbeTKrRm7lyawauZUns66fm33c7s0CvgAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGALSgBQsWxIknnhjDhw+P3r17x+jRo+OMM86IZ599ttlDqy2ZVSO38mRWjdzKkxlAfSljAFrMY489FuPGjYt58+bFNddcE/Pnz48ZM2bErbfeGhMmTIjnnnuu2UOsHZlVI7fyZFaN3MqTGUC99Wz2AADYvqZOnVq8AnrLLbdEv379ittGjRoVBx98cOy1115x9tlnx/Tp05s9zFqRWTVyK09m1citPJkB1JuZMQAtJL3SOXv27DjttNPW73x3GDp0aEyZMiWuu+66aDQaTRtj3cisGrmVJ7Nq5FaezADqTxkD0ELSdPS0c73vvvtu9uPp9qVLl8bTTz+dfWx1JbNq5FaezKqRW3kyA2jRMubSSy+N3XffPfr27Rtve9vbYs6cOdt/ZABU5tXO8mRWjdzKk1k1citPZgAtVMakKY1nnnlmnHvuufGzn/0sDjrooJg0aVI89dRTnTNCALbamDFjoq2tLR5++OHNfjzdPmjQoBgyZEj2sdWVzKqRW3kyq0Zu5ckMaHUPPvNgnDT7pOK625QxF198cXzyk5+ME044Ifbbb79iVfYddtghvvOd70RLWbgw4rbb1l3z+uRVney2nQzX23nnnWPixIkxbdq0WLFixUYfW7JkSVx11VVx3HHHFTvprCOzauRWnsyqkVt5MoMuzH7tVrnx0RtjzpI58R+/uOYPeW2YXRfIsdTZlFatWhX33XdfnHXWWetv69GjRxxxxBFx1113RSt4adWaaL/iO9H7tFOjbe3aaPToEaumTY81J5zYtDGtWL0m2nr1Ka5fWvVy1E3d8uoqudUxu7pmlh6XW3T55REnnxyxdm36gxQxa1bESSdFd3bJJZfE29/+9mLW4vnnnx977LFHPPjgg/H5z38+RowYERdccEGzh1g7MqtGbuXJrBq5lScz6Fo8D319i198Mp5f+Xy0RVvc9PjNxW03PfS9eP83vxmNtrYY9MKaGP7MqoiOojkdqlnj5welyphnnnkm1qxZE2984xs3uj29/6tf/WqzX7Ny5cri0mH58uVRZ0f/1bVx54xPRdsrx9imB0L7qafGu3/RJ5YM2KVp4xp15vdi4swHIiJd6mPo8mdqmVfdc6tzdnXO7FVS091RxCTp+pRTIiZNihg5MrqrsWPHxr333lscTjp58uTirBrp7BnHHntscdvgwYObPcTakVk1citPZtXIrTyZQdfieejr67/vF//wToqpLeK5/u1x3Hlj1t/8y088sK6E6VDj5welypgqLrzwwjjvvPOizvr1ao9xowfFvU8sjT2WPhntmyx21rOxNnZ//slalAt1I6/qZFdeepymx+t68+b9oYjpsGZNxPz5tftjm9vo0aPjyiuvbPYwuhSZVSO38mRWjdzKkxnUm+eh5axYdFz0Hf6v0da2tihiCq/Mgmlf04jz/2ELhyTV9PlBqTJml112ifb29vjtb3+70e3p/dS0b046pCkt+LvhzJjddtst6iQdL/uvn5qwbhrWwv2jcf2XiyayQ6O9Pa44/yPRaNIvb9GiRbHPPvvE3Llzi2mldVLHvLpCbnXNru6Zpf+wNjq+fezYdVMPNyxk2tvTyoVNGR8AAGwtz0PLmhS/eu4DcfzsP3/VR67+X4/Gfk/8fvNfVtPnB6XKmN69e8chhxwSt956azHFMVm7dm3x/umnn77Zr+nTp09x6QoPhB1694zYc/d1x5SlqUypQWtvj7aZM6Nfur2JT0Abq1cW18UY66SGeXWJ3GqaXe0z21T6j2mTDGPmzNq13gAAsDmeh5bT95VZ8mndmEY0om1tIxo92tbNkOl4kTa9nS7p7Ro/PyidaJrlcvzxx8e4ceNi/Pjx8a1vfStefPHF4uxKLSMt7pOOKUtTmVKDVsNfXK3IqzrZbTsZAgDQCuzXvq7BfQfHzn13jqE7Do0/Hfun8W8PXhtLXlwSg39wZ0TfIX/ILql5jqXLmHQavKeffjrOOeec4tR4b3nLW+Lmm29+1aK+XV76hdX0l1ZL8qpOdttOhgAAtAL7ta8plTC3fOiW6NWjVzGr6MN7fzhWr10dvdt7r/uEDbOreY6V5hqlQ5K2dFgSAAAAQGfo3VG8vHKY14bvdyU9mj0AAAAAgO5EGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTIGAAAAICNlDAAAAEBGyhgAAACAjJQxAAAAABkpYwAAAAAy6hmZNRqN4nrx4sW577rL6shKZuXIrTyZVSO38mRWjdzKk1k1citPZtXIrTyZVSO38mRWTUdeHd3HlrQ1Xu8ztpNLL720uKxatSoeffTRHHcJAAAAkN2CBQti5MiRzS9jOqxduzb23nvvuO+++6KtrS3qYvny5bHbbrsVgQ0YMCDqZpdddolnnnmm2cPocuqYm22tNdUxN9taa6pjbra11lTH3GxrramOudnWWlMdc7OttZ5UsRxyyCHxyCOPRI8ePepzmFIaTO/evWPgwIFRR+kBUMcHQSqu6jiuuqtzbra11lLn3GxrraXOudnWWkudc7OttZY652Zbay11zs221lpS5/FaRUzTFvCdOnVqM+62S/vABz7Q7CF0SXIrT2bVyK08mVUjt/JkVo3cypNZNXIrT2bVyK08mXVe55H9MKW6StPD0mydZcuWaf7oVLY1crGtkYttjVxsa+RiWyMX21r35dTWr+jTp0+ce+65xTV0JtsaudjWyMW2Ri62NXKxrZGLba37MjMGAAAAICMzYwAAAAAyUsYAAAAAZKSMAQAAAMhIGQMAAACQkTLmFZdeemnsvvvu0bdv33jb294Wc+bMafaQaDEXXnhhHHroodG/f//Ydddd49hjj425c+c2e1h0A1//+tejra0t/vIv/7LZQ6EFLVq0KD760Y/GzjvvHP369YsDDjgg7r333mYPixazZs2a+Ou//uvYY489iu1sr732iq9+9avhPBRsqzvuuCOOOeaYGD58ePF/5b//+79v9PG0jZ1zzjkxbNiwYts74ogjYt68eU0bL625ra1evTq+8IUvFP+H7rjjjsXnfPzjH48nn3yyqWOmcyljIuK6666LM888szil2M9+9rM46KCDYtKkSfHUU081e2i0kB//+McxderUuPvuu+MHP/hB8Uf3yCOPjBdffLHZQ6OF3XPPPTFz5sw48MADmz0UWtDSpUvjHe94R/Tq1StuuummeOihh+Kb3/xmDBo0qNlDo8VcdNFFMX369Ljkkkvi4YcfLt7/m7/5m/j7v//7Zg+NLi7th6V9//TC7Oak7ezb3/52zJgxI/7rv/6reKKcnif8/ve/zz5WWndbe+mll4rnoal0Ttf/9m//Vrxo+/73v78pYyUPp7aOKGbCpBkL6T/4ZO3atbHbbrvFpz/96fjiF7/Y7OHRop5++ulihkwqad71rnc1ezi0oBdeeCHe+ta3xrRp0+L888+Pt7zlLfGtb32r2cOihaT/I++88874yU9+0uyh0OKOPvroeOMb3xiXX375+ts++MEPFjMV/uVf/qWpY6N1pNkK3//+94vZy0l6mpRmKPzP//k/43Of+1xx27Jly4pt8corr4w/+7M/a/KIaZVtbUsvqI0fPz6eeOKJGDVqVNbxkUe3nxmzatWquO+++4ophx169OhRvH/XXXc1dWy0tvSfeTJ48OBmD4UWlWZive9979vo7xtsTzfeeGOMGzcuPvzhDxfl8sEHHxyXXXZZs4dFC3r7298et956azzyyCPF+7/4xS/ipz/9aRx11FHNHhot7PHHH48lS5Zs9P/owIEDixdyPU8gx3OFVNrstNNOzR4KnaRndHPPPPNMcRxyarg3lN7/1a9+1bRx0drS7Ku0fkea3v/mN7+52cOhBV177bXFNNf0qgp0lscee6w4dCQd6vulL32p2N4+85nPRO/eveP4449v9vBosVlYy5cvjze96U3R3t5e7LtdcMEFMWXKlGYPjRaWiphkc88TOj4GnSEdBpfWkPnIRz4SAwYMaPZw6CTdvoyBZs1YeOCBB4pX9WB7W7BgQZxxxhnF2kRpUXLozGI5zYz52te+VryfZsakv21pbQVlDNvT9ddfH1dddVVcffXVsf/++8f9999fvKiRDiGxrQGtJK0rOXny5OIwufSCB62r2x+mtMsuuxSvsPz2t7/d6Pb0/tChQ5s2LlrX6aefHv/5n/8Zt912W4wcObLZw6EFpUMv0wLkab2Ynj17Fpe0NlFagDC9nV5Rhu0hnV1kv/322+i2fffdN37zm980bUy0ps9//vPF7Ji0Rkc628jHPvax+OxnP1ucqRA6S8dzAc8TyF3EpHVi0otqZsW0tm5fxqSp1IccckhxHPKGr/Sl9ydMmNDUsdFaUrudipi0WNePfvSj4vSc0Bne+973xi9/+cvileOOS5q9kKbzp7dTAQ3bQzrUMp3tYUNpTY/Ro0c3bUy0pnSmkbSm34bS37K0zwadJe2rpdJlw+cJ6XC5dFYlzxPorCImnTr9hz/8Yey8887NHhKdzGFKEcWx7mmKa3qyklasTmcbSaceO+GEE5o9NFrs0KQ0vfqGG26I/v37rz/WOC0El84GAdtL2r42XYsonYoz/adujSK2pzQzIS2smg5TSjuQc+bMiVmzZhUX2J6OOeaYYo2YdEaRdJjSz3/+87j44ovjxBNPbPbQaIEzD86fP3+jRXvTCxfpBAtpe0uHw6UzEo4dO7YoZ9Kph9Phca91Fhwou62lmaYf+tCHivX+0gz6NIu547lC+niaQEDrcWrrV6TTWn/jG98oNvp0+tc0nT+tlA7bS1oNfXOuuOKK+MQnPpF9PHQvhx12mFNb0ynSTuNZZ51VvJKXnqikFzg++clPNntYtJjf/e53xZPgNLs0HYaZngynhS3POeccT1LYJrfffnscfvjhr7o9vVCbTl+dniqde+65Rcn8/PPPxzvf+c6YNm1a7L333k0ZL625rX3lK1/Z4qz5tLRB2o+j9ShjAAAAADLq9mvGAAAAAOSkjAEAAADISBkDAAAAkJEyBgAAACAjZQwAAABARsoYAAAAgIyUMQAAAAAZKWMAAAAAMlLGAAAAAGSkjAEAAADISBkDAAAAkJEyBgAAACDy+f8BJvbDYTfwq7gAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "798d452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_cql_model(agent, save_dir=\"./saved_models\", model_name=None, \n",
    "                   include_optimizers=True, additional_info=None):\n",
    "    \"\"\"\n",
    "    保存CQL模型的完整状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        save_dir: 保存目录\n",
    "        model_name: 模型名称，如果为None则使用时间戳\n",
    "        include_optimizers: 是否保存优化器状态\n",
    "        additional_info: 额外信息字典（如训练轮数、超参数等）\n",
    "    \n",
    "    Returns:\n",
    "        str: 保存的文件路径\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 生成模型名称\n",
    "    if model_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"cql_model_{timestamp}\"\n",
    "    \n",
    "    # 准备保存的状态字典\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_1_state_dict': agent.critic_1.state_dict(),\n",
    "        'critic_2_state_dict': agent.critic_2.state_dict(),\n",
    "        'target_critic_1_state_dict': agent.target_critic_1.state_dict(),\n",
    "        'target_critic_2_state_dict': agent.target_critic_2.state_dict(),\n",
    "        'log_alpha': agent.log_alpha,\n",
    "        \n",
    "        # 模型超参数\n",
    "        'hyperparameters': {\n",
    "            'gamma': agent.gamma,\n",
    "            'tau': agent.tau,\n",
    "            'target_entropy': agent.target_entropy,\n",
    "            'beta': agent.beta,\n",
    "            'num_random': agent.num_random,\n",
    "            'action_dim': agent.action_dim,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 可选：保存优化器状态\n",
    "    if include_optimizers:\n",
    "        checkpoint.update({\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_1_optimizer_state_dict': agent.critic_1_optimizer.state_dict(),\n",
    "            'critic_2_optimizer_state_dict': agent.critic_2_optimizer.state_dict(),\n",
    "            'log_alpha_optimizer_state_dict': agent.log_alpha_optimizer.state_dict(),\n",
    "        })\n",
    "    \n",
    "    # 添加额外信息\n",
    "    if additional_info:\n",
    "        checkpoint['additional_info'] = additional_info\n",
    "    \n",
    "    # 保存时间戳\n",
    "    checkpoint['save_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # 保存文件\n",
    "    file_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(checkpoint, file_path)\n",
    "    \n",
    "    print(f\"模型已保存到: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def load_cql_model(agent, file_path, load_optimizers=True):\n",
    "    \"\"\"\n",
    "    加载CQL模型状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        file_path: 模型文件路径\n",
    "        load_optimizers: 是否加载优化器状态\n",
    "    \n",
    "    Returns:\n",
    "        dict: 额外信息（如果有的话）\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {file_path}\")\n",
    "    \n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(file_path, map_location=agent.device)\n",
    "    \n",
    "    # 加载网络参数\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    agent.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])\n",
    "    agent.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])\n",
    "    agent.target_critic_1.load_state_dict(checkpoint['target_critic_1_state_dict'])\n",
    "    agent.target_critic_2.load_state_dict(checkpoint['target_critic_2_state_dict'])\n",
    "    agent.log_alpha = checkpoint['log_alpha']\n",
    "    \n",
    "    # 可选：加载优化器状态\n",
    "    if load_optimizers and 'actor_optimizer_state_dict' in checkpoint:\n",
    "        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        agent.critic_1_optimizer.load_state_dict(checkpoint['critic_1_optimizer_state_dict'])\n",
    "        agent.critic_2_optimizer.load_state_dict(checkpoint['critic_2_optimizer_state_dict'])\n",
    "        agent.log_alpha_optimizer.load_state_dict(checkpoint['log_alpha_optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"模型已从 {file_path} 加载\")\n",
    "    print(f\"保存时间: {checkpoint.get('save_timestamp', '未知')}\")\n",
    "    \n",
    "    # 返回超参数和额外信息\n",
    "    return {\n",
    "        'hyperparameters': checkpoint.get('hyperparameters', {}),\n",
    "        'additional_info': checkpoint.get('additional_info', {})\n",
    "    }\n",
    "\n",
    "# 使用示例：\n",
    "# 保存模型\n",
    "additional_info = {\n",
    "    'epoch': i_epoch,\n",
    "    'return_list': return_list,\n",
    "    'best_return': max(return_list) if return_list else 0,\n",
    "    'training_params': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_trains_per_epoch': num_trains_per_epoch,\n",
    "        'buffer_size': buffer_size\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存当前训练的模型\n",
    "# save_path = save_cql_model(\n",
    "#     agent, \n",
    "#     save_dir=\"./saved_models\",\n",
    "#     model_name=\"cql_model_64\",\n",
    "#     include_optimizers=True,\n",
    "#     additional_info=additional_info\n",
    "# )\n",
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a16c1b",
   "metadata": {},
   "source": [
    "下面开始进行微调数据，这里我尝试采用更优质的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "id": "611dd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=20)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=20)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "for _ in range(10):\n",
    "    top_20_trajectories.append(top_20_trajectories[11])\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "1cc0ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 50 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/50 条轨迹\n",
      "总共添加了 1942 个转移到回放池\n",
      "完成！回放池当前大小: 1942\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "id": "ca520b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均奖励: 64.0\n",
      "完成！回放池当前大小: 2332\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10      #采样的batch大小\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "\n",
    "    env.step(h_iter, actions.cpu())\n",
    "\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    if h_iter == 0:\n",
    "        reward = mat_return[-1]\n",
    "    else:\n",
    "        reward = mat_return[-1]-mat_return[-2]\n",
    "\n",
    "    if h_iter == H-2:\n",
    "        next_state = batch_state\n",
    "        done = 1\n",
    "    else:\n",
    "        next_state = append_state(mat_state, H-1)\n",
    "        done = 0\n",
    "\n",
    "    for j in range(params[\"common\"][\"batch_size\"]):\n",
    "        replay_buffer.add(batch_state[j],actions[j],reward[j],next_state[j],done)\n",
    "print(f\"平均奖励: {mat_return[-1].float().mean()}\")\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "id": "6bad08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 640\n",
    "# new_state=expand_trajectory_states(mat_state, H)\n",
    "# new_state[-1],new_state[-2],new_state[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "id": "303999b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:57<00:00,  5.71s/it, epoch=10, return=64.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:57<00:00,  5.76s/it, epoch=20, return=61.600]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:56<00:00,  5.62s/it, epoch=30, return=58.000]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:56<00:00,  5.66s/it, epoch=40, return=58.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:56<00:00,  5.65s/it, epoch=50, return=58.040]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:58<00:00,  5.88s/it, epoch=60, return=56.110]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:58<00:00,  5.89s/it, epoch=70, return=58.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:01<00:00,  6.14s/it, epoch=80, return=42.040]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:58<00:00,  5.89s/it, epoch=90, return=48.920]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:00<00:00,  6.08s/it, epoch=100, return=58.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f21ed209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "e98386d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 24), (28, 38), (29, 52), (30, 53), (31, 54), (32, 40), (33, 26), (34, 12), (35, 12), (36, 12), (37, 13), (38, 13), (39, 13)]\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)\n",
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ad9d5",
   "metadata": {},
   "source": [
    "看来不太行，下面用轨迹试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "b8942034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 50 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 50 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=50)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=50)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "for _ in range(10):\n",
    "    top_20_trajectories.append(top_20_trajectories[11])\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "c4a6bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "id": "76a4de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 11), (38, 10), (39, 24)]\n"
     ]
    }
   ],
   "source": [
    "path = create_path_with_timesteps(top_20_trajectories[11][\"states\"])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "id": "60948bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[34., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., 10.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., 10.]])]"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_trajectory_states(top_20_trajectories[11][\"states\"], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "id": "f3fd36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryReplayBuffer:\n",
    "    \"\"\"\n",
    "    一个简单的经验回放池，用于存储和采样整个轨迹。\n",
    "    - 添加: add(transition_dict)\n",
    "    - 采样: sample() -> transition_dict\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, trajectory_dict):\n",
    "        self.buffer.append(trajectory_dict)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.choice(self.buffer)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "id": "42f39f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_buffer = TrajectoryReplayBuffer(capacity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "id": "2b48bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理并加载 110 条轨迹...\n",
      "加载完成！回放池当前大小: 110 条轨迹。\n",
      "\n",
      "从回放池中采样出的一条完整轨迹 (transition_dict):\n",
      "  'states': 包含 38 个元素\n",
      "  'actions': 包含 38 个元素\n",
      "  'next_states': 包含 38 个元素\n",
      "  'rewards': 包含 38 个元素\n",
      "  'dones': 包含 38 个元素\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_89990/2226817346.py:48: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['states'].append(np.array(expanded_states[i].squeeze()))\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_89990/2226817346.py:51: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['next_states'].append(np.array(expanded_states[i + 1].squeeze()))\n"
     ]
    }
   ],
   "source": [
    "def process_and_load_trajectories(trajectories, trajectory_buffer, env, H, params):\n",
    "    \"\"\"\n",
    "    处理一个轨迹列表，将每条轨迹转换为一个transition_dict，并添加到trajectory_buffer中。\n",
    "    \n",
    "    Args:\n",
    "        trajectories (list): 从文件中加载的原始轨迹列表。\n",
    "        trajectory_buffer (TrajectoryReplayBuffer): 目标回放池。\n",
    "        env: 环境对象。\n",
    "        H (int): 时间范围参数。\n",
    "        params (dict): 参数字典。\n",
    "    \"\"\"\n",
    "    print(f\"开始处理并加载 {len(trajectories)} 条轨迹...\")\n",
    "    \n",
    "    for traj_data in trajectories:\n",
    "        # --- 开始处理单条轨迹 ---\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))\n",
    "        \n",
    "        # 1. 实时计算边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        for i in range(min_length + 1):\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            marginal_rewards.append(marginal_reward)\n",
    "\n",
    "        # 2. 拓展轨迹状态以匹配网络输入\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 3. 组装成一个完整的 transition_dict\n",
    "        transition_dict = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'next_states': [],\n",
    "            'rewards': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        \n",
    "        for i in range(min_length):\n",
    "            transition_dict['states'].append(np.array(expanded_states[i].squeeze()))\n",
    "            transition_dict['actions'].append(trajectory_actions[i])\n",
    "            transition_dict['rewards'].append(marginal_rewards[i+1])\n",
    "            transition_dict['next_states'].append(np.array(expanded_states[i + 1].squeeze()))\n",
    "            transition_dict['dones'].append(1 if i >= H - 2 else 0)\n",
    "        \n",
    "        # --- 单条轨迹处理结束 ---\n",
    "\n",
    "        # 将这个字典作为一个整体添加到缓冲区\n",
    "        if transition_dict['states']: # 确保轨迹不是空的\n",
    "            trajectory_buffer.add(transition_dict)\n",
    "    \n",
    "    print(f\"加载完成！回放池当前大小: {trajectory_buffer.size()} 条轨迹。\")\n",
    "\n",
    "\n",
    "# --- 使用示例 ---\n",
    "\n",
    "# 1. 初始化 TrajectoryReplayBuffer\n",
    "trajectory_buffer = TrajectoryReplayBuffer(capacity=200)\n",
    "\n",
    "# 2. 将 top_20_trajectories 加载到 buffer 中\n",
    "# top_20_trajectories 是之前代码单元中已经加载好的数据\n",
    "process_and_load_trajectories(top_20_trajectories, trajectory_buffer, env, H, params)\n",
    "\n",
    "# 3. 从 buffer 中采样一条轨迹并检查其内容\n",
    "if trajectory_buffer.size() > 0:\n",
    "    sampled_full_trajectory = trajectory_buffer.sample()\n",
    "    print(\"\\n从回放池中采样出的一条完整轨迹 (transition_dict):\")\n",
    "    for key, value in sampled_full_trajectory.items():\n",
    "        # 打印每个列表的长度，以确认是完整的轨迹\n",
    "        print(f\"  '{key}': 包含 {len(value)} 个元素\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "id": "3a563a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': [array([34., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32)], 'actions': [1, 1, 1, 2, 1, 2, 2, 1, 1, 4, 4, 4, 4, 4, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 3], 'next_states': [array([34., 35., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32)], 'rewards': [tensor([6.], dtype=torch.float64), tensor([2]), tensor([2]), tensor([1]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1])], 'dones': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(trajectory_buffer.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "a162ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "7bad0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:10<00:00,  1.10s/it, epoch=10, return=59.900]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it, epoch=20, return=60.410]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, epoch=30, return=55.600]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it, epoch=40, return=50.430]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:10<00:00,  1.08s/it, epoch=50, return=56.030]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, epoch=60, return=58.510]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:10<00:00,  1.07s/it, epoch=70, return=57.740]\n",
      "Iteration 7: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it, epoch=80, return=55.640]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:10<00:00,  1.05s/it, epoch=90, return=58.600]\n",
      "Iteration 9: 100%|██████████| 10/10 [00:10<00:00,  1.06s/it, epoch=100, return=43.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                transition_dict = trajectory_buffer.sample()\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28595d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
