{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94472d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "760dc028",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d75e119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 30, 'y': 30}, 'horizon': 80, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'single_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 50, 'lr': 0.01}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 500}, 'visu': {'wb': 'online', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001, 18.4999, 18.5001, 19.4999, 19.5001, 20.4999, 20.5001, 21.4999, 21.5001, 22.4999, 22.5001, 23.4999, 23.5001, 24.4999, 24.5001, 25.4999, 25.5001, 26.4999, 26.5001, 27.4999, 27.5001, 28.4999, 28.5001, 29.4999, 29.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001, 14.4999, 14.5001, 15.4999, 15.5001, 16.4999, 16.5001, 17.4999, 17.5001, 18.4999, 18.5001, 19.4999, 19.5001, 20.4999, 20.5001, 21.4999, 21.5001, 22.4999, 22.5001, 23.4999, 23.5001, 24.4999, 24.5001, 25.4999, 25.5001, 26.4999, 26.5001, 27.4999, 27.5001, 28.4999, 28.5001, 29.4999, 29.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 30, \"y\": 30},\n",
    "        \"horizon\": 80,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"single_room\",\n",
    "        \"num\": 1,\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 50,\n",
    "        \"lr\": 0.01\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 500\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"online\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b2f94f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(gamma, lmbda, td_delta):\n",
    "    td_delta = td_delta.detach().numpy()\n",
    "    advantage_list = []\n",
    "    advantage = 0.0\n",
    "    for delta in td_delta[::-1]:\n",
    "        advantage = gamma * lmbda * advantage + delta\n",
    "        advantage_list.append(advantage)\n",
    "    advantage_list.reverse()\n",
    "    return torch.tensor(advantage_list, dtype=torch.float)\n",
    "\n",
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class ValueNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        super(ValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    ''' PPO算法,采用截断方式 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 lmbda, epochs, eps, gamma, device):\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
    "                                                 lr=critic_lr)\n",
    "        self.gamma = gamma\n",
    "        self.lmbda = lmbda\n",
    "        self.epochs = epochs  # 一条序列的数据用来训练轮数\n",
    "        self.eps = eps  # PPO中截断范围的参数\n",
    "        self.device = device\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
    "                                                                       dones)\n",
    "        td_delta = td_target - self.critic(states)\n",
    "        advantage = compute_advantage(self.gamma, self.lmbda,\n",
    "                                               td_delta.cpu()).to(self.device)\n",
    "        old_log_probs = torch.log(self.actor(states).gather(1,\n",
    "                                                            actions)).detach()\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            log_probs = torch.log(self.actor(states).gather(1, actions))\n",
    "            ratio = torch.exp(log_probs - old_log_probs)\n",
    "            surr1 = ratio * advantage\n",
    "            surr2 = torch.clamp(ratio, 1 - self.eps,\n",
    "                                1 + self.eps) * advantage  # 截断\n",
    "            actor_loss = torch.mean(-torch.min(surr1, surr2))  # PPO损失函数\n",
    "            critic_loss = torch.mean(\n",
    "                F.mse_loss(self.critic(states), td_target.detach()))\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            critic_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "            self.critic_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5576e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_lr = 1e-3\n",
    "critic_lr = 1e-2\n",
    "num_episodes = 500\n",
    "hidden_dim = 128\n",
    "gamma = 0.98\n",
    "lmbda = 0.95\n",
    "epochs = 10\n",
    "eps = 0.2\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
    "    \"cpu\")\n",
    "\n",
    "# torch.manual_seed(0)\n",
    "state_dim = H-1\n",
    "action_dim = 5\n",
    "agent = PPO(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, lmbda,\n",
    "            epochs, eps, gamma, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f0dc8157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0:   0%|          | 0/50 [00:00<?, ?it/s]/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_32987/2203991611.py:41: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['states'].append(np.array(batch_state[j]))\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_32987/2203991611.py:43: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['next_states'].append(np.array(next_state[j]))\n",
      "Iteration 0:  86%|████████▌ | 43/50 [00:12<00:01,  3.56it/s, episode=40, return=46.611]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (100, 5)) of distribution Categorical(probs: torch.Size([100, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m batch_state = append_state(mat_state, H-\u001b[32m1\u001b[39m)\n\u001b[32m     21\u001b[39m probs = agent.actor(batch_state.to(device))\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m actions_dist = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m actions = actions_dist.sample()\n\u001b[32m     25\u001b[39m env.step(h_iter, actions.cpu())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/categorical.py:73\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     70\u001b[39m batch_shape = (\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py:72\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     70\u001b[39m         valid = constraint.check(value)\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._is_all_true(valid):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m             )\n\u001b[32m     79\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter probs (Tensor of shape (100, 5)) of distribution Categorical(probs: torch.Size([100, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan],\n        [nan, nan, nan, nan, nan]], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100      #采样的batch大小\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_episodes / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_episode in range(int(num_episodes / 10)):\n",
    "            transition_dict = {\n",
    "                'states': [],\n",
    "                'actions': [],\n",
    "                'next_states': [],\n",
    "                'rewards': [],\n",
    "                'dones': []\n",
    "            }\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                batch_state = append_state(mat_state, H-1)\n",
    "\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "\n",
    "                env.step(h_iter, actions.cpu())\n",
    "\n",
    "                mat_state.append(env.state)  # s+1\n",
    "                mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "                if h_iter == 0:\n",
    "                    reward = mat_return[-1]\n",
    "                else:\n",
    "                    reward = mat_return[-1]-mat_return[-2]\n",
    "\n",
    "                if h_iter == H-2:\n",
    "                    next_state = batch_state\n",
    "                    done = 1\n",
    "                else:\n",
    "                    next_state = append_state(mat_state, H-1)\n",
    "                    done = 0\n",
    "                for j in range(params[\"common\"][\"batch_size\"]):\n",
    "                    transition_dict['states'].append(np.array(batch_state[j]))\n",
    "                    transition_dict['actions'].append(actions[j])\n",
    "                    transition_dict['next_states'].append(np.array(next_state[j]))\n",
    "                    transition_dict['rewards'].append(reward[j])\n",
    "                    transition_dict['dones'].append(done)\n",
    "            return_list.append(mat_return[-1].float().mean())\n",
    "            agent.update(transition_dict)\n",
    "            if (i_episode + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'episode':\n",
    "                    '%d' % (num_episodes / 10 * i + i_episode + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f17a348c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0f7800ef",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (1, 5)) of distribution Categorical(probs: torch.Size([1, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan]], grad_fn=<DivBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m     batch_state = append_state(mat_state, H-\u001b[32m1\u001b[39m)\n\u001b[32m     19\u001b[39m probs = agent.actor(batch_state.to(device))\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m actions_dist = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m actions = actions_dist.sample()\n\u001b[32m     22\u001b[39m env.step(h_iter, actions.cpu())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/categorical.py:73\u001b[39m, in \u001b[36mCategorical.__init__\u001b[39m\u001b[34m(self, probs, logits, validate_args)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28mself\u001b[39m._num_events = \u001b[38;5;28mself\u001b[39m._param.size()[-\u001b[32m1\u001b[39m]\n\u001b[32m     70\u001b[39m batch_shape = (\n\u001b[32m     71\u001b[39m     \u001b[38;5;28mself\u001b[39m._param.size()[:-\u001b[32m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._param.ndimension() > \u001b[32m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch.Size()\n\u001b[32m     72\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/distributions/distribution.py:72\u001b[39m, in \u001b[36mDistribution.__init__\u001b[39m\u001b[34m(self, batch_shape, event_shape, validate_args)\u001b[39m\n\u001b[32m     70\u001b[39m         valid = constraint.check(value)\n\u001b[32m     71\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._is_all_true(valid):\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     73\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value.shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     75\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     76\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     77\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m             )\n\u001b[32m     79\u001b[39m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n",
      "\u001b[31mValueError\u001b[39m: Expected parameter probs (Tensor of shape (1, 5)) of distribution Categorical(probs: torch.Size([1, 5])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([[nan, nan, nan, nan, nan]], grad_fn=<DivBackward0>)"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions.cpu())\n",
    "    mat_state.append(env.state)  # s+1\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)\n",
    "print(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61da6c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4077da04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4008d682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3da737",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
