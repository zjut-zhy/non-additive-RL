{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        # random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size*self.num_random,1)),\n",
    "        #                                       dtype=torch.long).to(self.device)\n",
    "        random_unif_actions = torch.arange(0, self.action_dim, device=self.device).long().repeat(batch_size).view(-1, 1)\n",
    "        random_unif_log_pi = np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
       "         26., 12., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (3,),\n",
       " (tensor([2]),),\n",
       " array([[34., 35., 36., 37., 51., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
       "         26., 12., 11., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:21<00:00,  8.13s/it, epoch=10, return=22.832]\n",
      "Iteration 1: 100%|██████████| 10/10 [01:20<00:00,  8.04s/it, epoch=20, return=23.058]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:20<00:00,  8.02s/it, epoch=30, return=23.869]\n",
      "Iteration 3: 100%|██████████| 10/10 [01:21<00:00,  8.11s/it, epoch=40, return=29.152]\n",
      "Iteration 4: 100%|██████████| 10/10 [01:21<00:00,  8.16s/it, epoch=50, return=27.269]\n",
      "Iteration 5: 100%|██████████| 10/10 [01:22<00:00,  8.25s/it, epoch=60, return=31.844]\n",
      "Iteration 6: 100%|██████████| 10/10 [01:24<00:00,  8.43s/it, epoch=70, return=32.529]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:22<00:00,  8.27s/it, epoch=80, return=33.703]\n",
      "Iteration 8: 100%|██████████| 10/10 [01:22<00:00,  8.28s/it, epoch=90, return=36.372]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:25<00:00,  8.50s/it, epoch=100, return=34.443]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "# num_random = 5\n",
    "num_random = action_dim  # CQL中的动作采样数,这里设置为动作空间的大小\n",
    "num_epochs = 200\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([32])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 16), (6, 2), (7, 1), (8, 0), (9, 0), (10, 0), (11, 0), (12, 0), (13, 0), (14, 0), (15, 0), (16, 0), (17, 14), (18, 28), (19, 42), (20, 43), (21, 44), (22, 45), (23, 46), (24, 47), (25, 48), (26, 49), (27, 50), (28, 50), (29, 36), (30, 36), (31, 36), (32, 50), (33, 36), (34, 36), (35, 36), (36, 36), (37, 36), (38, 36), (39, 36)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8]\n",
      "y [2, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59bd4044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC agent's return: 36.93410110473633, min: 22.0, max: 46.0, mean: 36.93410110473633, var: 13.05106258392334\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10000\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions.cpu())\n",
    "    mat_state.append(env.state)  # s+1\n",
    "min_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().min()\n",
    "max_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().max()\n",
    "mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "mean_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "var_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().var()\n",
    "print(f\"BC agent's return: {mat_return}, min: {min_return}, max: {max_return}, mean: {mean_return}, var: {var_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "798d452e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "def save_cql_model(agent, save_dir=\"./saved_models\", model_name=None, \n",
    "                   include_optimizers=True, additional_info=None):\n",
    "    \"\"\"\n",
    "    保存CQL模型的完整状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        save_dir: 保存目录\n",
    "        model_name: 模型名称，如果为None则使用时间戳\n",
    "        include_optimizers: 是否保存优化器状态\n",
    "        additional_info: 额外信息字典（如训练轮数、超参数等）\n",
    "    \n",
    "    Returns:\n",
    "        str: 保存的文件路径\n",
    "    \"\"\"\n",
    "    # 创建保存目录\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # 生成模型名称\n",
    "    if model_name is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        model_name = f\"cql_model_{timestamp}\"\n",
    "    \n",
    "    # 准备保存的状态字典\n",
    "    checkpoint = {\n",
    "        'actor_state_dict': agent.actor.state_dict(),\n",
    "        'critic_1_state_dict': agent.critic_1.state_dict(),\n",
    "        'critic_2_state_dict': agent.critic_2.state_dict(),\n",
    "        'target_critic_1_state_dict': agent.target_critic_1.state_dict(),\n",
    "        'target_critic_2_state_dict': agent.target_critic_2.state_dict(),\n",
    "        'log_alpha': agent.log_alpha,\n",
    "        \n",
    "        # 模型超参数\n",
    "        'hyperparameters': {\n",
    "            'gamma': agent.gamma,\n",
    "            'tau': agent.tau,\n",
    "            'target_entropy': agent.target_entropy,\n",
    "            'beta': agent.beta,\n",
    "            'num_random': agent.num_random,\n",
    "            'action_dim': agent.action_dim,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # 可选：保存优化器状态\n",
    "    if include_optimizers:\n",
    "        checkpoint.update({\n",
    "            'actor_optimizer_state_dict': agent.actor_optimizer.state_dict(),\n",
    "            'critic_1_optimizer_state_dict': agent.critic_1_optimizer.state_dict(),\n",
    "            'critic_2_optimizer_state_dict': agent.critic_2_optimizer.state_dict(),\n",
    "            'log_alpha_optimizer_state_dict': agent.log_alpha_optimizer.state_dict(),\n",
    "        })\n",
    "    \n",
    "    # 添加额外信息\n",
    "    if additional_info:\n",
    "        checkpoint['additional_info'] = additional_info\n",
    "    \n",
    "    # 保存时间戳\n",
    "    checkpoint['save_timestamp'] = datetime.now().isoformat()\n",
    "    \n",
    "    # 保存文件\n",
    "    file_path = os.path.join(save_dir, f\"{model_name}.pth\")\n",
    "    torch.save(checkpoint, file_path)\n",
    "    \n",
    "    print(f\"模型已保存到: {file_path}\")\n",
    "    return file_path\n",
    "\n",
    "def load_cql_model(agent, file_path, load_optimizers=True):\n",
    "    \"\"\"\n",
    "    加载CQL模型状态\n",
    "    \n",
    "    Args:\n",
    "        agent: CQL智能体对象\n",
    "        file_path: 模型文件路径\n",
    "        load_optimizers: 是否加载优化器状态\n",
    "    \n",
    "    Returns:\n",
    "        dict: 额外信息（如果有的话）\n",
    "    \"\"\"\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(f\"模型文件不存在: {file_path}\")\n",
    "    \n",
    "    # 加载检查点\n",
    "    checkpoint = torch.load(file_path, map_location=agent.device)\n",
    "    \n",
    "    # 加载网络参数\n",
    "    agent.actor.load_state_dict(checkpoint['actor_state_dict'])\n",
    "    agent.critic_1.load_state_dict(checkpoint['critic_1_state_dict'])\n",
    "    agent.critic_2.load_state_dict(checkpoint['critic_2_state_dict'])\n",
    "    agent.target_critic_1.load_state_dict(checkpoint['target_critic_1_state_dict'])\n",
    "    agent.target_critic_2.load_state_dict(checkpoint['target_critic_2_state_dict'])\n",
    "    agent.log_alpha = checkpoint['log_alpha']\n",
    "    \n",
    "    # 可选：加载优化器状态\n",
    "    if load_optimizers and 'actor_optimizer_state_dict' in checkpoint:\n",
    "        agent.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])\n",
    "        agent.critic_1_optimizer.load_state_dict(checkpoint['critic_1_optimizer_state_dict'])\n",
    "        agent.critic_2_optimizer.load_state_dict(checkpoint['critic_2_optimizer_state_dict'])\n",
    "        agent.log_alpha_optimizer.load_state_dict(checkpoint['log_alpha_optimizer_state_dict'])\n",
    "    \n",
    "    print(f\"模型已从 {file_path} 加载\")\n",
    "    print(f\"保存时间: {checkpoint.get('save_timestamp', '未知')}\")\n",
    "    \n",
    "    # 返回超参数和额外信息\n",
    "    return {\n",
    "        'hyperparameters': checkpoint.get('hyperparameters', {}),\n",
    "        'additional_info': checkpoint.get('additional_info', {})\n",
    "    }\n",
    "\n",
    "# 使用示例：\n",
    "# 保存模型\n",
    "additional_info = {\n",
    "    'epoch': i_epoch,\n",
    "    'return_list': return_list,\n",
    "    'best_return': max(return_list) if return_list else 0,\n",
    "    'training_params': {\n",
    "        'batch_size': batch_size,\n",
    "        'num_trains_per_epoch': num_trains_per_epoch,\n",
    "        'buffer_size': buffer_size\n",
    "    }\n",
    "}\n",
    "\n",
    "# 保存当前训练的模型\n",
    "# save_path = save_cql_model(\n",
    "#     agent, \n",
    "#     save_dir=\"./saved_models\",\n",
    "#     model_name=\"cql_model_64\",\n",
    "#     include_optimizers=True,\n",
    "#     additional_info=additional_info\n",
    "# )\n",
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a16c1b",
   "metadata": {},
   "source": [
    "下面开始进行微调数据，这里我尝试采用更优质的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611dd846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=20)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=20)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "for _ in range(10):\n",
    "    top_20_trajectories.append(top_20_trajectories[11])\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc0ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 50 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/50 条轨迹\n",
      "总共添加了 1942 个转移到回放池\n",
      "完成！回放池当前大小: 1942\n"
     ]
    }
   ],
   "source": [
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca520b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "平均奖励: 64.0\n",
      "完成！回放池当前大小: 2332\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10      #采样的batch大小\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "\n",
    "    env.step(h_iter, actions.cpu())\n",
    "\n",
    "    mat_state.append(env.state)  # s+1\n",
    "    mat_return.append(env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]))\n",
    "    if h_iter == 0:\n",
    "        reward = mat_return[-1]\n",
    "    else:\n",
    "        reward = mat_return[-1]-mat_return[-2]\n",
    "\n",
    "    if h_iter == H-2:\n",
    "        next_state = batch_state\n",
    "        done = 1\n",
    "    else:\n",
    "        next_state = append_state(mat_state, H-1)\n",
    "        done = 0\n",
    "\n",
    "    for j in range(params[\"common\"][\"batch_size\"]):\n",
    "        replay_buffer.add(batch_state[j],actions[j],reward[j],next_state[j],done)\n",
    "print(f\"平均奖励: {mat_return[-1].float().mean()}\")\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bad08a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 640\n",
    "# new_state=expand_trajectory_states(mat_state, H)\n",
    "# new_state[-1],new_state[-2],new_state[-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303999b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:57<00:00,  5.71s/it, epoch=10, return=64.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:57<00:00,  5.76s/it, epoch=20, return=61.600]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:56<00:00,  5.62s/it, epoch=30, return=58.000]\n",
      "Iteration 3: 100%|██████████| 10/10 [00:56<00:00,  5.66s/it, epoch=40, return=58.000]\n",
      "Iteration 4: 100%|██████████| 10/10 [00:56<00:00,  5.65s/it, epoch=50, return=58.040]\n",
      "Iteration 5: 100%|██████████| 10/10 [00:58<00:00,  5.88s/it, epoch=60, return=56.110]\n",
      "Iteration 6: 100%|██████████| 10/10 [00:58<00:00,  5.89s/it, epoch=70, return=58.000]\n",
      "Iteration 7: 100%|██████████| 10/10 [01:01<00:00,  6.14s/it, epoch=80, return=42.040]\n",
      "Iteration 8: 100%|██████████| 10/10 [00:58<00:00,  5.89s/it, epoch=90, return=48.920]\n",
      "Iteration 9: 100%|██████████| 10/10 [01:00<00:00,  6.08s/it, epoch=100, return=58.000]\n"
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21ed209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([58])"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98386d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 24), (28, 38), (29, 52), (30, 53), (31, 54), (32, 40), (33, 26), (34, 12), (35, 12), (36, 12), (37, 13), (38, 13), (39, 13)]\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 3, 3, 3, 2, 1, 0, 0, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)\n",
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ad9d5",
   "metadata": {},
   "source": [
    "看来不太行，下面用轨迹试试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8942034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 20 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 68\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=20)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=20)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "for _ in range(100):\n",
    "    top_20_trajectories.append(top_20_trajectories[11])\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6bd4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    }
   ],
   "source": [
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a4de51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 11), (38, 10), (39, 24)]\n"
     ]
    }
   ],
   "source": [
    "path = create_path_with_timesteps(top_20_trajectories[11][\"states\"])\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60948bce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[34., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., -1., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., -1., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., -1., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., -1., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., -1., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., -1., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., -1.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., -1., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., -1., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., -1., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., -1., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., -1., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., -1., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., -1., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., -1., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., -1.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., 10.]]),\n",
       " tensor([[34., 33., 32., 31., 30., 44., 58., 72., 71., 70., 56., 42., 28., 14.,\n",
       "           0.,  1.,  2.,  3., 17., 31., 32., 33., 34., 35., 36., 37., 38., 52.,\n",
       "          66., 80., 81., 82., 68., 54., 40., 26., 12., 11., 10.]])]"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_trajectory_states(top_20_trajectories[11][\"states\"], H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryReplayBuffer:\n",
    "    \"\"\"\n",
    "    一个简单的经验回放池，用于存储和采样整个轨迹。\n",
    "    - 添加: add(transition_dict)\n",
    "    - 采样: sample() -> transition_dict\n",
    "    \"\"\"\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, trajectory_dict):\n",
    "        self.buffer.append(trajectory_dict)\n",
    "\n",
    "    def sample(self):\n",
    "        return random.choice(self.buffer)\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f39f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectory_buffer = TrajectoryReplayBuffer(capacity=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b48bdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始处理并加载 140 条轨迹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_89990/3476544066.py:48: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['states'].append(np.array(expanded_states[i].squeeze()))\n",
      "/var/folders/59/76h52s611154cw1jnz1dgz5r0000gn/T/ipykernel_89990/3476544066.py:51: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  transition_dict['next_states'].append(np.array(expanded_states[i + 1].squeeze()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "加载完成！回放池当前大小: 140 条轨迹。\n",
      "\n",
      "从回放池中采样出的一条完整轨迹 (transition_dict):\n",
      "  'states': 包含 39 个元素\n",
      "  'actions': 包含 39 个元素\n",
      "  'next_states': 包含 39 个元素\n",
      "  'rewards': 包含 39 个元素\n",
      "  'dones': 包含 39 个元素\n"
     ]
    }
   ],
   "source": [
    "def process_and_load_trajectories(trajectories, trajectory_buffer, env, H, params):\n",
    "    \"\"\"\n",
    "    处理一个轨迹列表，将每条轨迹转换为一个transition_dict，并添加到trajectory_buffer中。\n",
    "    \n",
    "    Args:\n",
    "        trajectories (list): 从文件中加载的原始轨迹列表。\n",
    "        trajectory_buffer (TrajectoryReplayBuffer): 目标回放池。\n",
    "        env: 环境对象。\n",
    "        H (int): 时间范围参数。\n",
    "        params (dict): 参数字典。\n",
    "    \"\"\"\n",
    "    print(f\"开始处理并加载 {len(trajectories)} 条轨迹...\")\n",
    "    \n",
    "    for traj_data in trajectories:\n",
    "        # --- 开始处理单条轨迹 ---\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))\n",
    "        \n",
    "        # 1. 实时计算边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        for i in range(min_length + 1):\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            marginal_rewards.append(marginal_reward)\n",
    "\n",
    "        # 2. 拓展轨迹状态以匹配网络输入\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 3. 组装成一个完整的 transition_dict\n",
    "        transition_dict = {\n",
    "            'states': [],\n",
    "            'actions': [],\n",
    "            'next_states': [],\n",
    "            'rewards': [],\n",
    "            'dones': []\n",
    "        }\n",
    "        \n",
    "        for i in range(min_length):\n",
    "            transition_dict['states'].append(np.array(expanded_states[i].squeeze()))\n",
    "            transition_dict['actions'].append(trajectory_actions[i])\n",
    "            transition_dict['rewards'].append(marginal_rewards[i+1])\n",
    "            transition_dict['next_states'].append(np.array(expanded_states[i + 1].squeeze()))\n",
    "            transition_dict['dones'].append(1 if i >= H - 2 else 0)\n",
    "        \n",
    "        # --- 单条轨迹处理结束 ---\n",
    "\n",
    "        # 将这个字典作为一个整体添加到缓冲区\n",
    "        if transition_dict['states']: # 确保轨迹不是空的\n",
    "            trajectory_buffer.add(transition_dict)\n",
    "    \n",
    "    print(f\"加载完成！回放池当前大小: {trajectory_buffer.size()} 条轨迹。\")\n",
    "\n",
    "\n",
    "# --- 使用示例 ---\n",
    "\n",
    "# 1. 初始化 TrajectoryReplayBuffer\n",
    "trajectory_buffer = TrajectoryReplayBuffer(capacity=2000)\n",
    "\n",
    "# 2. 将 top_20_trajectories 加载到 buffer 中\n",
    "# top_20_trajectories 是之前代码单元中已经加载好的数据\n",
    "process_and_load_trajectories(top_20_trajectories, trajectory_buffer, env, H, params)\n",
    "\n",
    "# 3. 从 buffer 中采样一条轨迹并检查其内容\n",
    "if trajectory_buffer.size() > 0:\n",
    "    sampled_full_trajectory = trajectory_buffer.sample()\n",
    "    print(\"\\n从回放池中采样出的一条完整轨迹 (transition_dict):\")\n",
    "    for key, value in sampled_full_trajectory.items():\n",
    "        # 打印每个列表的长度，以确认是完整的轨迹\n",
    "        print(f\"  '{key}': 包含 {len(value)} 个元素\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a563a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'states': [array([34., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32)], 'actions': [1, 1, 1, 1, 2, 2, 2, 1, 1, 4, 4, 4, 4, 4, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 3, 3, 2, 2, 2, 2, 1, 1, 2, 3, 3], 'next_states': [array([34., 35., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., -1., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., -1., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., -1., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., -1., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., -1., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., -1., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., -1., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., -1., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., -1.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1., -1., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., -1., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., -1., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., -1., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., -1., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., -1., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., -1., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., -1., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., -1.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32), array([34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 68., 54., 40.,\n",
      "       26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31.,\n",
      "       30., 16.,  2.,  1.,  0., 14., 28., 42., 56., 57., 58., 72., 71.],\n",
      "      dtype=float32)], 'rewards': [tensor([6.], dtype=torch.float64), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([0]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([2]), tensor([1]), tensor([1])], 'dones': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(trajectory_buffer.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a162ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bad0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [00:11<00:00,  1.13s/it, epoch=10, return=63.780]\n",
      "Iteration 1: 100%|██████████| 10/10 [00:10<00:00,  1.02s/it, epoch=20, return=64.000]\n",
      "Iteration 2: 100%|██████████| 10/10 [00:10<00:00,  1.01s/it, epoch=30, return=56.450]\n",
      "Iteration 3:  20%|██        | 2/10 [00:02<00:11,  1.48s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[560]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_trains_per_epoch):\n\u001b[32m     32\u001b[39m     transition_dict = trajectory_buffer.sample()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i_epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     36\u001b[39m     pbar.set_postfix({\n\u001b[32m     37\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     38\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m % (num_epochs / \u001b[32m10\u001b[39m * i + i_epoch + \u001b[32m1\u001b[39m),\n\u001b[32m     39\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     40\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33m'\u001b[39m % np.mean(return_list[-\u001b[32m10\u001b[39m:])\n\u001b[32m     41\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 163\u001b[39m, in \u001b[36mCQL.update\u001b[39m\u001b[34m(self, transition_dict)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28mself\u001b[39m.critic_2_optimizer.zero_grad()\n\u001b[32m    162\u001b[39m qf2_loss.backward(retain_graph=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcritic_2_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# 更新策略网络\u001b[39;00m\n\u001b[32m    166\u001b[39m probs = \u001b[38;5;28mself\u001b[39m.actor(states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:485\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    480\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    481\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    482\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    483\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m485\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    488\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:246\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    234\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    236\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    237\u001b[39m         group,\n\u001b[32m    238\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    243\u001b[39m         state_steps,\n\u001b[32m    244\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    247\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:933\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    931\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m933\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    935\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    936\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    937\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    938\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    939\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    940\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.12/site-packages/torch/optim/adam.py:525\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    523\u001b[39m         denom = (max_exp_avg_sqs[i].sqrt() / bias_correction2_sqrt).add_(eps)\n\u001b[32m    524\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m         denom = (\u001b[43mexp_avg_sq\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m / bias_correction2_sqrt).add_(eps)\n\u001b[32m    527\u001b[39m     param.addcdiv_(exp_avg, denom, value=-step_size)\n\u001b[32m    529\u001b[39m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return > 67:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                transition_dict = trajectory_buffer.sample()\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c7a988",
   "metadata": {},
   "source": [
    "下面尝试修改奖励，将达到68的奖励放大（从头训练）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a39de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28595d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards_2(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            if current_return == 68:\n",
    "                current_return += 10 # 将达到68的奖励加一个额外的达成奖励\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards_2(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9250fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型已从 ./saved_models/cql_model_64.pth 加载\n",
      "保存时间: 2025-07-01T19:17:51.199338\n",
      "加载的超参数: {'gamma': 0.99, 'tau': 0.005, 'target_entropy': -2, 'beta': 5.0, 'num_random': 5, 'action_dim': 5}\n",
      "额外信息: {'epoch': 9, 'return_list': [tensor(17.9100), tensor(25.5000), tensor(23.6100), tensor(21.1300), tensor(23.6600), tensor(23.6100), tensor(35.2600), tensor(36.3600), tensor(43.4600), tensor(37.6000), tensor(40.9700), tensor(36.3400), tensor(46.2800), tensor(32.1600), tensor(32.3200), tensor(39.8100), tensor(27.9900), tensor(28.3700), tensor(28.5600), tensor(28.4500), tensor(27.4800), tensor(55.7700), tensor(54.4600), tensor(55.4800), tensor(57.), tensor(57.2600), tensor(55.3400), tensor(57.4200), tensor(57.7900), tensor(57.2600), tensor(55.1300), tensor(57.2200), tensor(57.9400), tensor(58.4700), tensor(58.3700), tensor(58.2900), tensor(58.5000), tensor(57.7800), tensor(58.0600), tensor(58.6100), tensor(58.6900), tensor(58.), tensor(58.5100), tensor(58.9700), tensor(58.9100), tensor(58.9600), tensor(58.9800), tensor(59.0300), tensor(58.), tensor(57.9700), tensor(57.9700), tensor(58.), tensor(57.9100), tensor(59.0900), tensor(58.9100), tensor(58.4200), tensor(58.8500), tensor(58.), tensor(58.9500), tensor(58.9600), tensor(58.), tensor(57.9700), tensor(58.), tensor(57.9700), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(58.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(56.), tensor(64.), tensor(64.), tensor(64.), tensor(62.4000), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.), tensor(64.)], 'best_return': tensor(64.), 'training_params': {'batch_size': 640, 'num_trains_per_epoch': 500, 'buffer_size': 100000}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 10/10 [01:03<00:00,  6.37s/it, epoch=10, return=64.000]\n",
      "Iteration 1: 100%|██████████| 10/10 [01:03<00:00,  6.36s/it, epoch=20, return=60.568]\n",
      "Iteration 2: 100%|██████████| 10/10 [01:03<00:00,  6.36s/it, epoch=30, return=58.991]\n",
      "Iteration 3:   0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[597]\u001b[39m\u001b[32m, line 73\u001b[39m\n\u001b[32m     65\u001b[39m     b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n\u001b[32m     66\u001b[39m     transition_dict = {\n\u001b[32m     67\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mstates\u001b[39m\u001b[33m'\u001b[39m: b_s,\n\u001b[32m     68\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m'\u001b[39m: b_a,\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mdones\u001b[39m\u001b[33m'\u001b[39m: b_d\n\u001b[32m     72\u001b[39m     }\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m     \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransition_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (i_epoch + \u001b[32m1\u001b[39m) % \u001b[32m10\u001b[39m == \u001b[32m0\u001b[39m:\n\u001b[32m     76\u001b[39m     pbar.set_postfix({\n\u001b[32m     77\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     78\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m % (num_epochs / \u001b[32m10\u001b[39m * i + i_epoch + \u001b[32m1\u001b[39m),\n\u001b[32m     79\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mreturn\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     80\u001b[39m         \u001b[33m'\u001b[39m\u001b[38;5;132;01m%.3f\u001b[39;00m\u001b[33m'\u001b[39m % np.mean(return_list[-\u001b[32m10\u001b[39m:])\n\u001b[32m     81\u001b[39m     })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[596]\u001b[39m\u001b[32m, line 187\u001b[39m, in \u001b[36mCQL.update\u001b[39m\u001b[34m(self, transition_dict)\u001b[39m\n\u001b[32m    184\u001b[39m alpha_loss.backward()\n\u001b[32m    185\u001b[39m \u001b[38;5;28mself\u001b[39m.log_alpha_optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoft_update\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcritic_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_critic_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m \u001b[38;5;28mself\u001b[39m.soft_update(\u001b[38;5;28mself\u001b[39m.critic_2, \u001b[38;5;28mself\u001b[39m.target_critic_2)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[596]\u001b[39m\u001b[32m, line 83\u001b[39m, in \u001b[36mCQL.soft_update\u001b[39m\u001b[34m(self, net, target_net)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msoft_update\u001b[39m(\u001b[38;5;28mself\u001b[39m, net, target_net):\n\u001b[32m     81\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m param_target, param \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(target_net.parameters(),\n\u001b[32m     82\u001b[39m                                    net.parameters()):\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m         \u001b[43mparam_target\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_target\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[43m                                \u001b[49m\u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "num_random = 5\n",
    "num_epochs = 100\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "save_path = \"./saved_models/cql_model_64.pth\"\n",
    "# 加载模型示例\n",
    "loaded_info = load_cql_model(agent, save_path, load_optimizers=True)\n",
    "print(\"加载的超参数:\", loaded_info['hyperparameters'])\n",
    "print(\"额外信息:\", loaded_info['additional_info'])\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8087bcca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a57aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
