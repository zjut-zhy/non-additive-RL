{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        # random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size*self.num_random,1)),\n",
    "        #                                       dtype=torch.long).to(self.device)\n",
    "        random_unif_actions = torch.arange(0, self.action_dim, device=self.device).long().repeat(batch_size).view(-1, 1)\n",
    "        random_unif_log_pi = np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 33., 32., 31., 30., 16.,  2.,  1.,  0., 14., 28., 42., 56.,\n",
       "         70., 71., 72., 58., 44., 45., 46., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (1,),\n",
       " (tensor([0]),),\n",
       " array([[34., 33., 32., 31., 30., 16.,  2.,  1.,  0., 14., 28., 42., 56.,\n",
       "         70., 71., 72., 58., 44., 45., 46., 47., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 20/20 [01:55<00:00,  5.78s/it, epoch=20, return=23.058]\n",
      "Iteration 1: 100%|██████████| 20/20 [02:05<00:00,  6.27s/it, epoch=40, return=29.152]\n",
      "Iteration 2: 100%|██████████| 20/20 [02:06<00:00,  6.33s/it, epoch=60, return=31.844]\n",
      "Iteration 3: 100%|██████████| 20/20 [02:06<00:00,  6.32s/it, epoch=80, return=33.703]\n",
      "Iteration 4: 100%|██████████| 20/20 [02:07<00:00,  6.38s/it, epoch=100, return=34.443]\n",
      "Iteration 5: 100%|██████████| 20/20 [02:10<00:00,  6.50s/it, epoch=120, return=23.689]\n",
      "Iteration 6: 100%|██████████| 20/20 [02:24<00:00,  7.22s/it, epoch=140, return=22.573]\n",
      "Iteration 7: 100%|██████████| 20/20 [02:26<00:00,  7.31s/it, epoch=160, return=24.159]\n",
      "Iteration 8: 100%|██████████| 20/20 [02:28<00:00,  7.40s/it, epoch=180, return=30.995]\n",
      "Iteration 9: 100%|██████████| 20/20 [02:56<00:00,  8.84s/it, epoch=200, return=26.694]\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "# num_random = 5\n",
    "num_random = action_dim  # CQL中的动作采样数,这里设置为动作空间的大小\n",
    "num_epochs = 200\n",
    "num_trains_per_epoch = 500\n",
    "\n",
    "agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "            critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "            num_random)\n",
    "\n",
    "return_list = []\n",
    "for i in range(10):\n",
    "    with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "        for i_epoch in range(int(num_epochs / 10)):\n",
    "            # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "            mat_state = []\n",
    "            mat_return = []\n",
    "            env.initialize()\n",
    "            mat_state.append(env.state)\n",
    "            init_state = env.state\n",
    "            for h_iter in range(H-1):\n",
    "                if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                    batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                    # append time index to the state\n",
    "                    batch_state = torch.cat(\n",
    "                        [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                else:\n",
    "                    batch_state = append_state(mat_state, H-1)\n",
    "                probs = agent.actor(batch_state.to(device))\n",
    "                actions_dist = torch.distributions.Categorical(probs)\n",
    "                actions = actions_dist.sample()\n",
    "                env.step(h_iter, actions.cpu())\n",
    "                mat_state.append(env.state)  # s+1\n",
    "\n",
    "            mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "            return_list.append(mat_return)\n",
    "            \n",
    "            if mat_return == 68:\n",
    "                break\n",
    "\n",
    "            for _ in range(num_trains_per_epoch):\n",
    "                b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                transition_dict = {\n",
    "                    'states': b_s,\n",
    "                    'actions': b_a,\n",
    "                    'next_states': b_ns,\n",
    "                    'rewards': b_r,\n",
    "                    'dones': b_d\n",
    "                }\n",
    "                agent.update(transition_dict)\n",
    "\n",
    "            if (i_epoch + 1) % 10 == 0:\n",
    "                pbar.set_postfix({\n",
    "                    'epoch':\n",
    "                    '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                    'return':\n",
    "                    '%.3f' % np.mean(return_list[-10:])\n",
    "                })\n",
    "                \n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 44), (6, 58), (7, 72), (8, 71), (9, 70), (10, 56), (11, 42), (12, 28), (13, 14), (14, 0), (15, 1), (16, 2), (17, 3), (18, 17), (19, 31), (20, 32), (21, 33), (22, 34), (23, 35), (24, 36), (25, 37), (26, 38), (27, 52), (28, 66), (29, 80), (30, 81), (31, 82), (32, 68), (33, 54), (34, 40), (35, 26), (36, 12), (37, 13), (38, 13), (39, 13)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 2, 2, 2, 1, 0, 0, 0, 0, 0, 0, 1, 2, 3, 3, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10, 11, 12, 12, 12, 12, 12, 12, 13, 13, 13]\n",
      "y [2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 4, 3, 2, 1, 0, 0, 0, 0]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "59bd4044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC agent's return: 64.0, min: 64.0, max: 64.0, mean: 64.0, var: 0.0\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10000\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions.cpu())\n",
    "    mat_state.append(env.state)  # s+1\n",
    "min_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().min()\n",
    "max_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().max()\n",
    "mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "mean_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "var_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().var()\n",
    "print(f\"BC agent's return: {mat_return}, min: {min_return}, max: {max_return}, mean: {mean_return}, var: {var_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86d45866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 64.00±0.00, max: 64.00±0.00, mean: 64.00±0.00, median: 64.00±0.00\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    params[\"common\"][\"batch_size\"]=1000\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07235537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
