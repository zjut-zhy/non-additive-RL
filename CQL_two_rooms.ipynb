{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccbbcde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import gym\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "# import rl_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9acddda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import errno\n",
    "import os\n",
    "import random\n",
    "from importlib.metadata import requires\n",
    "from timeit import timeit\n",
    "import dill as pickle\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import wandb\n",
    "import yaml\n",
    "from sympy import Matrix, MatrixSymbol, derive_by_array, symarray\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from subrl.utils.environment import GridWorld\n",
    "from subrl.utils.network import append_state\n",
    "from subrl.utils.network import policy as agent_net\n",
    "from subrl.utils.visualization import Visu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4db21012",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    ''' 经验回放池 '''\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = collections.deque(maxlen=capacity)  # 队列,先进先出\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):  # 将数据加入buffer\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):  # 从buffer中采样数据,数量为batch_size\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = zip(*transitions)\n",
    "        return np.array(state), action, reward, np.array(next_state), done\n",
    "\n",
    "    def size(self):  # 目前buffer中数据的数量\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c77e36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNet(torch.nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return F.softmax(self.fc2(x), dim=1)\n",
    "\n",
    "\n",
    "class QValueNet(torch.nn.Module):\n",
    "    ''' 只有一层隐藏层的Q网络 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
    "        super(QValueNet, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "class CQL:\n",
    "    ''' 处理离散动作的SAC算法 '''\n",
    "    def __init__(self, state_dim, hidden_dim, action_dim, actor_lr, critic_lr,\n",
    "                 alpha_lr, target_entropy, tau, gamma, device, beta, num_random):\n",
    "        # 策略网络\n",
    "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第一个Q网络\n",
    "        self.critic_1 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        # 第二个Q网络\n",
    "        self.critic_2 = QValueNet(state_dim, hidden_dim, action_dim).to(device)\n",
    "        self.target_critic_1 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第一个目标Q网络\n",
    "        self.target_critic_2 = QValueNet(state_dim, hidden_dim,\n",
    "                                         action_dim).to(device)  # 第二个目标Q网络\n",
    "        # 令目标Q网络的初始参数和Q网络一样\n",
    "        self.target_critic_1.load_state_dict(self.critic_1.state_dict())\n",
    "        self.target_critic_2.load_state_dict(self.critic_2.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),\n",
    "                                                lr=actor_lr)\n",
    "        self.critic_1_optimizer = torch.optim.Adam(self.critic_1.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        self.critic_2_optimizer = torch.optim.Adam(self.critic_2.parameters(),\n",
    "                                                   lr=critic_lr)\n",
    "        # 使用alpha的log值,可以使训练结果比较稳定\n",
    "        self.log_alpha = torch.tensor(np.log(0.01), dtype=torch.float)\n",
    "        self.log_alpha.requires_grad = True  # 可以对alpha求梯度\n",
    "        self.log_alpha_optimizer = torch.optim.Adam([self.log_alpha],\n",
    "                                                    lr=alpha_lr)\n",
    "        self.target_entropy = target_entropy  # 目标熵的大小\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.device = device\n",
    "\n",
    "        self.beta = beta  # CQL损失函数中的系数\n",
    "        self.num_random = num_random  # CQL中的动作采样数\n",
    "        self.action_dim = action_dim  # 动作空间的维度\n",
    "\n",
    "    def take_action(self, state):\n",
    "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "        action_dist = torch.distributions.Categorical(probs)\n",
    "        action = action_dist.sample()\n",
    "        return action.item()\n",
    "\n",
    "    # 计算目标Q值,直接用策略网络的输出概率进行期望计算\n",
    "    def calc_target(self, rewards, next_states, dones):\n",
    "        next_probs = self.actor(next_states)\n",
    "        next_log_probs = torch.log(next_probs + 1e-8)\n",
    "        entropy = -torch.sum(next_probs * next_log_probs, dim=1, keepdim=True)\n",
    "        q1_value = self.target_critic_1(next_states)\n",
    "        q2_value = self.target_critic_2(next_states)\n",
    "        min_qvalue = torch.sum(next_probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)\n",
    "        next_value = min_qvalue + self.log_alpha.exp() * entropy\n",
    "        td_target = rewards + self.gamma * next_value * (1 - dones)\n",
    "        return td_target\n",
    "\n",
    "    def soft_update(self, net, target_net):\n",
    "        for param_target, param in zip(target_net.parameters(),\n",
    "                                       net.parameters()):\n",
    "            param_target.data.copy_(param_target.data * (1.0 - self.tau) +\n",
    "                                    param.data * self.tau)\n",
    "\n",
    "    def update(self, transition_dict):\n",
    "        states = torch.tensor(transition_dict['states'],\n",
    "                              dtype=torch.float).to(self.device)\n",
    "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
    "            self.device)  # 动作不再是float类型\n",
    "        rewards = torch.tensor(transition_dict['rewards'],\n",
    "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
    "        next_states = torch.tensor(transition_dict['next_states'],\n",
    "                                   dtype=torch.float).to(self.device)\n",
    "        dones = torch.tensor(transition_dict['dones'],\n",
    "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
    "\n",
    "        # 更新两个Q网络\n",
    "        td_target = self.calc_target(rewards, next_states, dones)\n",
    "        critic_1_q_values = self.critic_1(states).gather(1, actions)\n",
    "        critic_1_loss = torch.mean(\n",
    "            F.mse_loss(critic_1_q_values, td_target.detach()))\n",
    "        critic_2_q_values = self.critic_2(states).gather(1, actions)\n",
    "        critic_2_loss = torch.mean(\n",
    "            F.mse_loss(critic_2_q_values, td_target.detach()))\n",
    "        \n",
    "        # 以上与SAC相同,以下Q网络更新是CQL的额外部分\n",
    "        batch_size = states.shape[0]\n",
    "        # 1. 均匀分布的动作\n",
    "        # random_unif_actions =  torch.tensor(np.random.randint(0, self.action_dim, size=(batch_size*self.num_random,1)),\n",
    "        #                                       dtype=torch.long).to(self.device)\n",
    "        random_unif_actions = torch.arange(0, self.action_dim, device=self.device).long().repeat(batch_size).view(-1, 1)\n",
    "        random_unif_log_pi = np.log(1.0 / self.action_dim)\n",
    "\n",
    "        # 扩展状态维度（对应连续版本的tmp_states）\n",
    "        tmp_states = states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, states.shape[-1])\n",
    "        tmp_next_states = next_states.unsqueeze(1).repeat(1, self.num_random, 1).view(-1, next_states.shape[-1])\n",
    "\n",
    "        #获取当前的动作\n",
    "        random_curr_pi = self.actor(tmp_states)\n",
    "        random_curr_actions_dist = torch.distributions.Categorical(random_curr_pi)\n",
    "        random_curr_actions = random_curr_actions_dist.sample().unsqueeze(1)\n",
    "        random_curr_log_pi = torch.log(random_curr_pi.gather(1, random_curr_actions))\n",
    "        #获取下一个动作\n",
    "        random_next_pi = self.actor(tmp_next_states)\n",
    "        random_next_actions_dist = torch.distributions.Categorical(random_next_pi)\n",
    "        random_next_actions = random_next_actions_dist.sample().unsqueeze(1)\n",
    "        random_next_log_pi = torch.log(random_next_pi.gather(1, random_next_actions))\n",
    "\n",
    "        q1_unif = self.critic_1(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "        q2_unif = self.critic_2(tmp_states).gather(1, random_unif_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_curr = self.critic_1(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "        q2_curr = self.critic_2(tmp_states).gather(1, random_curr_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_next = self.critic_1(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "        q2_next = self.critic_2(tmp_states).gather(1, random_next_actions).view(-1, self.num_random, 1)\n",
    "\n",
    "        q1_cat = torch.cat([\n",
    "            q1_unif - random_unif_log_pi,\n",
    "            q1_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q1_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "\n",
    "        q2_cat = torch.cat([\n",
    "            q2_unif - random_unif_log_pi,\n",
    "            q2_curr - random_curr_log_pi.detach().view(-1, self.num_random, 1),\n",
    "            q2_next - random_next_log_pi.detach().view(-1, self.num_random, 1)\n",
    "        ],dim=1)\n",
    "\n",
    "        qf1_loss_1 = torch.logsumexp(q1_cat, dim=1).mean()\n",
    "        qf2_loss_1 = torch.logsumexp(q2_cat, dim=1).mean()\n",
    "        qf1_loss_2 = self.critic_1(states).gather(1, actions).mean()\n",
    "        qf2_loss_2 = self.critic_2(states).gather(1, actions).mean()\n",
    "        qf1_loss = critic_1_loss + self.beta * (qf1_loss_1 - qf1_loss_2)\n",
    "        qf2_loss = critic_2_loss + self.beta * (qf2_loss_1 - qf2_loss_2)\n",
    "\n",
    "        self.critic_1_optimizer.zero_grad()\n",
    "        qf1_loss.backward(retain_graph=True)\n",
    "        self.critic_1_optimizer.step()\n",
    "        self.critic_2_optimizer.zero_grad()\n",
    "        qf2_loss.backward(retain_graph=True)\n",
    "        self.critic_2_optimizer.step()\n",
    "\n",
    "        # 更新策略网络\n",
    "        probs = self.actor(states)\n",
    "        log_probs = torch.log(probs + 1e-8)\n",
    "        # 直接根据概率计算熵\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1, keepdim=True)  #\n",
    "        q1_value = self.critic_1(states)\n",
    "        q2_value = self.critic_2(states)\n",
    "        min_qvalue = torch.sum(probs * torch.min(q1_value, q2_value),\n",
    "                               dim=1,\n",
    "                               keepdim=True)  # 直接根据概率计算期望\n",
    "        actor_loss = torch.mean(-self.log_alpha.exp() * entropy - min_qvalue)\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # 更新alpha值\n",
    "        alpha_loss = torch.mean(\n",
    "            (entropy - self.target_entropy).detach() * self.log_alpha.exp())\n",
    "        self.log_alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.log_alpha_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.critic_1, self.target_critic_1)\n",
    "        self.soft_update(self.critic_2, self.target_critic_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5da1c083",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_size = 100000\n",
    "replay_buffer = ReplayBuffer(buffer_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b2b9b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env': {'start': 1, 'step_size': 0.1, 'shape': {'x': 7, 'y': 14}, 'horizon': 40, 'node_weight': 'constant', 'disc_size': 'small', 'n_players': 3, 'Cx_lengthscale': 2, 'Cx_noise': 0.001, 'Fx_lengthscale': 1, 'Fx_noise': 0.001, 'Cx_beta': 1.5, 'Fx_beta': 1.5, 'generate': False, 'env_file_name': 'env_data.pkl', 'cov_module': 'Matern', 'stochasticity': 0.0, 'domains': 'two_room', 'num': 1}, 'alg': {'gamma': 1, 'type': 'NM', 'ent_coef': 0.0, 'epochs': 140, 'lr': 0.02}, 'common': {'a': 1, 'subgrad': 'greedy', 'grad': 'pytorch', 'algo': 'both', 'init': 'deterministic', 'batch_size': 3000}, 'visu': {'wb': 'disabled', 'a': 1}}\n",
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n"
     ]
    }
   ],
   "source": [
    "workspace = \"subrl\"\n",
    "\n",
    "params = {\n",
    "    \"env\": {\n",
    "        \"start\": 1,\n",
    "        \"step_size\": 0.1,\n",
    "        \"shape\": {\"x\": 7, \"y\": 14},\n",
    "        \"horizon\": 40,\n",
    "        \"node_weight\": \"constant\",\n",
    "        \"disc_size\": \"small\",\n",
    "        \"n_players\": 3,\n",
    "        \"Cx_lengthscale\": 2,\n",
    "        \"Cx_noise\": 0.001,\n",
    "        \"Fx_lengthscale\": 1,\n",
    "        \"Fx_noise\": 0.001,\n",
    "        \"Cx_beta\": 1.5,\n",
    "        \"Fx_beta\": 1.5,\n",
    "        \"generate\": False,\n",
    "        \"env_file_name\": 'env_data.pkl',\n",
    "        \"cov_module\": 'Matern',\n",
    "        \"stochasticity\": 0.0,\n",
    "        \"domains\": \"two_room\",\n",
    "        \"num\": 1  # 替代原来的args.env\n",
    "    },\n",
    "    \"alg\": {\n",
    "        \"gamma\": 1,\n",
    "        \"type\": \"NM\",\n",
    "        \"ent_coef\": 0.0,\n",
    "        \"epochs\": 140,\n",
    "        \"lr\": 0.02\n",
    "    },\n",
    "    \"common\": {\n",
    "        \"a\": 1,\n",
    "        \"subgrad\": \"greedy\",\n",
    "        \"grad\": \"pytorch\",\n",
    "        \"algo\": \"both\",\n",
    "        \"init\": \"deterministic\",\n",
    "        \"batch_size\": 3000\n",
    "    },\n",
    "    \"visu\": {\n",
    "        \"wb\": \"disabled\",\n",
    "        \"a\": 1\n",
    "    }\n",
    "}\n",
    "\n",
    "print(params)\n",
    "\n",
    "# 2) Set the path and copy params from file\n",
    "env_load_path = workspace + \\\n",
    "    \"/environments/\" + params[\"env\"][\"node_weight\"]+ \"/env_\" + \\\n",
    "    str(params[\"env\"][\"num\"])\n",
    "\n",
    "\n",
    "\n",
    "epochs = params[\"alg\"][\"epochs\"]\n",
    "\n",
    "H = params[\"env\"][\"horizon\"]\n",
    "MAX_Ret = 2*(H+1)\n",
    "if params[\"env\"][\"disc_size\"] == \"large\":\n",
    "    MAX_Ret = 3*(H+2)\n",
    "\n",
    "# 3) Setup the environement\n",
    "env = GridWorld(\n",
    "    env_params=params[\"env\"], common_params=params[\"common\"], visu_params=params[\"visu\"], env_file_path=env_load_path)\n",
    "node_size = params[\"env\"][\"shape\"]['x']*params[\"env\"][\"shape\"]['y']\n",
    "# TransitionMatrix = torch.zeros(node_size, node_size)\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\" or params[\"env\"][\"node_weight\"] == \"steiner_covering\" or params[\"env\"][\"node_weight\"] == \"GP\": \n",
    "    a_file = open(env_load_path +\".pkl\", \"rb\")\n",
    "    data = pickle.load(a_file)\n",
    "    a_file.close()\n",
    "\n",
    "if params[\"env\"][\"node_weight\"] == \"entropy\":\n",
    "    env.cov = data\n",
    "if params[\"env\"][\"node_weight\"] == \"steiner_covering\":\n",
    "    env.items_loc = data\n",
    "if params[\"env\"][\"node_weight\"] == \"GP\":\n",
    "    env.weight = data\n",
    "\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "\n",
    "env.get_horizon_transition_matrix()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_excellent_trajectories(filepath=\"go_explore_archive_spacetime_10m.pkl\", \n",
    "                                  method='top_n', \n",
    "                                  n=10, \n",
    "                                  p=0.1, \n",
    "                                  threshold=0):\n",
    "    \"\"\"\n",
    "        Load data from the Go-Explore archive and sample high-quality trajectories based on the specified method.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to the .pkl archive file.\n",
    "            method (str): Sampling method. Options are 'top_n', 'top_p', or 'threshold'.\n",
    "            n (int): Number of trajectories to sample for the 'top_n' method.\n",
    "            p (float): Percentage of top trajectories to sample for the 'top_p' method (e.g., 0.1 means top 10%).\n",
    "            threshold (float): Minimum reward threshold for the 'threshold' method.\n",
    "        \n",
    "        Returns:\n",
    "            list: A list of trajectory dictionaries with high rewards, sorted in descending order of reward.\n",
    "                  Returns an empty list if the file does not exist or the archive is empty.\n",
    "    \"\"\"\n",
    "    # 1. Check if the file exists and load the data\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"Error: Archive file not found '{filepath}'\")\n",
    "        return []\n",
    "    \n",
    "    try:\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            archive = pickle.load(f)\n",
    "        if not archive:\n",
    "            print(\"警告：存檔庫為空。\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"讀取文件時出錯: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 2. 提取所有軌跡數據並按獎勵排序\n",
    "    # archive.values() 返回的是包含 reward, states, actions 等信息的字典\n",
    "    all_trajectories_data = list(archive.values())\n",
    "    \n",
    "    # 按 'reward' 鍵從高到低排序\n",
    "    all_trajectories_data.sort(key=lambda x: x['reward'], reverse=True)\n",
    "\n",
    "    # 3. 根據指定方法進行採樣\n",
    "    sampled_trajectories = []\n",
    "    if method == 'top_n':\n",
    "        # 取獎勵最高的前 N 條\n",
    "        num_to_sample = min(n, len(all_trajectories_data))\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-N。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的 {len(sampled_trajectories)} 條。\")\n",
    "\n",
    "    elif method == 'top_p':\n",
    "        # 取獎勵最高的前 P%\n",
    "        if not (0 < p <= 1):\n",
    "            print(\"錯誤：百分比 'p' 必須在 (0, 1] 之間。\")\n",
    "            return []\n",
    "        num_to_sample = int(len(all_trajectories_data) * p)\n",
    "        sampled_trajectories = all_trajectories_data[:num_to_sample]\n",
    "        print(f\"方法: Top-P。從 {len(all_trajectories_data)} 條軌跡中篩選出最好的前 {p*100:.1f}% ({len(sampled_trajectories)} 條)。\")\n",
    "\n",
    "    elif method == 'threshold':\n",
    "        # 取獎勵高於指定門檻的所有軌跡\n",
    "        sampled_trajectories = [data for data in all_trajectories_data if data['reward'] >= threshold]\n",
    "        print(f\"方法: Threshold。從 {len(all_trajectories_data)} 條軌跡中篩選出 {len(sampled_trajectories)} 條獎勵不低於 {threshold} 的軌跡。\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"錯誤：未知的採樣方法 '{method}'。請使用 'top_n', 'top_p', 或 'threshold'。\")\n",
    "\n",
    "    return sampled_trajectories\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "635bcd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "方法: Top-N。從 2312 條軌跡中篩選出最好的 100 條。\n",
      "其中最好的一條獎勵為: 68\n",
      "最差的一條（在這20條中）獎勵為: 64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_20_trajectories = sample_excellent_trajectories(method='top_n', n=100)\n",
    "top_20_trajectories_2=sample_excellent_trajectories(\n",
    "    \"go_explore_archive_spacetime_.pkl\",method='top_n', n=100)\n",
    "top_20_trajectories= top_20_trajectories + top_20_trajectories_2\n",
    "if top_20_trajectories:\n",
    "    print(f\"其中最好的一條獎勵為: {top_20_trajectories[0]['reward']}\")\n",
    "    print(f\"最差的一條（在這20條中）獎勵為: {top_20_trajectories[-1]['reward']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89475230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_20_trajectories[-1]['states'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9fbe86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始轨迹状态数量: 38\n",
      "拓展后状态数量: 38\n",
      "拓展后第一个状态的形状: torch.Size([1, 39])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14., -1., -1.]]),\n",
       " tensor([[34., 35., 36., 37., 38., 52., 66., 80., 81., 82., 82., 68., 54., 40.,\n",
       "          26., 12., 11., 10., 24., 38., 37., 36., 35., 34., 33., 32., 31., 30.,\n",
       "          44., 58., 72., 71., 70., 56., 42., 28., 14.,  0., -1.]]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def expand_trajectory_states(trajectory_states, H):\n",
    "    \"\"\"\n",
    "    将轨迹状态按照 append_state 的方式进行拓展\n",
    "    \n",
    "    Args:\n",
    "        trajectory_states: 轨迹中的状态列表\n",
    "        H: 时间范围参数\n",
    "        \n",
    "    Returns:\n",
    "        expanded_states: 拓展后的状态列表\n",
    "    \"\"\"\n",
    "    expanded_states = []\n",
    "    \n",
    "    # 模拟原始代码中的 mat_state 构建过程\n",
    "    mat_state = []\n",
    "    \n",
    "    for i, state in enumerate(trajectory_states):\n",
    "        mat_state.append(state)\n",
    "        \n",
    "        # 对于除了最后一个状态外的所有状态，都进行 append_state 拓展\n",
    "        if i < H - 1:\n",
    "            # 使用 append_state 函数进行状态拓展\n",
    "            batch_state = append_state(mat_state, H-1)\n",
    "            expanded_states.append(batch_state)\n",
    "        else:\n",
    "            expanded_states.append(expanded_states[-1])  # 最后一个状态不需要拓展，直接重复最后一个状态\n",
    "    \n",
    "    return expanded_states\n",
    "\n",
    "# 使用示例：拓展最佳轨迹的状态\n",
    "H = params[\"env\"][\"horizon\"]  # 使用环境参数中的 horizon\n",
    "trajectory_states=top_20_trajectories[-1]['states']\n",
    "expanded_trajectory_states = expand_trajectory_states(trajectory_states, H)\n",
    "\n",
    "print(f\"原始轨迹状态数量: {len(trajectory_states)}\")\n",
    "print(f\"拓展后状态数量: {len(expanded_trajectory_states)}\")\n",
    "\n",
    "# 查看拓展后的第一个状态的形状\n",
    "if expanded_trajectory_states:\n",
    "    print(f\"拓展后第一个状态的形状: {expanded_trajectory_states[0].shape}\")\n",
    "expanded_trajectory_states[-2],expanded_trajectory_states[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02171b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始添加 200 条轨迹到回放池（实时计算边际奖励）...\n",
      "已处理 50/200 条轨迹\n",
      "已处理 100/200 条轨迹\n",
      "已处理 150/200 条轨迹\n",
      "已处理 200/200 条轨迹\n",
      "总共添加了 7670 个转移到回放池\n",
      "完成！回放池当前大小: 7670\n"
     ]
    }
   ],
   "source": [
    "def add_trajectories_to_buffer_with_calculated_rewards(trajectories, replay_buffer, H, env, params):\n",
    "    \"\"\"\n",
    "    将多条轨迹的拓展数据添加到回放池中，实时计算边际奖励\n",
    "    \n",
    "    Args:\n",
    "        trajectories: 轨迹数据列表，每个元素包含 'states', 'actions', 'reward' 等\n",
    "        replay_buffer: 回放池对象\n",
    "        H: 时间范围参数\n",
    "        env: 环境对象\n",
    "        params: 参数字典\n",
    "    \"\"\"\n",
    "    total_transitions = 0\n",
    "    \n",
    "    for traj_idx, traj_data in enumerate(trajectories):\n",
    "        trajectory_states = traj_data['states']\n",
    "        trajectory_actions = traj_data['actions']\n",
    "        \n",
    "        # 确保状态和动作数量匹配\n",
    "        min_length = min(len(trajectory_states) - 1, len(trajectory_actions))  # 减1因为状态比动作多一个\n",
    "        \n",
    "        # 计算每个时间步的累积和边际奖励\n",
    "        mat_state_temp = []\n",
    "        cumulative_returns = []\n",
    "        marginal_rewards = []\n",
    "        \n",
    "        for i in range(min_length + 1):  # +1 包含初始状态\n",
    "            mat_state_temp.append(trajectory_states[i])\n",
    "            \n",
    "            # 计算到当前时间步的累积奖励\n",
    "            current_return = env.weighted_traj_return(mat_state_temp, type=params[\"alg\"][\"type\"])\n",
    "            cumulative_returns.append(current_return)\n",
    "            \n",
    "            # 计算边际奖励\n",
    "            if i == 0:\n",
    "                marginal_reward = current_return  # 第一步的边际奖励就是累积奖励\n",
    "            else:\n",
    "                marginal_reward = current_return - cumulative_returns[i-1]\n",
    "            \n",
    "            marginal_rewards.append(marginal_reward)\n",
    "        \n",
    "        # 拓展轨迹状态（用于网络输入）\n",
    "        expanded_states = expand_trajectory_states(trajectory_states, H)\n",
    "        \n",
    "        # 为每个时间步创建转移数据\n",
    "        for i in range(min_length):\n",
    "            # 当前状态（拓展后的）\n",
    "            current_state = expanded_states[i].squeeze()\n",
    "            \n",
    "            # 当前动作\n",
    "            current_action = trajectory_actions[i]\n",
    "            \n",
    "            # 边际奖励\n",
    "            reward = marginal_rewards[i+1]\n",
    "            \n",
    "            next_state = expanded_states[i + 1].squeeze()\n",
    "\n",
    "            # 下一个状态\n",
    "            if i < H - 2:\n",
    "                done = 0\n",
    "            else:\n",
    "                # 最后一步\n",
    "                done = 1\n",
    "            \n",
    "            # 添加到回放池\n",
    "            replay_buffer.add(current_state, current_action, reward, next_state, done)\n",
    "            total_transitions += 1\n",
    "        \n",
    "        if (traj_idx + 1) % 50 == 0:\n",
    "            print(f\"已处理 {traj_idx + 1}/{len(trajectories)} 条轨迹\")\n",
    "    \n",
    "    print(f\"总共添加了 {total_transitions} 个转移到回放池\")\n",
    "replay_buffer = ReplayBuffer(buffer_size)  # 重置回放池\n",
    "# 使用实时计算奖励的版本\n",
    "print(f\"开始添加 {len(top_20_trajectories)} 条轨迹到回放池（实时计算边际奖励）...\")\n",
    "add_trajectories_to_buffer_with_calculated_rewards(top_20_trajectories, replay_buffer, H, env, params)\n",
    "print(f\"完成！回放池当前大小: {replay_buffer.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20df0d25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[34., 33., 32., 31., 30., 16.,  2.,  1.,  0., 14., 28., 42., 56.,\n",
       "         70., 71., 72., 58., 44., 45., -1., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (1,),\n",
       " (tensor([0]),),\n",
       " array([[34., 33., 32., 31., 30., 16.,  2.,  1.,  0., 14., 28., 42., 56.,\n",
       "         70., 71., 72., 58., 44., 45., 46., -1., -1., -1., -1., -1., -1.,\n",
       "         -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1., -1.]],\n",
       "       dtype=float32),\n",
       " (0,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replay_buffer.sample(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "03da4d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "params[\"common\"][\"batch_size\"]=100\n",
    "actor_lr = 3e-4\n",
    "critic_lr = 3e-3\n",
    "alpha_lr = 3e-4\n",
    "num_episodes = 100\n",
    "hidden_dim = 128\n",
    "gamma = 0.99\n",
    "tau = 0.005  # 软更新参数\n",
    "buffer_size = 100000\n",
    "minimal_size = 1000\n",
    "batch_size = 640\n",
    "state_dim = H-1  # 状态维度\n",
    "action_dim = 5  # 动作维度\n",
    "target_entropy = -2  # 目标熵值\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "random.seed(0)\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "beta = 5.0\n",
    "# num_random = 5\n",
    "num_random = action_dim  # CQL中的动作采样数,这里设置为动作空间的大小\n",
    "num_epochs = 200\n",
    "num_trains_per_epoch = 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dd19ed91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    agent = CQL(state_dim, hidden_dim, action_dim,  actor_lr,\n",
    "                critic_lr, alpha_lr, target_entropy, tau, gamma, device, beta,\n",
    "                num_random)\n",
    "\n",
    "    return_list = []\n",
    "    for i in range(10):\n",
    "        with tqdm(total=int(num_epochs / 10), desc='Iteration %d' % i) as pbar:\n",
    "            for i_epoch in range(int(num_epochs / 10)):\n",
    "                # 此处与环境交互只是为了评估策略,最后作图用,不会用于训练\n",
    "                mat_state = []\n",
    "                mat_return = []\n",
    "                env.initialize()\n",
    "                mat_state.append(env.state)\n",
    "                init_state = env.state\n",
    "                for h_iter in range(H-1):\n",
    "                    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "                        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "                        # append time index to the state\n",
    "                        batch_state = torch.cat(\n",
    "                            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "                    else:\n",
    "                        batch_state = append_state(mat_state, H-1)\n",
    "                    probs = agent.actor(batch_state.to(device))\n",
    "                    actions_dist = torch.distributions.Categorical(probs)\n",
    "                    actions = actions_dist.sample()\n",
    "                    env.step(h_iter, actions.cpu())\n",
    "                    mat_state.append(env.state)  # s+1\n",
    "\n",
    "                mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "                return_list.append(mat_return)\n",
    "                \n",
    "                if mat_return == 68:\n",
    "                    break\n",
    "\n",
    "                for _ in range(num_trains_per_epoch):\n",
    "                    b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
    "                    transition_dict = {\n",
    "                        'states': b_s,\n",
    "                        'actions': b_a,\n",
    "                        'next_states': b_ns,\n",
    "                        'rewards': b_r,\n",
    "                        'dones': b_d\n",
    "                    }\n",
    "                    agent.update(transition_dict)\n",
    "\n",
    "                if (i_epoch + 1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        'epoch':\n",
    "                        '%d' % (num_epochs / 10 * i + i_epoch + 1),\n",
    "                        'return':\n",
    "                        '%.3f' % np.mean(return_list[-10:])\n",
    "                    })\n",
    "                    \n",
    "                pbar.update(1)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7b79fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 0: 100%|██████████| 20/20 [06:41<00:00, 20.08s/it, epoch=20, return=28.450]\n",
      "Iteration 1: 100%|██████████| 20/20 [08:39<00:00, 25.96s/it, epoch=40, return=23.710]\n",
      "Iteration 2: 100%|██████████| 20/20 [08:31<00:00, 25.56s/it, epoch=60, return=24.812]\n",
      "Iteration 3: 100%|██████████| 20/20 [08:22<00:00, 25.11s/it, epoch=80, return=23.951]\n",
      "Iteration 4: 100%|██████████| 20/20 [08:09<00:00, 24.48s/it, epoch=100, return=23.807]\n",
      "Iteration 5: 100%|██████████| 20/20 [08:03<00:00, 24.18s/it, epoch=120, return=22.159]\n",
      "Iteration 6: 100%|██████████| 20/20 [07:55<00:00, 23.78s/it, epoch=140, return=22.001]\n",
      "Iteration 7: 100%|██████████| 20/20 [07:43<00:00, 23.20s/it, epoch=160, return=22.002]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:46<00:00, 20.33s/it, epoch=180, return=22.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:29<00:00, 19.46s/it, epoch=200, return=22.012]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:15<00:00, 18.76s/it, epoch=20, return=22.116]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:56<00:00, 20.85s/it, epoch=40, return=26.413]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:54<00:00, 20.74s/it, epoch=60, return=25.670]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:52<00:00, 20.64s/it, epoch=80, return=26.268]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:56<00:00, 20.84s/it, epoch=100, return=27.482]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:40<00:00, 20.03s/it, epoch=120, return=26.024]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:41<00:00, 20.06s/it, epoch=140, return=28.482]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:50<00:00, 20.50s/it, epoch=160, return=26.039]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:53<00:00, 20.66s/it, epoch=180, return=26.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:59<00:00, 20.97s/it, epoch=200, return=26.202]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:25<00:00, 19.27s/it, epoch=20, return=19.993]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:39<00:00, 19.96s/it, epoch=40, return=20.000]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:31<00:00, 19.56s/it, epoch=60, return=20.000]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:36<00:00, 19.81s/it, epoch=80, return=20.000]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:45<00:00, 20.25s/it, epoch=100, return=20.000]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:46<00:00, 20.32s/it, epoch=120, return=20.000]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:39<00:00, 19.96s/it, epoch=140, return=20.000]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:33<00:00, 19.68s/it, epoch=160, return=20.000]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:25<00:00, 19.26s/it, epoch=180, return=20.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:32<00:00, 19.62s/it, epoch=200, return=20.000]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:14<00:00, 18.71s/it, epoch=20, return=33.663]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:48<00:00, 20.41s/it, epoch=40, return=31.768]\n",
      "Iteration 2: 100%|██████████| 20/20 [07:00<00:00, 21.01s/it, epoch=60, return=30.412]\n",
      "Iteration 3: 100%|██████████| 20/20 [07:35<00:00, 22.75s/it, epoch=80, return=44.336]\n",
      "Iteration 4: 100%|██████████| 20/20 [08:08<00:00, 24.41s/it, epoch=100, return=51.656]\n",
      "Iteration 5: 100%|██████████| 20/20 [08:24<00:00, 25.23s/it, epoch=120, return=51.778]\n",
      "Iteration 6: 100%|██████████| 20/20 [08:19<00:00, 24.96s/it, epoch=140, return=50.170]\n",
      "Iteration 7: 100%|██████████| 20/20 [08:23<00:00, 25.16s/it, epoch=160, return=51.400]\n",
      "Iteration 8: 100%|██████████| 20/20 [08:11<00:00, 24.57s/it, epoch=180, return=52.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [08:06<00:00, 24.34s/it, epoch=200, return=52.000]\n",
      "Iteration 0: 100%|██████████| 20/20 [07:32<00:00, 22.63s/it, epoch=20, return=21.715]\n",
      "Iteration 1: 100%|██████████| 20/20 [07:09<00:00, 21.46s/it, epoch=40, return=25.046]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:56<00:00, 20.83s/it, epoch=60, return=21.489]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:50<00:00, 20.54s/it, epoch=80, return=24.879]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:52<00:00, 20.64s/it, epoch=100, return=25.622]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:52<00:00, 20.65s/it, epoch=120, return=22.196]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:54<00:00, 20.74s/it, epoch=140, return=24.744]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:55<00:00, 20.79s/it, epoch=160, return=25.963]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:52<00:00, 20.63s/it, epoch=180, return=24.181]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:51<00:00, 20.56s/it, epoch=200, return=23.987]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:07<00:00, 18.38s/it, epoch=20, return=52.206]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:56<00:00, 20.81s/it, epoch=40, return=55.322]\n",
      "Iteration 2: 100%|██████████| 20/20 [07:25<00:00, 22.28s/it, epoch=60, return=55.943]\n",
      "Iteration 3: 100%|██████████| 20/20 [07:42<00:00, 23.13s/it, epoch=80, return=55.987]\n",
      "Iteration 4: 100%|██████████| 20/20 [07:35<00:00, 22.77s/it, epoch=100, return=55.458]\n",
      "Iteration 5: 100%|██████████| 20/20 [07:26<00:00, 22.33s/it, epoch=120, return=56.000]\n",
      "Iteration 6:  80%|████████  | 16/20 [05:52<01:28, 22.04s/it, epoch=130, return=60.400]\n",
      "Iteration 7:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Iteration 8:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Iteration 9:   0%|          | 0/20 [00:00<?, ?it/s]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:29<00:00, 19.47s/it, epoch=20, return=24.220]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:31<00:00, 19.60s/it, epoch=40, return=24.000]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:35<00:00, 19.77s/it, epoch=60, return=24.000]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:46<00:00, 20.34s/it, epoch=80, return=23.411]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:52<00:00, 20.61s/it, epoch=100, return=23.745]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:42<00:00, 20.13s/it, epoch=120, return=24.000]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:49<00:00, 20.48s/it, epoch=140, return=24.000]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:57<00:00, 20.87s/it, epoch=160, return=24.000]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:58<00:00, 20.91s/it, epoch=180, return=24.000]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:49<00:00, 20.50s/it, epoch=200, return=24.000]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:06<00:00, 18.33s/it, epoch=20, return=19.990]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:59<00:00, 21.00s/it, epoch=40, return=20.012]\n",
      "Iteration 2: 100%|██████████| 20/20 [07:12<00:00, 21.63s/it, epoch=60, return=21.200]\n",
      "Iteration 3: 100%|██████████| 20/20 [07:21<00:00, 22.08s/it, epoch=80, return=23.976]\n",
      "Iteration 4: 100%|██████████| 20/20 [07:13<00:00, 21.67s/it, epoch=100, return=22.076]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:56<00:00, 20.84s/it, epoch=120, return=24.362]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:45<00:00, 20.27s/it, epoch=140, return=23.400]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:45<00:00, 20.28s/it, epoch=160, return=23.998]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:30<00:00, 19.51s/it, epoch=180, return=24.030]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:32<00:00, 19.64s/it, epoch=200, return=25.700]\n",
      "Iteration 0: 100%|██████████| 20/20 [06:30<00:00, 19.53s/it, epoch=20, return=32.000]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:51<00:00, 20.57s/it, epoch=40, return=29.292]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:47<00:00, 20.37s/it, epoch=60, return=23.687]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:45<00:00, 20.27s/it, epoch=80, return=26.683]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:44<00:00, 20.21s/it, epoch=100, return=22.830]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:38<00:00, 19.94s/it, epoch=120, return=22.000]\n",
      "Iteration 6: 100%|██████████| 20/20 [06:30<00:00, 19.53s/it, epoch=140, return=22.004]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:29<00:00, 19.46s/it, epoch=160, return=22.807]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:34<00:00, 19.70s/it, epoch=180, return=23.940]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:36<00:00, 19.80s/it, epoch=200, return=23.988]\n",
      "Iteration 0: 100%|██████████| 20/20 [05:56<00:00, 17.84s/it, epoch=20, return=25.706]\n",
      "Iteration 1: 100%|██████████| 20/20 [06:46<00:00, 20.32s/it, epoch=40, return=20.996]\n",
      "Iteration 2: 100%|██████████| 20/20 [06:59<00:00, 20.97s/it, epoch=60, return=23.984]\n",
      "Iteration 3: 100%|██████████| 20/20 [06:24<00:00, 19.23s/it, epoch=80, return=27.806]\n",
      "Iteration 4: 100%|██████████| 20/20 [06:24<00:00, 19.21s/it, epoch=100, return=29.550]\n",
      "Iteration 5: 100%|██████████| 20/20 [06:41<00:00, 20.09s/it, epoch=120, return=26.892]\n",
      "Iteration 6: 100%|██████████| 20/20 [07:06<00:00, 21.33s/it, epoch=140, return=27.220]\n",
      "Iteration 7: 100%|██████████| 20/20 [06:55<00:00, 20.76s/it, epoch=160, return=22.853]\n",
      "Iteration 8: 100%|██████████| 20/20 [06:40<00:00, 20.01s/it, epoch=180, return=23.879]\n",
      "Iteration 9: 100%|██████████| 20/20 [06:31<00:00, 19.58s/it, epoch=200, return=24.450]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 30.40±15.30, max: 32.00±14.86, mean: 30.76±15.12, median: 30.80±15.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    # params[\"common\"][\"batch_size\"]=1000\n",
    "    agent = train()\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39d55ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([22])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=1\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    if params[\"alg\"][\"type\"]==\"M\" or params[\"alg\"][\"type\"]==\"SRL\":\n",
    "        batch_state = mat_state[-1].reshape(-1, 1).float()\n",
    "        # append time index to the state\n",
    "        batch_state = torch.cat(\n",
    "            [batch_state, h_iter*torch.ones_like(batch_state)], 1)\n",
    "    else:\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions)\n",
    "    mat_state.append(env.state)  # s+1\n",
    "env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eca63e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 34), (1, 33), (2, 32), (3, 31), (4, 30), (5, 29), (6, 28), (7, 28), (8, 28), (9, 28), (10, 29), (11, 30), (12, 44), (13, 58), (14, 72), (15, 73), (16, 73), (17, 73), (18, 73), (19, 73), (20, 73), (21, 73), (22, 73), (23, 73), (24, 87), (25, 87), (26, 87), (27, 87), (28, 87), (29, 87), (30, 87), (31, 87), (32, 87), (33, 87), (34, 87), (35, 87), (36, 87), (37, 87), (38, 87), (39, 87)]\n"
     ]
    }
   ],
   "source": [
    "def create_path_with_timesteps(states):\n",
    "    \"\"\"\n",
    "    从轨迹数据创建带时间步的路径\n",
    "    \"\"\"\n",
    "    # 将状态转换为带时间步的格式\n",
    "    path_with_time = [(t, state.item()) for t, state in enumerate(states)]\n",
    "    return path_with_time\n",
    "path = create_path_with_timesteps(mat_state)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e51fcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001, 7.4999, 7.5001, 8.4999, 8.5001, 9.4999, 9.5001, 10.4999, 10.5001, 11.4999, 11.5001, 12.4999, 12.5001, 13.4999, 13.5001]\n",
      "y_ticks [-0.5001, -0.4999, 0.4999, 0.5001, 1.4999, 1.5001, 2.4999, 2.5001, 3.4999, 3.5001, 4.4999, 4.5001, 5.4999, 5.5001, 6.4999, 6.5001]\n",
      "x [6, 5, 4, 3, 2, 1, 0, 0, 0, 0, 1, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "y [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import importlib\n",
    "import visualization\n",
    "importlib.reload(visualization)\n",
    "from visualization import Visu\n",
    "visu = Visu(env_params=params[\"env\"])\n",
    "visu.visu_path(path,env.Hori_ActionTransitionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59bd4044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BC agent's return: 22.00149917602539, min: 19.0, max: 23.0, mean: 22.00149917602539, var: 0.0026980198454111814\n"
     ]
    }
   ],
   "source": [
    "params[\"common\"][\"batch_size\"]=10000\n",
    "mat_state = []\n",
    "mat_return = []\n",
    "env.initialize()\n",
    "mat_state.append(env.state)\n",
    "init_state = env.state\n",
    "for h_iter in range(H-1):\n",
    "    batch_state = append_state(mat_state, H-1)\n",
    "    probs = agent.actor(batch_state.to(device))\n",
    "    actions_dist = torch.distributions.Categorical(probs)\n",
    "    actions = actions_dist.sample()\n",
    "    env.step(h_iter, actions.cpu())\n",
    "    mat_state.append(env.state)  # s+1\n",
    "min_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().min()\n",
    "max_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().max()\n",
    "mat_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "mean_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().mean()\n",
    "var_return = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float().var()\n",
    "print(f\"BC agent's return: {mat_return}, min: {min_return}, max: {max_return}, mean: {mean_return}, var: {var_return}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86d45866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 21.70±0.90, max: 22.70±0.46, mean: 22.00±0.00, median: 22.00±0.00\n"
     ]
    }
   ],
   "source": [
    "min_return = []\n",
    "max_return = []\n",
    "mean_return = []\n",
    "median_return = []\n",
    "for iter in range(10):\n",
    "    params[\"common\"][\"batch_size\"]=1000\n",
    "    mat_state = []\n",
    "    mat_return = []\n",
    "    env.initialize()\n",
    "    mat_state.append(env.state)\n",
    "    init_state = env.state\n",
    "    for h_iter in range(H-1):\n",
    "        batch_state = append_state(mat_state, H-1)\n",
    "        probs = agent.actor(batch_state.to(device))\n",
    "        actions_dist = torch.distributions.Categorical(probs)\n",
    "        actions = actions_dist.sample()\n",
    "        env.step(h_iter, actions)\n",
    "        mat_state.append(env.state)  # s+1\n",
    "\n",
    "    returns = env.weighted_traj_return(mat_state, type = params[\"alg\"][\"type\"]).float()\n",
    "    min_return.append(returns.min())\n",
    "    max_return.append(returns.max())\n",
    "    mean_return.append(returns.mean())\n",
    "    median_return.append(returns.median())\n",
    "mean_min_return = np.mean(min_return)\n",
    "std_min_return = np.std(min_return)\n",
    "mean_max_return = np.mean(max_return)\n",
    "std_max_return = np.std(max_return)\n",
    "mean_mean_return = np.mean(mean_return)\n",
    "std_mean_return = np.std(mean_return)\n",
    "mean_median_return = np.mean(median_return)\n",
    "std_median_return = np.std(median_return)\n",
    "print(f\"min: {mean_min_return:.2f}±{std_min_return:.2f}, max: {mean_max_return:.2f}±{std_max_return:.2f}, mean: {mean_mean_return:.2f}±{std_mean_return:.2f}, median: {mean_median_return:.2f}±{std_median_return:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07235537",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
